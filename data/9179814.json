{
    "abstract": "Imbalanced datasets are pervasive in classification tasks and would cause degradation of the performance of classifiers in predicting minority samples. Oversampling is effective in resolving the class imbalance problem. However, existing oversampling methods generally introduce noise examples into original datasets, especially when the datasets contain class overlapping regions. In this study, a n...",
    "articleNumber": "9179814",
    "articleTitle": "Constrained Oversampling: An Oversampling Approach to Reduce Noise Generation in Imbalanced Datasets With Class Overlapping",
    "authors": [
        {
            "preferredName": "Changhui Liu",
            "normalizedName": "C. Liu",
            "firstName": "Changhui",
            "lastName": "Liu",
            "searchablePreferredName": "Changhui Liu",
            "id": 37088592711
        },
        {
            "preferredName": "Sun Jin",
            "normalizedName": "S. Jin",
            "firstName": "Sun",
            "lastName": "Jin",
            "searchablePreferredName": "Sun Jin",
            "id": 37086853078
        },
        {
            "preferredName": "Donghong Wang",
            "normalizedName": "D. Wang",
            "firstName": "Donghong",
            "lastName": "Wang",
            "searchablePreferredName": "Donghong Wang",
            "id": 37089514491
        },
        {
            "preferredName": "Zichao Luo",
            "normalizedName": "Z. Luo",
            "firstName": "Zichao",
            "lastName": "Luo",
            "searchablePreferredName": "Zichao Luo",
            "id": 37089516376
        },
        {
            "preferredName": "Jianbo Yu",
            "normalizedName": "J. Yu",
            "firstName": "Jianbo",
            "lastName": "Yu",
            "searchablePreferredName": "Jianbo Yu",
            "id": 37878462300
        },
        {
            "preferredName": "Binghai Zhou",
            "normalizedName": "B. Zhou",
            "firstName": "Binghai",
            "lastName": "Zhou",
            "searchablePreferredName": "Binghai Zhou",
            "id": 37289066800
        },
        {
            "preferredName": "Changlin Yang",
            "normalizedName": "C. Yang",
            "firstName": "Changlin",
            "lastName": "Yang",
            "searchablePreferredName": "Changlin Yang",
            "id": 37089515897
        }
    ],
    "doi": "10.1109/ACCESS.2020.3018911",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9179814/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION I.</div><h2>Introduction</h2></div><p>The class imbalance problem occurs when some of the classes in a dataset have significantly more samples than the others. The discrepancy in sample number between classes brings about the imbalanced data structure in such kinds of datasets. Researchers have reported that this imbalanced structure hampers learning, resulting in an undesirable performance of data mining algorithms <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\" data-range=\"ref1 ref2 ref3 ref4\">[1]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\" data-range=\"ref1 ref2 ref3 ref4\">[4]</a>.</p><p>Imbalanced datasets exist in many real-world classification applications, such as detecting oil spills in satellite radar images <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\" data-range=\"ref5\">[5]</a>, conducting medical diagnosis <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6\">[6]</a>, detecting credit card frauds <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\" data-range=\"ref7\">[7]</a>, analyzing neuroimaging data <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref8\">[8]</a>, predicting binding site <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\" data-range=\"ref9\">[9]</a> and so on. By convention, classes with a larger quantity of samples are called the negative classes or majority classes, while the others are referred to as the positive classes or minority classes <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\" data-range=\"ref10\">[10]</a>. In these domains, minority samples (e.g. oil spills) are often the ones of interest and misclassifying minority samples is costly <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\" data-range=\"ref3\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\" data-range=\"ref4\">[4]</a>. However, traditional classification algorithms, constructed under the assumption or expectation of balanced data distribution, are usually inadequate in tackling imbalanced classification problems. Disappointing performances of classifiers are often derived on recognizing and predicting minority instances because the imbalanced structure of training set biases the decision boundary of the classifiers toward the majority class. Class imbalance issue has received considerable attention and is identified as one of the main challenges in data mining <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\" data-range=\"ref11\">[11]</a>.</p><p>Methods designated to remedy this pervasive problem roughly fall into two categories: data-based methods <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\" data-range=\"ref12 ref13 ref14 ref15\">[12]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\" data-range=\"ref12 ref13 ref14 ref15\">[15]</a>, and algorithm-based methods <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\" data-range=\"ref16 ref17 ref18 ref19\">[16]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\" data-range=\"ref16 ref17 ref18 ref19\">[19]</a>. Algorithm-based methods attempt to modify learning algorithms to adjust the classifier to imbalanced datasets <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\" data-range=\"ref20 ref21 ref22\">[20]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\" data-range=\"ref20 ref21 ref22\">[22]</a>. They rely on selected classifiers so once we demand the use of a different classification algorithm, the extra computation cost is inevitable <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\" data-range=\"ref23\">[23]</a>. Moreover, the research presented by Maloof suggests that operations of re-sampling and adjusting classification algorithms by varying the cost matrix produce similar sets of classifiers <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\" data-range=\"ref24\">[24]</a>. Thus, we narrow the scope of our study to data-level solutions to imbalanced classification problem without loss of generality.</p><p>Data-based methods usually refer to re-sampling approaches that preprocess datasets to achieve a balanced data structure. Before data-level approaches are applied, we should notice that datasets differ from each other in the number of classes: binary-class datasets contain only one majority class and one minority class while multi-class datasets compose of several majority classes and minority classes. In multiple-class conditions, we can either decompose the problem into multiple two-class classification problems <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\" data-range=\"ref25\">[25]</a> or extend the schemes developed on binary-class scenarios to multi-class problems via pair-wise coupling techniques <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_1\" data-range=\"ref26\">[26]</a>. Therefore, we focus our efforts on imbalanced classification with binary classes the same as most researches in the literature.</p><p>Data-level strategies on binary-class imbalanced classification problems can be further divided into undersampling and oversampling. Undersampling removes samples belonging to the majority class according to designed criteria to reduce the degree of imbalance in the dataset. The least intricate undersampling proposal is Random Under Sampling (RUS) which randomly discards samples in the majority category to adjust the imbalanced data distribution. One of the more advanced undersampling techniques is the One-Sided Selection proposed by Kubat and Matwin <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_1\" data-range=\"ref27\">[27]</a>. This method detects and then deletes noisy and redundant samples in the majority category and leaves minority category untouched. By choosing a representative subset of the negative examples, it balances the dataset. Thus, the classifier learns a clean boundary and shows better predicting performances. Undersampling based on clustering put forward by Yen and Lee <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1\" data-range=\"ref28\">[28]</a>, is an approach that clusters all the training samples in the first step. Then this method decides the number of majority instances that should remain in each cluster according to the ratio of the number of majority samples to the number of minority samples in the particular cluster. In the last step, it removes the calculated amount of majority samples in each cluster to mitigate the imbalance between classes. ACOSampling, presented by Yu <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_1\" data-range=\"ref29\">[29]</a>, employs an ant colony optimization algorithm to remove less informative instances and search for optimal subsets of a randomly divided part of the original dataset. By doing that repeatedly, the statistical results from all local optimal training subsets are achieved and presented in a frequency list. Then high-frequency majority samples are extracted and they are combined with the original minority set to form a new training set.</p><p>Unlike undersampling, oversampling concentrates on increasing the number of minority samples. Japkowicz and Stephen <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_1\" data-range=\"ref30\">[30]</a> conducted a series of experiments on how re-sampling methods perform on data sets of different complexity and concluded that undersampling methods appear to be less effective than oversampling when two classes were assigned a symmetric cost. Meanwhile, oversampling was shown to help quite dramatically at all complexity and training set size levels. Similar results were reported by Bastista <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_1\" data-range=\"ref31\">[31]</a>. We are more concerned with oversampling methods because they are reported to be more effective and are also more related to our work.</p><p>Random Over Sampling is a simple oversampling method that replicates minority class samples at random to get prescribed imbalance level. It was pointed out in previous work that ROS is prone to incur overfitting problem in learning <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_1\" data-range=\"ref32\">[32]</a>. Based on this intuitive method, more sophisticated oversampling techniques have been developed in researches. Some of the improved methods are as below.</p><p>Synthetic Minority Oversampling Technique (SMOTE), proposed by Chawla <i>et al.</i>, forms new minority samples by linearly interpolating between minority samples that lie close to each other in feature space <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_1\" data-range=\"ref33\">[33]</a>. Despite its efficaciousness, this method has a major shortcoming: it blindly generates new samples for minority class examples without considering the distribution of original data, so sometimes noise samples are added into the dataset. This problem can be more serious when the original data set has one or more overlapping regions or holds noise samples in the original minority class.</p><p>Han <i>et al.</i> developed an oversampling technology called Borderline-SMOTE based on the observation that misclassified samples usually located on the borderline between minority and majority class <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_1\" data-range=\"ref34\">[34]</a>. In this proposal, only minority samples that lie near the borderline are operated on, and thus learning on the borderline is reinforced. There were two versions for their proposal: Borderline-SMOTE1 which only generates samples among borderline instances belonging to minority class and Borderline-SMOTE2 which also synthesizes examples between minority samples on the boundary and their nearest negative neighbors.</p><p>Another improved oversampling method is Adaptive Synthetic Sampling (ADASYN), which was presented by He <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_1\" data-range=\"ref35\">[35]</a>. This method considers the original distribution. This method uses the number of samples that belong to the majority class in k nearest neighbors of a minority sample as a criterion to automatically judge the number of samples synthesized around this specific minority sample. More samples would be generated around instances situate close to the majority class region than those far away from the borderline between classes.</p><p>Cluster Based Synthetic Oversampling (CBOS), devised by Barua <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_1\" data-range=\"ref36\">[36]</a>, attempts to avoid noise generation by first conducting unsupervised clustering among minority samples and then carrying out SMOTE process inside the clusters. This data generation mechanism avoids the creation of synthetic minority samples in the majority region under some circumstances. However, when overlapping areas between classes occur in the dataset, majority samples would also be included inside the clusters. Thus, oversampling in these clusters generates minority samples that fall into regions belonging to the majority class and leads to misclassification of majority samples.</p><p>Safe-level-SMOTE assigns each positive instance a safe level ratio, which is the ratio of positive samples in k nearest neighbors of that instance <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_1\" data-range=\"ref37\">[37]</a>. Synthetic samples are generated along the same line segment as SMOTE, but these samples are placed closer to minority instances to which higher safe level rations are attached. This approach is effective in eliminating noise generation; however, it does not help recognize samples in an overlapping region since it generates no samples in this region. As a result, the usefulness of this proposal is weakened by the fact that overlapping between classes is pervading in real-world datasets.</p><p>All in all, oversampling is widely investigated and used as a remediation strategy to cope with imbalanced datasets. By far, researchers have developed a batch of advanced oversampling methods, such as SMOTE, Borderline-SMOTE, and so on. They have been applied to many real-world imbalanced datasets and are shown to be effective in improving the performance of classifiers. But unfortunately, though it was pointed out that class overlapping is a main cause of misclassification in imbalanced datasets <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_1\" data-range=\"ref38\">[38]</a>; previous researchers have not yet devised competent oversampling methods to deal with class overlapping regions as far as we know.</p><p>In class overlapping regions, majority samples mingle with minority samples so the boundaries between categories become ambiguous. Since classification algorithms are designed to learn the borderline of each class, they are less powerful in overlapping areas where borderlines seem to be confusing. Consequently, the examples situated in these regions are prone to be misclassified. Poor learning in overlapping regions calls for the demand of oversampling in these regions to improve the performance of classifiers. However, most existing oversampling approaches rest on the assumption that samples close to each other in feature space belong to the same class, which is not the case in overlapping regions. So, when existing oversampling methods are applied to these overlapping regions, noise minority samples that fall into the majority region are introduced into the datasets. These noise samples are detrimental to the performance of classification algorithms. As a result, traditional oversampling approaches show a deficiency in handling datasets with overlapping between classes.</p><p>In this study, we introduce an oversampling method that distinguishes itself from other oversampling techniques by incorporating constraints in the oversampling process to inhibit noise generation in overlapping regions. This method, namely Constrained Oversampling (CO), composes of three steps. First, samples placed in the overlapping area are identified with a simple k-nearest-neighbors (KNN) based algorithm. Second, in these overlapping regions, we apply the ant colony optimization (ACO) algorithm to search for feasible paths from randomly chosen majority samples to a specific minority sample, and the majority points on the paths which are closest to the destination are picked out as boundary samples. Finally, oversampling under distance constraints is performed between boundary samples and the chosen minority point to attain a more balanced dataset.</p><p>The rest of this paper is organized as follows. The proposed constrained oversampling method is described in <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>. In <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>, experiments are conducted on various real-world datasets from the UCI repository to test this method, and the results are presented and discussed. At last, <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> concludes our work.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION II.</div><h2>A New Oversampling Method: Constrained Oversampling</h2></div><p>In order to balance the dataset without producing noise samples, we execute oversampling in overlapping regions, and the oversampling process; constraints are used to prevent noise generation. As is depicted in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1</a>, our strategy is divided into three stages.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu1-3018911-large.gif\" data-fig-id=\"fig1\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu1-3018911-small.gif\" alt=\"FIGURE 1. - Three steps of Constrained Oversampling.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Three steps of Constrained Oversampling.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p><div class=\"section_2\" id=\"sec2a\"><h3>A. The Extraction of Overlapping Area</h3><p>In the first step of our method, we build a KNN-based method to extract overlapping regions included in the original dataset. As a geometric classifier, the KNN classifier determines the class membership of a data point from its distance to reference data points <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2a\" data-range=\"ref39\">[39]</a>, and it is widely used in clustering <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2a\" data-range=\"ref40 ref41 ref42\">[40]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_2a\" data-range=\"ref40 ref41 ref42\">[42]</a>. Thus, KNN is a density-based method to classify the input data by the percentage of samples belonging to a different class in their nearest neighbors. In overlapping regions, minority and majority samples mix with each other. By the definition of KNN, minority samples locate in overlapping regions are more likely to be misclassified. This characteristic of KNN can be taken advantage of to identify samples locate in overlapping regions. When the minority samples in overlapping regions are classified to the majority class by the KNN classifier, we can know they are misclassifications because the data labels are known in our study.</p><p class=\"has-inline-formula\">First, we apply a 5-NN classifier to the original data set as recommended by Mani and Zhang <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_2a\" data-range=\"ref43\">[43]</a> and record the misclassified minority samples. For every misclassified minority sample, we choose its <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> nearest neighbors to form an overlapping set. If there are <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> misclassified minority samples, then we get <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m\\ast n$\n</tex-math></inline-formula> data points. In these data points, there are some duplicate samples. The overlapping set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula> is extracted when these duplicate samples are removed. The similarity metric of the KNN algorithm adopted in this paper is Euclidean distance. The Pseudo-code description of this step is summarized as <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a>.<div class=\"algorithm rule-both\" id=\"alg1\"><h3>Algorithm 1 The ACO-Based Method to Extract Overlapping Regions</h3><div class=\"alg-item label\"><span class=\"label\">1:</span><p class=\"has-inline-formula\"><b>Input</b>: Original training set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula>, controlling parameter of the scope of overlapping region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>.</p></div><div class=\"alg-item label\"><span class=\"label\">2:</span><p><b>Process</b>:</p></div><div class=\"alg-item label\"><span class=\"label\">3:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>: number of minority samples in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">4:</span><p class=\"has-inline-formula\">find 5 nearest neighbors for minority sample <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Min}_{i}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">5:</span><p>if (number of positive samples) &lt; 5/2</p></div><div class=\"alg-item label\"><span class=\"label\">6:</span><p class=\"has-inline-formula\">Add <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Min}_{i}$\n</tex-math></inline-formula> and its <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> nearest neighbors in overlapping set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">7:</span><p>endif</p></div><div class=\"alg-item label\"><span class=\"label\">8:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">9:</span><p class=\"has-inline-formula\">Remove duplicate samples in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">10:</span><p class=\"has-inline-formula\"><b>Output</b>: Overlapping set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula></p></div></div></p><p>It should be noted that the total number of overlapping regions do not need to define first. It is determined by the distribution of the misclassified minority samples. It forms when the overlapping set is extracted.</p><p>To better illustrate our method, we apply it to a simulated dataset with two features. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2 (a)</a> displays the binary-class imbalanced dataset with two overlapping regions which consist of 800 majority samples (represented by green circles) and 80 minority instances (represented by red pluses). After the above-mentioned method is applied to the dataset, overlapping regions which contain 191 samples are extracted and shown in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2 (b)</a>. According to the distribution of the misclassified minority samples, two overlapping regions are generated (shown in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2 (b)</a>). The following steps of our oversampling method will be carried out on these overlapping regions.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu2ab-3018911-large.gif\" data-fig-id=\"fig2\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu2ab-3018911-small.gif\" alt=\"FIGURE 2. - (a) The original data distribution (b) Overlapping regions extracted with the KNN-based method.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>(a) The original data distribution (b) Overlapping regions extracted with the KNN-based method.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Boundary Definition Based on Ant Colony Optimization Algorithm</h3><p>Traditional boundary definition methods such as Tomek-links <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\" data-range=\"ref27\">[27]</a> and KNN are density-based approaches that judge the boundary samples by the percentage of samples belonging to a different class in their nearest neighbors. In overlapping regions where minority and majority samples mix with each together, traditional boundary definition methods are prone to take all majority samples in the overlapping regions as boundary samples and are thus imprecise <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2b\" data-range=\"ref19\">[19]</a>.</p><p>We developed an ACO-based method to define the boundaries between classes in extracted overlapping regions. With this method, the boundaries between majority and minority regions are expressed in the form of a set of majority samples. Our proposal embodies two major merits: on one hand, it is more robust to noise in the original minority set since it does not solely depend on information conveyed by minority samples; on the other hand, it is advantageous in expanding decision region of minority category to identify boundary majority samples and then introduce them into the oversampling process.</p><p>In defining the boundaries, we require an algorithm that can automatically seek feasible itineraries from one class to the other. Ant colony optimization, developed by Colorni <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_2b\" data-range=\"ref44\">[44]</a>, has been reported to perform well in solving various discrete combinatorial optimization problems, such as traveling salesman problem <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_2b\" data-range=\"ref45\">[45]</a>, protein folding <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_2b\" data-range=\"ref46\">[46]</a>, fingerprint matching <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_2b\" data-range=\"ref47\">[47]</a>, fuzzy identification <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_2b\" data-range=\"ref48\">[48]</a>, map matching <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_2b\" data-range=\"ref49\">[49]</a>, route improving <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_2b\" data-range=\"ref50\">[50]</a>. This population stochastic search method was inspired by the communication of ants during the foraging process. Its ability to find an optimal path from nest to food in discrete space through iterative search is what we demand in defining the boundaries surrounding minority regions.</p><p class=\"has-inline-formula\">In this work, the ant system version proposed by Dorigo et al is adopted <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_2b\" data-range=\"ref51\">[51]</a> and in particular, the sight of ants is limited to 3 (i.e. an ant can only search in the area of 3 nearest points at once to figure out a way toward the food source). In each iteration of our ACO-based method, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> different majority samples are randomly chosen from the pre-defined overlapping region as well as one minority example. Then <i>na</i> ants are put on each majority instance and are ordered to search for the shortest path to that particular minority sample. We get <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> paths from which we can get to that minority sample from the majority area and then the last station in majority region is picked out and added to boundary set. After each minority sample has been taken as the destination once, the boundaries between majority and minority categories are constructed and described with a collection of majority samples. <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> demonstrates this process step by step, and pseudo-code of the establishment of boundaries based on ACO is listed as <a ref-type=\"algorithm\" anchor=\"alg2\" class=\"fulltext-link\">Algorithm 2</a>.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu3abc-3018911-large.gif\" data-fig-id=\"fig3\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu3abc-3018911-small.gif\" alt=\"FIGURE 3. - (a) Pick out three starting majority points (represented by magenta solid circles) and a destination in minority class (displayed by the red solid circle) (b) Generate a feasible path generated with ACO (represented by the black line) (c) Choose the last station in majority region as a boundary point for the particular minority sample (represented by the solid black circle).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>(a) Pick out three starting majority points (represented by magenta solid circles) and a destination in minority class (displayed by the red solid circle) (b) Generate a feasible path generated with ACO (represented by the black line) (c) Choose the last station in majority region as a boundary point for the particular minority sample (represented by the solid black circle).</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div><div class=\"algorithm rule-both\" id=\"alg2\"><h3>Algorithm 2 The ACO-Based Method to Define the Boundaries Between Classes in Extracted Overlapping Regions</h3><div class=\"alg-item label\"><span class=\"label\">1:</span><p class=\"has-inline-formula\"><b>Input</b>: Overlapping set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula>, number of ants on each node <i>na</i>, max iteration number <i>NC_max,</i> number of starting points the ACO-based method in majority region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">2:</span><p><b>Process</b>:</p></div><div class=\"alg-item label\"><span class=\"label\">3:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>: number of minority samples in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">4:</span><p class=\"has-inline-formula\">Randomly choose <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> different majority samples as \u2018nests\u2019</p></div><div class=\"alg-item label\"><span class=\"label\">5:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j=1: t$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">6:</span><p class=\"has-inline-formula\">Start from majority instance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${nest}_{j}$\n</tex-math></inline-formula> and <i>na</i> ants are simultaneously ordered to search for the shortest path to minority sample <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Min}_{i}$\n</tex-math></inline-formula> using ACO</p></div><div class=\"alg-item label\"><span class=\"label\">7:</span><p class=\"has-inline-formula\">After <i>NC_max</i> iterations, the optimal path is saved as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${path}_{i,j}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">8:</span><p class=\"has-inline-formula\">The last majority sample in this path is stored as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${boundary}_{i,j}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">9:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">10:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">11:</span><p class=\"has-inline-formula\"><b>Output</b>: A set of majority samples defining the borderline of minority region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${boundary}_{i,j}$\n</tex-math></inline-formula></p></div></div></p><p>It should be pointed out that, in our method, the rigid convergence of ACO is not a must since we aim at finding borderline points through which we can get into minority areas from the majority region. Once a feasible path is identified, we can find a majority sample locates on the boundary between two classes so it does not matter that whether this route is the optimal one or only one of the sub-optimal solutions.</p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. The Creation of Synthetic Samples by Oversampling Under Constraints</h3><p>In step 3, we construct the final training set by oversampling between the boundary samples and the minority instances in overlapping regions.</p><p>To reduce the generation of synthetic minority samples in majority regions, we impose distance constraints to the oversampling process. The aim of applying constraints to oversampling is to constrain new samples to the neighborhood of existing minority samples but not to that of majority samples.</p><p>Before oversampling, we shall deliberate over three questions: which samples are worthy of oversampling? How many sample points shall be created for each chosen minority sample? Where to place the generated minority samples? Question 1 is responded to in Subsection A and B of <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>. In terms of the second question, ADASYN <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2c\" data-range=\"ref26\">[26]</a> is integrated into our method to decide the number of new synthetic samples generated for each minority sample in overlapping regions. And question 3 can be answered by the constraints demonstrated as follows.</p><p>When a sample between a minority sample in the overlapping regions <i>Min</i> and a majority sample belonging to the boundary set <i>Maj</i> need to be generated, we denote the Euclidean distance between these two instances as <i>Dist(Min, Maj)</i>, the distance between <i>Min</i> and its <i>5<sup>th</sup></i> nearest neighbor as <i>Dist(Min, 5<sup><i>th</i></sup> NN of Min)</i> and that between <i>Maj</i> and its <i>5th</i> nearest neighbor as <i>Dist(Maj, 5<sup><i>th</i></sup> NN of Maj)</i>. The distance between the new synthetic sample and <i>Min</i>, <i>Dist</i>, is decided according to rules depicted in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>. \n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu4abc-3018911-large.gif\" data-fig-id=\"fig4\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu4abc-3018911-small.gif\" alt=\"FIGURE 4. - Rules to generate a new sample in different scenes (Green circles stand for majority samples; red pluses represent minority samples and blue squares are newly generated minority samples).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Rules to generate a new sample in different scenes (Green circles stand for majority samples; red pluses represent minority samples and blue squares are newly generated minority samples).</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p><p>After <i>Dist</i> is calculated, the new sample is synthesized according to:<disp-formula id=\"deqn1-deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Dir=&amp;(Maj-Min)/{norm~of~vector} (Maj-Min) \\qquad \\tag{1}\\\\ Min\\_{}new=&amp;Min+\\alpha \\ast Dirst\\ast Dir,\\quad \\alpha \\in (0,1)\\tag{2}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Dir=&amp;(Maj-Min)/{norm~of~vector} (Maj-Min) \\qquad \\tag{1}\\\\ Min\\_{}new=&amp;Min+\\alpha \\ast Dirst\\ast Dir,\\quad \\alpha \\in (0,1)\\tag{2}\\end{align*}\n</span></span></disp-formula> where <i>Min_new</i> refers to the synthetic instance, <i>Dir</i> is a direction vector. The illustrations of the location of the newly synthesized sample on different occasions are as in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>.</p><p>In Subsection A, we applied a 5-NN classifier to the original data set as recommended by Mani and Zhang <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_2c\" data-range=\"ref43\">[43]</a> to extract the overlapping area. For this data set, 5-NN has the best performance in classifying the original data. It means that 5-NN reflects the distribution characteristic of the original data very well when classifying the data. In the creation of synthetic samples for the minority samples, we increase the density of the minority samples in the overlapping area but the distribution characteristic of the original data should be inherited. Therefore, when we define the <i>Dist</i> the 5<sup>th</sup> NN of <i>Maj</i> or <i>Min</i> is chosen. In this way, when we classify the newly generated data <i>Min_new</i> it will be classified with a low error rate.</p><p class=\"has-inline-formula\">The whole procedure of this stage is described in detail as <a ref-type=\"algorithm\" anchor=\"alg3\" class=\"fulltext-link\">Algorithm 3</a>.<div class=\"algorithm rule-both\" id=\"alg3\"><h3>Algorithm 3 The Constrained Oversampling Method</h3><div class=\"alg-item label\"><span class=\"label\">1:</span><p class=\"has-inline-formula\"><b>Input</b>: Original data set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula>, borderline sample set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$boundary_{i,j}$\n</tex-math></inline-formula>, minority set in overlapping region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula>, the majority set in overlapping region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{maj}$\n</tex-math></inline-formula>, amount of SMOTE <i>N%</i>, number of nearest neighbors <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">2:</span><p><b>Process</b>:</p></div><div class=\"alg-item label\"><span class=\"label\">3:</span><p class=\"has-inline-formula\">Calculate the number of minority samples to be generated: <i>G=N% * number of minority samples in original data set</i> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">4:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>: <i>number of samples in</i> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">5:</span><p class=\"has-inline-formula\">Find <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> nearest neighbors based on the Euclidean distance and get the ratio: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$r_{i}$\n</tex-math></inline-formula> = <i>number of majority samples in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> nearest neighbors / <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula></i></p></div><div class=\"alg-item label\"><span class=\"label\">6:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">7:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>: <i>number of samples in</i> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">8:</span><p class=\"has-inline-formula\">Calculate the number of synthetic samples <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$g_{i}$\n</tex-math></inline-formula> that need to be generated for each minority sample according to: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$g_{i}=r_{i} \\ast G /$\n</tex-math></inline-formula> <i>(sum of all</i> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$r_{i})$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">9:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">10:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>: number of samples in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">11:</span><p class=\"has-inline-formula\">for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$u=1$\n</tex-math></inline-formula>: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$g_{i} $\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">12:</span><p class=\"has-inline-formula\">Except for samples that have been selected in the selected sample set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{selected}$\n</tex-math></inline-formula>, randomly choose a minority sample in overlapping region <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Min}_{i}$\n</tex-math></inline-formula>, retrieve its <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> corresponding majority samples on the borderline in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${boundary}_{i,j}$\n</tex-math></inline-formula>. Randomly pick one out and denote it by <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Maj}_{i}$\n</tex-math></inline-formula>.</p></div><div class=\"alg-item label\"><span class=\"label\">13:</span><p class=\"has-inline-formula\">Record the minority sample selected in this iteration to the selected sample set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{selected}$\n</tex-math></inline-formula> so that the samples in this iteration will not be re-selected in the next iterations.</p></div><div class=\"alg-item label\"><span class=\"label\">14:</span><p class=\"has-inline-formula\">Generate a new sample <i>Min_new</i> between <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Min}_{i}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Maj}_{i}$\n</tex-math></inline-formula> according to the rules depicted in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a></p></div><div class=\"alg-item label\"><span class=\"label\">15:</span><p class=\"has-inline-formula\">Add <i>Min_new</i> to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{new}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">16:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">17:</span><p>endfor</p></div><div class=\"alg-item label\"><span class=\"label\">18:</span><p class=\"has-inline-formula\">Merge <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{new}$\n</tex-math></inline-formula> and original set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula> to form a new training set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{balanced}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">19:</span><p class=\"has-inline-formula\"><b>Output</b>: New balanced training set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{balanced}$\n</tex-math></inline-formula></p></div></div></p><p>In the way mentioned above, to enhance learning on minority areas, newly generated minority samples are placed in overlapping regions where learning and predicting minority samples are difficult. Meanwhile, learning on majority regions is not hindered since these synthesized minority samples would not become noise points situated across the boundaries between classes. Our constrain-based oversampling method is powerful in handling class overlapping regions where traditional clustering-based methods do not perform well.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION III.</div><h2>Experiment</h2></div><p>In this section, we conducted experiments on five UCI datasets <a ref-type=\"bibr\" anchor=\"ref52\" id=\"context_ref_52_3\" data-range=\"ref52\">[52]</a> with different numbers of features and imbalance levels. To investigate the performance of the proposed method, six benchmark oversampling strategies described in <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>, namely original data without oversampling (Origin), SMOTE, Borderline-SMOTE1 (BOS1), Borderline-SMOTE2 (BOS2), ADASYN, CBOS were applied to the datasets as well as Constrained Oversampling (CO) and the results were compared. A well-known binary decision tree learner called Classification And Regression Tree (CART) <a ref-type=\"bibr\" anchor=\"ref53\" id=\"context_ref_53_3\" data-range=\"ref53\">[53]</a> was chosen as the test classifier in these experiments.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Datasets</h3><p>Among the five benchmark datasets from UCI Repository that were used in our tests, Pima and Haberman are composed of binary-class samples and the others are multi-class. In multi-class problems, we selected one of the classes as the minority class, and the remainders were merged into the majority class. <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> shows the characteristics of the five data sets, namely label of the minority class, number of attributes, number of minority instances and number of majority samples, sorted by the number of minority instances in ascending order. Particularly, imbalance level (IL) in the table refers to the ratio of the number of majority examples to that of minority examples.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nDatasets Description</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t1-3018911-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t1-3018911-small.gif\" alt=\"Table 1- &#10;Datasets Description\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Evaluation Metrics and Parameters</h3><p>The performance of a classifier can simply be shown by the \u201craw data\u201d produced during testing \u2013 the counts of correct and incorrect classifications. Generally, this information would be displayed in a confusion matrix as follows.</p><p class=\"has-inline-formula\">In <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>, <i>Tp</i> and <i>Tn</i> are the numbers of true positives and true negatives, respectively. <i>Fp</i> and <i>Fn</i> are the numbers of false positives and false negatives, respectively. <div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nConfusion Matrix for a Two-Class Problem</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t2-3018911-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t2-3018911-small.gif\" alt=\"Table 2- &#10;Confusion Matrix for a Two-Class Problem\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p class=\"has-inline-formula\">A widely adopted measurement of classification performance is overall accuracy calculated as follows:<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} {Overall} {Accuracy} =(Tp+Tn)/(Tp+Fn+Fp+Tn)\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} {Overall} {Accuracy} =(Tp+Tn)/(Tp+Fn+Fp+Tn)\\tag{3}\\end{equation*}\n</span></span></disp-formula> However, in imbalanced data sets, accuracy behaves poorly as a metric to evaluate the performance of classifiers over minority examples. When a data set is highly imbalanced, we attain a high accuracy even if all of the minority examples are misclassified. To overcome the problem mentioned above, other statistics such as <i>G-mean</i> and <i>F-measure</i> are also introduced into the assessment of classifiers. They are calculated as follows:<disp-formula id=\"deqn4-deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} G-mean=&amp;\\sqrt {Acc^{+}\\ast Acc^{-}} \\tag{4}\\\\ F-measure=&amp;(1+\\beta ^{2})\\ast Recall\\ast \\Pr ecision/(\\beta ^{2}\\ast Recall \\\\&amp;+\\,Precision)\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} G-mean=&amp;\\sqrt {Acc^{+}\\ast Acc^{-}} \\tag{4}\\\\ F-measure=&amp;(1+\\beta ^{2})\\ast Recall\\ast \\Pr ecision/(\\beta ^{2}\\ast Recall \\\\&amp;+\\,Precision)\\tag{5}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta $\n</tex-math></inline-formula> corresponds to relative importance of precision versus recall and is usually set to 1. Both <i>F-measure</i> and <i>G-mean</i> are values between 0 and 1. Larger <i>F-measure</i> and <i>G-mean</i> indicate better performance. And <i>Precision</i>, <i>Recall</i>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{-}$\n</tex-math></inline-formula> are further defined as:<disp-formula id=\"deqn6-deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Precision=&amp;Tp/(Tp+Fp) \\tag{6}\\\\ Recall=&amp;Acc^{+}=Tp/(Tp+Fn) \\tag{7}\\\\ Acc^{-}=&amp;Tn/(Tn+Fp)\\tag{8}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Precision=&amp;Tp/(Tp+Fp) \\tag{6}\\\\ Recall=&amp;Acc^{+}=Tp/(Tp+Fn) \\tag{7}\\\\ Acc^{-}=&amp;Tn/(Tn+Fp)\\tag{8}\\end{align*}\n</span></span></disp-formula> They are more reasonable indications of overall precision than accuracy due to some of their unique properties. One is that the magnitude of either <i>G-mean</i> or <i>F-measure</i> relies heavily on how the classifier performs on the minority class.</p><p class=\"has-inline-formula\">It is determined that in this paper, Overall Accuracy, <i>G-mean</i>, <i>F-measure</i>, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> would all be calculated to provide a comprehensive understanding of the performance of the classifier on given datasets. The initial parameters of the proposed method in this study are listed in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nParameter Settings for Constrained Oversampling</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t3-3018911-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t3-3018911-small.gif\" alt=\"Table 3- &#10;Parameter Settings for Constrained Oversampling\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Results</h3><p class=\"has-inline-formula\">For each dataset presented in Subsection A of <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>, the minority class was over-sampled at 100%, 200%, 300%, 400% and 500% of its original size. We conducted 3 independent 10-fold cross-validation experiments at each percentage of oversampling and the final results were the average value of 3 tests <a ref-type=\"bibr\" anchor=\"ref54\" id=\"context_ref_54_3c\" data-range=\"ref54\">[54]</a>. In each test, all 7 different oversampling strategies \u2014 Origin, SMOTE, BOS1, BOS2, ADASYN, CBOS, and CO were applied to the same datasets. Performances of different methods on datasets oversampled at 200% are shown in <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>, where four different metrics, <i>Overall Accuracy</i>, <i>G-mean</i>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula>, and <i>F-measure</i> are listed and compared. Other results are displayed in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a> to <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\"/><a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\"/><a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\"/><a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">9</a>.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nPerformance Comparison for the 7 Sampling Methods on Datasets Oversampled at 200%</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t4-3018911-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t4-3018911-small.gif\" alt=\"Table 4- &#10;Performance Comparison for the 7 Sampling Methods on Datasets Oversampled at 200%\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu5abcd-3018911-large.gif\" data-fig-id=\"fig5\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu5abcd-3018911-small.gif\" alt=\"FIGURE 5. - Experimental results on Glass (a) Overall Accuracy (b) G-mean (c) Acc+ (d) F-measure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Experimental results on Glass (a) Overall Accuracy (b) G-mean (c) Acc<sup>+</sup> (d) F-measure.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div> \n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu6abcd-3018911-large.gif\" data-fig-id=\"fig6\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu6abcd-3018911-small.gif\" alt=\"FIGURE 6. - Experimental results on Abalone (a) Overall Accuracy (b) G-mean (c) Acc+ (d) F- measure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Experimental results on Abalone (a) Overall Accuracy (b) G-mean (c) Acc<sup>+</sup> (d) F- measure.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu7abcd-3018911-large.gif\" data-fig-id=\"fig7\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu7abcd-3018911-small.gif\" alt=\"FIGURE 7. - Experimental results on Yeast (a) Overall Accuracy (b) G-mean (c) Acc+ (d) F- measure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Experimental results on Yeast (a) Overall Accuracy (b) G-mean (c) Acc<sup>+</sup> (d) F- measure.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu8abcd-3018911-large.gif\" data-fig-id=\"fig8\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu8abcd-3018911-small.gif\" alt=\"FIGURE 8. - Experimental results on Haberman (a) Overall Accuracy (b) G-mean (c) Acc+ (d) F- measure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>Experimental results on Haberman (a) Overall Accuracy (b) G-mean (c) Acc<sup>+</sup> (d) F- measure.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig9\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu9abcd-3018911-large.gif\" data-fig-id=\"fig9\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu9abcd-3018911-small.gif\" alt=\"FIGURE 9. - Experimental results on Pima (a) Overall Accuracy (b) G-mean (c) Acc+ (d) F- measure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 9. </b><fig><p>Experimental results on Pima (a) Overall Accuracy (b) G-mean (c) Acc<sup>+</sup> (d) F- measure.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3d\"><h3>D. Discussion</h3><p>We present the discussion of the results displayed above in two aspects: the impact of oversampling rate on the behavior of oversampling methods and the influence of class overlapping to the classifier and oversampling methods:</p><div class=\"section_2\" id=\"sec3d1\"><h4>1) The Impact of Oversampling Rate on the Behavior of Oversampling Methods</h4><p>Generally speaking, a larger oversampling rate leads to a more advantageous dataset for learning the minority samples. This point is justified by <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5 (c)</a> to <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig. 9 (c)</a>. However, the progress in the performance of predicting minority is usually at the cost of more misclassified majority samples. As is depicted in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5 (a)</a> and <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6 (a)</a>, more synthesized samples sometimes mean more unreliable or noise minority instances that fall into the majority region and lead to the misclassification of majority samples. So a larger oversampling rate in traditional oversampling methods may give rise to lower <i>Overall Accuracy</i>, <i>G-mean</i>, and <i>F-measure</i>.</p><p>As to our method in which we try to eliminate the generation of noise in the oversampling process, the figures show ascending performance when the oversampling rate becomes larger. However, this approach expands the decision region of the minority class in a mild manner and this characteristic leads to a relatively low performance when the oversampling rate is not large enough. When more samples are generated, our method is shown to be effective.</p></div><div class=\"section_2\" id=\"sec3d2\"><h4>2) The Influence of Class Overlapping to the Classifier and Oversampling Methods</h4><p>As mentioned in <a ref-type=\"sec\" anchor=\"sec1\" class=\"fulltext-link\">Section I</a>, we rest our method on the opinion that overlapping between classes is to be blamed for the loss of performance of learning systems in imbalanced datasets.</p><p>To examine this stand, we exhibit the characteristics of an overlapping region extracted with the method proposed in Subsection A of <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a> in <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a>. <div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nDescription of Overlapping Region in the Datasets</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t5-3018911-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu.t5-3018911-small.gif\" alt=\"Table 5- &#10;Description of Overlapping Region in the Datasets\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p class=\"has-inline-formula\">First, we explored the influence of class overlapping on the performance of our classifier. We note that generally <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> of CART on datasets in which most minority samples located inside the overlapping region is undesirable. This phenomenon can be easily accepted by observing <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig. 10</a> which shows the relationship between the overlapping ratio of minority sample and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> in different datasets. It can be seen that on Haberman, which has as many as 64.20% minority samples placed inside the overlapping area, our decision tree suffers from great loss in precision \u2014 only an <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> of 25.97% is achieved. On the contrary, on Pima, a dataset which possesses a similar imbalance level with Haberman but differs in the ratio of minority samples in overlapping region (this ratio is only 38.81% in Pima), our classifier turns out to be far more effective in recognizing minority samples by resulting in an <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula> of 56.01%. Similar situations happen to the changes of <i>G-mean</i> and <i>F-measure</i> on various datasets. \n<div class=\"figure figure-full\" id=\"fig10\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu10-3018911-large.gif\" data-fig-id=\"fig10\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu10-3018911-small.gif\" alt=\"FIGURE 10. - The relationship between the overlapping ratio of the minority sample and Acc+ in different datasets.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 10. </b><fig><p>The relationship between the overlapping ratio of the minority sample and Acc<sup>+</sup> in different datasets.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p><p>Based on the discussion above, we conclude that the performance of classifiers, at least CART, on an imbalanced dataset is heavily influenced by the overlapping ratio of minority samples. The kind of data distribution, which most minority samples in the dataset are positioned in the overlapping region, could well lead to performance degradation. This result verifies and extends the conclusion reached by Yu <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3d2\" data-range=\"ref29\">[29]</a> and Jo and Japkowicz <a ref-type=\"bibr\" anchor=\"ref55\" id=\"context_ref_55_3d2\" data-range=\"ref55\">[55]</a>.</p><p class=\"has-inline-formula\">Further, we investigated the influence of class overlapping on different oversampling methods. The relationship between the overlapping ratio of minority sample and <i>F-measure</i> in different datasets when oversampling rate equals 200% is shown for different oversampling strategies in <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig. 11</a>. In Haberman, Abalone, and Yeast, where the large overlapping ratio of minority samples in these datasets may give birth to a lot of synthetic noise samples. SMOTE, ADASYN, and CBOS, which do not take the noise generation around the borderline into consideration, bear a greater loss in <i>G-mean</i>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula>, and <i>F-measure</i> compared to the other methods. Borderline-SMOTE shows better performance because they are devised to strengthen learning on the boundaries. But Borderline-SMOTE is still undesirable since its assumption is somehow oversimplified in handling overlapping regions. Among these techniques, our CO method renders to be the best solution in datasets with serious class overlapping. \n<div class=\"figure figure-full\" id=\"fig11\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu11-3018911-large.gif\" data-fig-id=\"fig11\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9179814/liu11-3018911-small.gif\" alt=\"FIGURE 11. - The relationship between the overlapping ratio of minority sample and F-measure in different datasets when oversampling rate equals to 200%.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 11. </b><fig><p>The relationship between the overlapping ratio of minority sample and F-measure in different datasets when oversampling rate equals to 200%.</p></fig></div><p class=\"links\"><a href=\"/document/9179814/figures\" class=\"all\">Show All</a></p></div></p><p>Based on this observation, we believe that noise generation in oversampling imposes a negative effect on learning algorithms and the noise-prevention mechanism would improve the effect of oversampling methods.</p></div></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IV.</div><h2>Conclusion</h2></div><p>In this paper, we have proposed a new oversampling technique, Constrained Oversampling, to address the class imbalance problem, especially in datasets with overlapping between classes.</p><p>This technique is executed in three successive steps: extract overlapping regions based on the KNN algorithm; define boundary samples for each minority instance using ACO; synthesize the required amount of minority samples under constraints. Two major results are expected after this technique is applied: learning on the border is strengthened so that minority samples in imbalanced datasets are more easily recognized and few noise samples are introduced to the original dataset when oversampling the minority class in overlapping regions so that the performance would not be harmed by the generation of noise.</p><p class=\"has-inline-formula\">According to our experimental results, CO generally produces satisfying accuracy, <i>G-mean</i>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${Acc}^{+}$\n</tex-math></inline-formula>, and <i>F-measure</i> on various datasets differ with each other in imbalance level, number of features, and size. Further analysis of the results also reveals the impact of class overlapping on the performance of the classifier and oversampling strategies.</p><p class=\"has-inline-formula\">It should be noted that this study has examined only the influence of class overlapping in the imbalanced classification problem. To get a comprehensive understanding of the impact of other factors in imbalanced datasets, there are still a lot of works to be done. Meanwhile, there remains substantial room for future work considering the excessive computational and storage cost of the proposed method.\n<table><div class=\"header article-hdr\"><h2>NOMENCLATURE</h2></div>AbbreviationExpansion<tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha$\n</tex-math></inline-formula></td><td><p>Random number between 0 and 1</p></td></tr><tr><td>ACO</td><td><p>Ant colony optimization</p></td></tr><tr><td>ADASYN</td><td><p>Adaptive Synthetic Sampling</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta$\n</tex-math></inline-formula></td><td><p>Relative importance of precision versus recall</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$boundary_{i,j}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">Borderline of minority region defined by a set of majority samples from <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$nest_{j}$\n</tex-math></inline-formula> to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Min_{i}$\n</tex-math></inline-formula> in O</p></td></tr><tr><td>BOS1</td><td><p>Borderline-SMOTE1</p></td></tr><tr><td>BOS2</td><td><p>Borderline-SMOTE2</p></td></tr><tr><td>CBOS</td><td><p>Cluster Based Synthetic Oversampling</p></td></tr><tr><td>CART</td><td><p>Classification and Regression Tree</p></td></tr><tr><td>CO</td><td><p>Constrained Oversampling</p></td></tr><tr><td><i>Dir</i></td><td><p>Direction vector</p></td></tr><tr><td><i>Dist</i></td><td><p>Distance between the newsynthetic sample and <i>Min</i></p></td></tr><tr><td><i>Dist(Min, Maj)</i></td><td><p>Distance between a minority sample in the overlapping regions <i>Min</i> and a majority sample belonging to the boundary set <i>Maj</i></p></td></tr><tr><td><i>Dist(Min, 5<sup>th</sup> NN of Min)</i></td><td><p>Distance between <i>Min</i> and its <i>5<sup>th</sup></i> nearest neighbor</p></td></tr><tr><td><i>Dist(Maj, 5<sup>th</sup> NN of Maj)</i></td><td><p>Distance between <i>Maj</i> and its <i>5<sup>th</sup></i> nearest neighbor</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Fn$\n</tex-math></inline-formula></td><td><p>Numbers of false negatives</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Fp$\n</tex-math></inline-formula></td><td><p>Numbers of false positives</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$g_{i}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">the number of synthetic samples needs to be generated for the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> minority sample</p></td></tr><tr><td><i>G</i></td><td><p>Number of minority samples to be generated</p></td></tr><tr><td>IL</td><td><p>Imbalance level</p></td></tr><tr><td>KNN</td><td><p>K-nearest-neighbors based algorithm</p></td></tr><tr><td><i>Maj</i></td><td><p>Majority sample</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Maj_{i}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">Corresponding majority sample on the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$boundary_{i,j}$\n</tex-math></inline-formula> for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Min_{i}$\n</tex-math></inline-formula></p></td></tr><tr><td><i>Min</i></td><td><p>Minority sample</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Min_{i}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">The <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> minority sample</p></td></tr><tr><td><i>Min_new</i></td><td><p>The synthetic instance, the new generated <i>Min</i></p></td></tr><tr><td><i>na</i></td><td><p>Number of ants on each node</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$nest_{j}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">The <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j^{\\mathrm {th}}$\n</tex-math></inline-formula> nest for ACO</p></td></tr><tr><td><i>NC_max</i></td><td><p>Max iteration number</p></td></tr><tr><td><i>N%</i></td><td><p>Amount of SMOTE</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula></td><td><p>Overlapping set</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$path_{i,j}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">The optimal path from <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$nest_{j}$\n</tex-math></inline-formula> to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Min_{i}$\n</tex-math></inline-formula> in O</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$r_{i}$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">Synthetic Ration for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Min_{i}$\n</tex-math></inline-formula></p></td></tr><tr><td>ROS</td><td><p>Random Over Sampling</p></td></tr><tr><td>RUS</td><td><p>Random Under Sampling</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula></td><td><p class=\"has-inline-formula\">Original training set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula></p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{balanced}$\n</tex-math></inline-formula></td><td><p>New balanced training set</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{maj}$\n</tex-math></inline-formula></td><td><p>majority set in the overlapping region</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula></td><td><p>minority set in the overlapping region</p></td></tr><tr><td>SMOTE</td><td><p>Synthetic Minority Oversampling Technique</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{new}$\n</tex-math></inline-formula></td><td><p>New synthetic samples set</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{selected}$\n</tex-math></inline-formula></td><td><p>The minority sample selected in iterations of generating synthetic samples</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Tp$\n</tex-math></inline-formula></td><td><p>the numbers of true positives</p></td></tr><tr><td><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Tn$\n</tex-math></inline-formula></td><td><p>the numbers of true negatives</p></td></tr></table></p></div>\n</div></div></response>\n"
}