{
    "abstract": "Brain-inspired architectures are gaining increased attention, especially for edge devices to perform cognitive tasks utilizing its limited energy budget and computing resources. Hyperdimensional computing (HDC) paradigm is an emerging framework inspired by an abstract representation of neuronal circuits\u2019 attributes in the human brain. That includes a fully holographic random representation, high-d...",
    "articleNumber": "9354795",
    "articleTitle": "Hyper-Dimensional Computing Challenges and Opportunities for AI Applications",
    "authors": [
        {
            "preferredName": "Eman Hassan",
            "normalizedName": "E. Hassan",
            "firstName": "Eman",
            "lastName": "Hassan",
            "searchablePreferredName": "Eman Hassan",
            "id": 37088966908
        },
        {
            "preferredName": "Yasmin Halawani",
            "normalizedName": "Y. Halawani",
            "firstName": "Yasmin",
            "lastName": "Halawani",
            "searchablePreferredName": "Yasmin Halawani",
            "id": 37085544769
        },
        {
            "preferredName": "Baker Mohammad",
            "normalizedName": "B. Mohammad",
            "firstName": "Baker",
            "lastName": "Mohammad",
            "searchablePreferredName": "Baker Mohammad",
            "id": 37574033300
        },
        {
            "preferredName": "Hani Saleh",
            "normalizedName": "H. Saleh",
            "firstName": "Hani",
            "lastName": "Saleh",
            "searchablePreferredName": "Hani Saleh",
            "id": 37391220900
        }
    ],
    "doi": "10.1109/ACCESS.2021.3059762",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9354795/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION I.</div><h2>Introduction</h2></div><p>Advancements in deep learning (DL) algorithms has outperformed conventional machine learning (ML) approaches in many applications: image classification, voice recognition <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\" data-range=\"ref1\">[1]</a>, activity recognition <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\" data-range=\"ref2\">[2]</a> and object tracking <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\" data-range=\"ref3\">[3]</a>. Convolutional neural networks (CNN), such as AlexNet <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\" data-range=\"ref4\">[4]</a>, provide excellent classification accuracy at the cost of large memory storage, memory access, and computing complexity. ML has traditionally been implemented in the cloud and data centers like Big Blue, Google,.. etc. The need to move the processing of ML algorithms to edge devices is gaining importance for many reasons <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\" data-range=\"ref5\">[5]</a>. Firstly, some applications are sensitive to response time, such as an autonomous vehicle, since they cannot tolerate the network\u2019s latency. Secondly, security and privacy, as storing sensitive information on the cloud, is vulnerable to hackers and viruses. Third, moving the data to/from the cloud is costly in terms of power and resources. To this end, many critical-domain applications such as health care and autonomous vehicles found the intensive ML algorithms impractical for real-time edge devices <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\" data-range=\"ref7\">[7]</a>. Therefore, it is crucial to design an efficient algorithm to perform the cognitive tasks and specialized hardware to provide high efficiency for edge devices.</p><p>Inspired by the brain\u2019s computational abilities with the advancements in-memory technologies, hyper-dimensional (HD) computing has open new avenues as a potential light-weight classifier for resource systems to perform a diversity of cognitive tasks <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref8\">[8]</a>. Hyper-dimensional computing focuses on dimensional expansion rather than reduction by emulating the neuron\u2019s activity. However, due to the large size of the brain\u2019s circuit, the neural activity patterns can be modeled as a point in high dimensional space, that is, with hyper-vector <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\" data-range=\"ref9\">[9]</a>. For vectors of thousands of dimensions (e.g., &gt; 1000), it is called an HD vector.</p><p>This article focuses on reviewing the state-of-the-art (SOTA) designs exploiting the HDC paradigm for classification tasks. Also, it highlights the main operations of HDC and compares it to the SOTA computing paradigm for classification tasks, CNN. The main reasons for conducting such a study are to (1) provide an overview of several methodologies commonly used in HD computing to solve classical and new cognitive tasks. (2) describes existing HD architectures and highlight their implication on the system\u2019s overall performance. (3) the impact of HD vector size and training set size on the system accuracy is analyzed. (4) compare the HDC with the competing CNN paradigm in terms of efficiency and accuracy.</p><p>The outline of this paper is as follows: the data representation and the fundamental operations in hyper-dimensional space are highlighted in <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a> presents the core of the HD computing framework for cognitive tasks and some of the algorithms that have been designed to unlock their potential. HD hardware architectures and memory design details are drawn in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a>. <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a> reviews major current applications that employ HDC frameworks. <a ref-type=\"sec\" anchor=\"sec6\" class=\"fulltext-link\">Section VI</a> depicts a reasonable comparison between the SOTA CNN model and the HD model for solving classification tasks. The paper concludes with the pending challenges and possible future works in <a ref-type=\"sec\" anchor=\"sec7\" class=\"fulltext-link\">Section VII</a>.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION II.</div><h2>Background on Hyperdimensional Representation and Structure</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. HD Properties and Basic Operations</h3><p>In this section, the roots of HDC used to represent human memory, perception, and cognitive abilities are presented using the mathematical proprieties of HD space, which is built on rich linear algebra. Brain-inspired computing using HDC is carried on large components with independent, identical distribution (i.i.d) and could be binary or non-binary. Hyper-dimensional vector (HD) or symbolic vector architecture are common names for presenting data in high-space. The main inspiration behind HDC comes from Kanerva\u2019s work in <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\" data-range=\"ref10\">[10]</a> to represent entities using several thousand dimensions vectors and manipulate them utilizing the traditional computing style.</p><p class=\"has-inline-formula\">Consider a case of the binary representation of 10,000 dimension vectors in HD space, having this staggering number of unique vectors (2<sup>10000</sup>), it is unlikely that any system will require this number of vectors to represents its variables <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\" data-range=\"ref11\">[11]</a>. HD vectors are generated independently and randomly with equal probability for zeroes and ones. Using this assumption, the average Hamming distance between any two vectors in the space is 5000 bits. Also, most HD space is gathered around 50 bits of standard deviation (SD) in a binomial distribution. SD depends on the vector\u2019s size, which means that for large size vectors, the binomial distribution becomes thinner, and the majority of the space is concentrated around 0.5 normalized Hamming distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H_{n}$\n</tex-math></inline-formula>. Furthermore, it is important to ascertain the inherent <b>symbolic</b> nature of HD computing. One of the first observed extraordinary properties for HDC resides in its ability for analogical and hierarchical reasoning <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2a\" data-range=\"ref12\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\" data-range=\"ref13\">[13]</a>. Capitalizing on that, HD can be used to build complex data structures such as sequences <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\" data-range=\"ref10\">[10]</a>, images <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\" data-range=\"ref11\">[11]</a>, and lists <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2a\" data-range=\"ref12\">[12]</a>. In the HD domain, three main operations, namely; multiplication, addition, and permutation, referred to as (MAP) operations, are utilized for vectors modeling:\n<ul style=\"list-style-type:disc\"><li><p><b>Multiplication (Binding)</b>: is used to bind two HD vectors together, which is usually done using XOR bitwise operation. The output HD vector is orthogonal (dissimilar) to HD vectors being bound. The binding operation is invertible. Also, binding distributes over addition, preserving the distance between vectors.</p></li><li><p class=\"has-inline-formula\"><b>Addition (Bundling)</b>: is used to combine different HD vectors into a single HD vector. The resulting HD vector is similar to each component used in the bundling. The final HD vector is binarized using a bitwise sum threshold of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> vectors, which yields 0 when <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n/2$\n</tex-math></inline-formula> of bits in the vectors are zero, and 1 otherwise. That happens when the number of vectors in the set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is odd. For of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> being even (number of zeros equal number of ones), one more random HD vectors added to the set to break the ties <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2a\" data-range=\"ref14\">[14]</a>. Besides, the terms \u201cthreshold sum,\u201d \u201cmajority sum,\u201d and \u201cconsensus sum\u201d are used interchangeably to represent the resultant vector and denoted as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S =[A + B + C]$\n</tex-math></inline-formula>, where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A, B, C$\n</tex-math></inline-formula> are HD vectors.</p></li><li><p class=\"has-inline-formula\"><b>Permutation (Shifting)</b>: is used as an alternative approach to bind the HD vector with a special kind of matrix, called permutation matrix <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\" data-range=\"ref10\">[10]</a>. It is important for the data/sequence where the order is important. For example, it is often convenient to use a fixed permutation (denoted as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\Pi $\n</tex-math></inline-formula>) to bind the item\u2019s position, in a sequence, to an HD vector representing the value of the item in that position. Because permutation is merely re-order, it preserves the distance between vectors.</p></li></ul></p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. HD Pipeline for Classification Tasks</h3><p>A versatile HD machine should handle the variety of input data-types and scale with an incremented number of inputs without affecting its fidelity. It demands efficient encoding algorithms that affect the system performance and the memory optimization for hardware realization. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1</a> demonstrates the general HD pipeline for supervised classification tasks. It consists of three main components related to the essential HD pipeline stages.\n<ul style=\"list-style-type:disc\"><li><p>Item Memory (IM): mapping inputs to high space starts, as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1</a> with assigning unique HD seeds representation for the bases defined over the given application. For example, Latin letters are considered bases for language recognition <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2b\" data-range=\"ref10\">[10]</a>. Every basis in the set is assigned a randomly HD vector and saved in an item memory (IM). Seed vectors selection could be orthogonal (such as letters in language recognition). However, representing integers or continuous values in a particular order, in the same way is not appropriate. Therefore, a continuous item memory (CIM) is used where two close numbers have a small distance in their corresponding HD vectors and are considered semi-orthogonal <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2b\" data-range=\"ref15\">[15]</a>. It is important to stress that this representation is kept constant during training and inference phases for the intended classification task. Nevertheless, the memory storage for seed vectors cannot be avoided as it is required to retain the seeds during training and testing stages.</p></li><li><p>Encoding module: combines all encoded hyper-vectors in the training stage to form one HD vector representing each class. The same encoding model is used to map the query HD vector, which will be compared later with all other classes in the inference stage</p></li><li><p>Associative Memory (AM): stores all trained HD vectors to be used later for the inference. The essential function of AM is to compare the incoming encoded query HD vector with the stored classes and return the closest HD class vector using the appropriate similarity metrics. The two similarity measurements adopted in current HD encoding algorithms are the Hamming distance that utilizes XOR operation and the cosine similarity, which uses the inner product. Few works used the overlap between coded vectors for measuring the similarity between HD vectors <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2b\" data-range=\"ref16\">[16]</a>.</p></li></ul> The next section will focus on the encoding stage roots, types of encoders, and the main encoding designs available in the literature.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham1-3059762-large.gif\" data-fig-id=\"fig1\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham1-3059762-small.gif\" alt=\"FIGURE 1. - HD general pipeline with main stages including IM/CIM, where main input\u2019s features (integers/continuous) HD vectors are randomly generated and stored. Encoding module, where all features HD values and features HD position are combined using MAC operations to form an abstracted HD representation for the given input. In the training stage, all HD vector samplers are combined to generate a class prototype stored in associative memory for inference.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>HD general pipeline with main stages including IM/CIM, where main input\u2019s features (integers/continuous) HD vectors are randomly generated and stored. Encoding module, where all features HD values and features HD position are combined using MAC operations to form an abstracted HD representation for the given input. In the training stage, all HD vector samplers are combined to generate a class prototype stored in associative memory for inference.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION III.</div><h2>HD Encoding Stage and Architecture</h2></div><p>In HD, a universal encoder capable of mapping any arbitrary data-type into HD space does not exist. Each type of encoder should be able to map application-specific data, after a proper pre-processing, to a suitable architecture able to accomplish the cognitive/classification tasks. The first step towards a general HD architecture is to demonstrate essential stages of HD algorithms in an abstracted manner. Moreover, in this section, we highlight the main encoding architectures that have been discussed in the literature.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Common Roots for HD Encoding Algorithms</h3><p class=\"has-inline-formula\">Few simple techniques have been used repeatedly for HD encoding. <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> summarize these techniques in addition to the number of bundling stages used during encoding. Most of the HD applications, which will be analyzed in details next section, are using the following techniques\n<ul style=\"list-style-type:disc\"><li><p class=\"has-inline-formula\">Multi-set of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-sequence: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-gram is used for text classification and modeling sequences. In <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-gram, the original data is re-modeled as long chunks of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-sequences <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_3a\" data-range=\"ref10\">[10]</a>. After mapping <i>seeds</i> set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${S_{i}}\\triangleq \\{s_{1}, s_{2} \\ldots.,s_{n} \\}$\n</tex-math></inline-formula> into a hyper vectors <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{1}, V_{2},\\ldots V_{n}$\n</tex-math></inline-formula> and having a set of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-sequence [<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$(m_{1}, m_{2} \\ldots.,m_{n})| m_{i}$\n</tex-math></inline-formula>], they are encoded as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$(V_{m1})\\otimes \\Pi ^{1}(V_{m2}) \\otimes \\Pi ^{2}(V_{m3})\\ldots..\\otimes \\Pi ^{n-1}(V_{mn})$\n</tex-math></inline-formula>. Likewise, all remaining <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${n}$\n</tex-math></inline-formula>-sequences for a particular input are encoded and bundled to generate the HD representation for the particular sequence.</p></li><li><p class=\"has-inline-formula\">Features superposition: in this technique, feature vectors are extracted and mapped to hyper-vector. Assume that we have a feature vector with dimension <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$d~\\{f_{1}, f_{2} \\ldots,f_{d} \\}$\n</tex-math></inline-formula>. Then for each position in the feature vector <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i\\in d$\n</tex-math></inline-formula>, a <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W_{i}$\n</tex-math></inline-formula> hyper-vector is formed. Similarly, each value in the feature vector <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$f_{i}$\n</tex-math></inline-formula> is assigned hyper-vector <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\{U_{1}, U_{2} \\ldots,U_{d} \\}$\n</tex-math></inline-formula>. To correlate the value with position, the binding operation is used [<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sum _{i=1}^{d} U_{i}\\otimes W_{i}$\n</tex-math></inline-formula>]. In the final step, all <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> samples are encoded and super-imposed to get the final representation of the feature [<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sum _{j=1}^{n}\\sum _{i=1}^{d} U_{i}\\otimes W_{i}$\n</tex-math></inline-formula>].</p></li></ul><div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nHD Mode Based on Main Encoding Techniques and Number of Superposition</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t1-3059762-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t1-3059762-small.gif\" alt=\"Table 1- &#10;HD Mode Based on Main Encoding Techniques and Number of Superposition\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p class=\"has-inline-formula\">For classification tasks, the final step in the HD encoding module is the bundling of all individual representations <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_3a\" data-range=\"ref17\">[17]</a>. Building on that, HD algorithms can be divided into two types according to the number of bundling used during HD processing:\n<ul style=\"list-style-type:disc\"><li><p class=\"has-inline-formula\">A single-stage algorithm, where the bundling operation used only once. Each term in the bundled vector is formed by binding its inputs and/or their permutation. In other words, <disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} {I} = \\sum _{i=1}^{K} f_{i}(\\mathbb {I}) \\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} {I} = \\sum _{i=1}^{K} f_{i}(\\mathbb {I}) \\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\mathbb {I}$\n</tex-math></inline-formula> represents the input stream of finite dimension of HD vectors <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\mathbb {I} \\triangleq \\{X = x_{1}, x_{2} \\ldots.,x_{n} \\xrightarrow {}HDV\\}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> represents the number of terms in the class, and the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>th term in <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Eq.1</a> is represented by <disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} f_{i}(\\mathbb {I})=&amp;(X_{b1} \\otimes X_{b2}\\ldots \\ldots \\otimes X_{bn}) \\\\&amp;\\otimes (\\Pi ^{p1}(X_{s1})\\otimes \\Pi ^{p2}(X_{s2})\\ldots..\\otimes \\Pi ^{pn}(X_{sn})) \\tag{2}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} f_{i}(\\mathbb {I})=&amp;(X_{b1} \\otimes X_{b2}\\ldots \\ldots \\otimes X_{bn}) \\\\&amp;\\otimes (\\Pi ^{p1}(X_{s1})\\otimes \\Pi ^{p2}(X_{s2})\\ldots..\\otimes \\Pi ^{pn}(X_{sn})) \\tag{2}\\end{align*}\n</span></span></disp-formula> Each term <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$f_{i}(\\mathbb {I})$\n</tex-math></inline-formula> depends on certain input value, some occurring by only using the binding operation <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${B_{i}}\\triangleq \\{b_{1}, b_{2} \\ldots.,b_{n} \\}$\n</tex-math></inline-formula> according to their position in the set {X}, and some requires permutation (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\Pi $\n</tex-math></inline-formula>), where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\{p_{1}, p_{2} \\ldots.,p_{n} \\}$\n</tex-math></inline-formula> is positive integers account the permutation.</p></li><li><p>A multi-stages algorithm uses the output of a single-stage algorithm as an input to another single-stage-HD algorithm. Similarly, we can construct any multi-stage HD architecture by combining smaller single-stage algorithms in a hierarchical manner. The multi-stage algorithm defines the complex non-linear relationship between variables such as time, position, and value. It is important to mention that only a few researchers <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_3a\" data-range=\"ref18\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_3a\" data-range=\"ref19\">[19]</a> are using the multi-stages HD architecture for cognitive/classification tasks, while the majority are focusing on using the single-stage algorithm for its simplicity.</p></li></ul></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. The Main Encoding Schemes</h3><p>There are several forms of mapping the data from its original space to HD-space and are classified as follows:\n<ul style=\"list-style-type:disc\"><li><p>Binary: where the value of HD elements are {0,1}.</p></li><li><p>Ternary: in which HD elements take values of {0,1,\u22121}.</p></li><li><p>Non-binary HD vector: where elements in such vector are represented using fixed-point, complex, or floating-point number.</p></li><li><p>Dense HD vector: in the dense HD vector, all HD elements have an equal distribution probability.</p></li><li><p>Sparse HD vector: in the sparse model, the 0 value dominates the HD vector with low presence for 1 or \u22121.</p></li></ul></p><p>Authors in <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_3b\" data-range=\"ref20\">[20]</a> proposed a sparse random binary HD vector utilizing permutation-based binding that operates on a segmented vector, which resulted in efficient-energy deployment. Introducing sparsity in the HD vector reduces the number of multiplications and additions required to encode the single HD vector. Holographic Graph Neuron (HoloGN) for one-pass pattern learning was introduced in <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_3b\" data-range=\"ref21\">[21]</a>. The sparse code representation of the pattern improves noise resistance in the architecture compared to the original HoloGN abstracted representation, leading to a tangible improvement in pattern accuracy. In <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_3b\" data-range=\"ref14\">[14]</a>, the author tested a synthesized and real-world data using two types of mapping (projection): orthogonal and distance preserving. In the first type, each symbol is assigned a unique random HD vector that is kept fixed over the system\u2019s life. For the second type of mapping, the features\u2019 value is quantized to a fixed number of levels m <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_3b\" data-range=\"ref22\">[22]</a>. Each unique feature is associated with the corresponding distributed HD vector considering different mapping preserving mechanisms: linear mapping, approximate linear mapping, and non-linear approximate mapping. The approach proved that the sparse and dense mapping using different mapping mechanisms showed nearly identical performance classification tasks.</p><p>Likewise, SparseHD in <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3b\" data-range=\"ref23\">[23]</a> explored the possibility of sparsity in hyper-vectors to improve the HD computing efficiency and reduce the computations required for inference. SparseHD, which enforces sparsity either dimension-wise or class-wise, takes the conventional trained HD model in non-binary representation and feeds it to a Model Sparser (MS). MS drops S% of least significant features in each class\u2019s trained HD model. Works in <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_3b\" data-range=\"ref6\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b\" data-range=\"ref24 ref25 ref26\">[24]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_3b\" data-range=\"ref24 ref25 ref26\">[26]</a> proposed a novel encoder that quantizes a continuous range of inputs (e.g., input range {\u22121, 1}) into M levels and then maps it into HD vector using the linear mapping technique mentioned in <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_3b\" data-range=\"ref22\">[22]</a> and stored in CIM. It was demonstrated that the quantization technique associated with CIM improved the encoder accuracy, and its implementation was hardware friendly when the range of scalar values is known. Besides, the retraining step was implemented in the training stage to update the AM and improve the classification accuracy for supervised learning by testing the classifier accuracy over the training set.</p><p>The encoding approach used in the brain-inspired classifier (BRIC) <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b\" data-range=\"ref25\">[25]</a> used locality-based sparse random projection, which is based on Locality Sensitive Hash algorithms. The generated HD vector for each feature value is represented by the sign of dot product between the feature vector in the n dimension and the random projected vector in the D dimension using the n-gram window. Also, a pre-determined index s in the projection matrix is selected to be non-zero, which creates a spatial locality pattern that the hardware can take advantage of. The AM is incrementally updated using the retraining approach. A generic hyper-vector manipulator (MAN) module was demonstrated in <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_3b\" data-range=\"ref27\">[27]</a> with cheaper logical operations to re-materialize them later. Furthermore, it allows the representational space to stay in a binary format by applying back-to-back bundling, which is fundamental for on-chip learning. FACD encoding module was suggested in <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3b\" data-range=\"ref28\">[28]</a> for non-binary HD representation and includes three main steps: model training, model refinement, and inference. In the refinement stage, a non-linear K-mean clustering has been applied on the trained class HD vectors to find the best centroids representing the distribution of the values in each HD class.</p><p>CompHD in <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_3b\" data-range=\"ref8\">[8]</a>, reduces the HD dimension intelligently in a way that does not sacrifice the system\u2019s accuracy. The training HD vectors are divided into segments s, each of length d=D/s, after mapped them to Hyperspace. The positional information for each segment is preserved using the Hadamard matrix <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3b\" data-range=\"ref29\">[29]</a>, and all segments are added up to form one compressed HV model for each class. This method reduces the cost of cosine similarity for associative search by reducing multiplication and addition operations. However, using many segments would increase the data to noise ratio and affect the model accuracy.</p><p>SemiHD in <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_3b\" data-range=\"ref30\">[30]</a> is an application-based model that trade accuracy-for-efficiency</p><p>A fully binarized SearcHD algorithm was discussed in <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b\" data-range=\"ref24\">[24]</a> to reduce the number of addition operations in the training stage by generating multiple binary HD vectors for each class N. SearcHD stochastically sharing the query HD vector elements with each HD class by exploiting bitwise substitution Though this algorithm increases the memory access overhead, which is introduced by bitwise substitution, it accumulates the training data more intelligently.</p><p class=\"has-inline-formula\">In the AdaptHD, the author utilized the author same concept of retaining the HD model used in <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b\" data-range=\"ref25\">[25]</a>. However, a learning rate <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> factor was introduced to speed the model convergence during the training phase. Two methods are proposed for adaptive retraining; iteration-dependent and data-dependent. However, the model sensitivity to a minor change in learning rate would result in a misleading accuracy, especially for noisy data.</p><p>Likewise, BinHD was proposed in <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_3b\" data-range=\"ref31\">[31]</a> which enables the HD model to be trained and tested in binary fashion using binary accumulators <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_3b\" data-range=\"ref32\">[32]</a>.</p><p>Recently, a new technique was proposed in <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_3b\" data-range=\"ref33\">[33]</a>, namely, QubitHD to eliminate the overhead of floating-point representation and reduce the gap between the classification accuracy of binarized and non-binarized HD classifiers. It exploits the principle that the information needs to be stored in a quantum bit (Qubit) before its measurement. The algorithm is based on QuantHD in <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3b\" data-range=\"ref34\">[34]</a>. However, it enables an efficient binarization of the HD model during the retraining stage using quantum measurement techniques.</p><p>An unsupervised learning algorithm, HDCluster in <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_3b\" data-range=\"ref35\">[35]</a> was investigated for clustering input data in HD space by fully mapping and processing clusters in memory.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IV.</div><h2>General HD Processor</h2></div><p>From the above abstraction, it is clear that the HD general processor has two main components: 1) the encoder, which is the only component that needs to be programmed for a particular application. However, the AM design can be optimized for more efficient search implementation. 2) the Data-flow is one-directional, where all input HD vectors usually flow from item memory to the encoder and end in the associative memory One-pass learning named in <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_4\" data-range=\"ref14\">[14]</a> is also given to the same Data-flow. An example of all the complete HD processor framework elements is illustrated in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a> for hand digit classification task.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham2-3059762-large.gif\" data-fig-id=\"fig2\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham2-3059762-small.gif\" alt=\"FIGURE 2. - General HD processor, for MNIST dataset classification. It consists of IM for storing seeds, an encoder to map data to HD space, AM for storing HD classes, and similarity measurements for the inference stage.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>General HD processor, for MNIST dataset classification. It consists of IM for storing seeds, an encoder to map data to HD space, AM for storing HD classes, and similarity measurements for the inference stage.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div></p><p>Data-parallel architectures are the potential candidate for HD systems since parallelism exists in HD operations. In both multiply and addition operations, the vector\u2019s resulting element depends only on the corresponding elements of its operands. The result vector element depends on the nearby operand element for the permutation operation. It is clear that for any fixed-width architecture less than the D=1000 dimension, it will be ineffectual to process the large intermediate HD vectors stored in the costly internal memory. Besides, a redundant computation is needed in permutation operation due to intra-word dependencies. To that end, the fundamental operations and inherent robustness in HDC make it a good candidate for data-flow based array architecture <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_4\" data-range=\"ref17\">[17]</a>. Besides, the manipulation of large patterns stored in the memory makes HDC a potential candidate for an emerging in-memory computing paradigm or computational memory based on nanoscale resistive memory or memristor devices <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_4\" data-range=\"ref36\">[36]</a>.</p><p>Furthermore, the HD computing characteristics inimitably matches with the inherent abilities of the FPGA. Therefore, efficient hardware FPGA implementations were proposed to speed-up the HD model during the training (inference) stage or improve the computational cost.</p></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION V.</div><h2>Benchmark Applications</h2></div><p>To date, the evolution of HD computing is still in the research domain, and the number of applications using HD computing for solving real-time problems is limited. However, there is a positive tone as the number of research advancing new applications for HD is increasing. In this section, the main works utilizing HD for cognitive tasks, grouped according to the data structure, are reviewed. We start with 1D data structure applications (sequences or vectors) such as voice recognition, text classification, bio-signal, and time-series applications. Then we highlight some tasks that pertain to the 2D structure data type and show how the HD computing approach provides a means of encoding visual scenes such as frame-based images as well as neuromorphic sensors output.</p><div class=\"section_2\" id=\"sec5a\"><h3>A. One-Dimension (1D) HD Applications</h3><div class=\"section_2\" id=\"sec5a1\"><h4>1) Random Indexing, Text Classification, and Language Recognition</h4><p>Random Indexing (RI) is one of the oldest and well-known applications used to study the relationship between the words in the language <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_5a1\" data-range=\"ref37\">[37]</a>. The RI method introduced the term \u201cindex vector,\u201d which assigns a sparse ternary vector for each document and semantic vector for every word. In the original RI, only two HD components have been used: random representation and bundling operation. In <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_5a1\" data-range=\"ref38\">[38]</a>, permutation operation was added to the encoding stage for semantic representation. For more works on RI approaches, processing and performance, the reader can refer to <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_5a1\" data-range=\"ref39\">[39]</a>, <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_5a1\" data-range=\"ref40\">[40]</a>.</p><p>A promising result is revealed in <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_5a1\" data-range=\"ref41\">[41]</a> for identifying language in text documents. The statistic of n-gram letters in a sentence is encoded using HD permutation, binding, and bundling operations. This method was tested on 21 languages using tri-gram letters utilizing Project Gutenberg Hart and Wortschatz Corpora. This approach accuracy surpassed the baseline ML learning methods used for the same applications.</p><p class=\"has-inline-formula\">In <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_5a1\" data-range=\"ref42\">[42]</a>, HD sparse vectors were used to store the symbol\u2019s occurrence statistics in a Willshaw associative memory using stochastic counters. Such a framework is useful for on-line learning applications where the system keeps learning with incoming data. Work in <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_5a1\" data-range=\"ref43\">[43]</a> proposed a bag-of-letters mapping scheme that helps identify a valid word in the dictionary given a permuted letters of the word and using Hamming distance as a similarity metric. The permuted text of the \u201cCambridge test\u201d was used as input. The method showed a good potential in reconstructing the original words. However, depending on the letter frequency in the world, a kind of bias is revealed when measuring similarity. A novel method for extracting the common sub-string in two strings with length <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L_{1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L_{2}$\n</tex-math></inline-formula> using binary HD vectors was proposed in <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_5a1\" data-range=\"ref44\">[44]</a> utilizing the same n-gram technique. For sequence prediction, work in <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_5a1\" data-range=\"ref45\">[45]</a> described a prediction model based on HDC exploiting the SDM system. The model used k consecutive points to predict the entire sequence. The prediction\u2019s rate was limited to the memory capacity and the dimension HD vector.</p></div><div class=\"section_2\" id=\"sec5a2\"><h4>2) Sensory Inputs Recognition</h4><p>The work in <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_5a2\" data-range=\"ref46\">[46]</a> uses HD\u2019s principles for modeling the dependencies between parallel multivariate inputs and proposes an HD-based predictor (HDCP). The approach\u2019s objective is to predict the future state of the sequence for the stream given their previous states. HDCP was tested on activity recognition tasks using data from different body sensors in the Palantir Context Data Library. The system performance outperformed the ML start-of-the-art results and showed its ability to account for differing reliability of different sensor inputs. However, in designing HDCP, a special memory architecture should be considered, such as sparse distributed memory (SDM) and not the linear additive memory <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_5a2\" data-range=\"ref10\">[10]</a>.</p><p>A similar approach was used in <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_5a2\" data-range=\"ref47\">[47]</a> for sequence prediction. But instead of dense bipolar HD vectors, this model used the sparse distributed HD coding to represent time-dependent structures using ternary values. The approach, which is called Sparse Distributed Predictor (SDP), was tested on real-time mobile phone users data, which used to predict the next application launched, next music playback logs and, next GPS location of the user.</p><p>VoiceHD was proposed in <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_5a2\" data-range=\"ref6\">[6]</a> for speech recognition. Their approach focused on transforming a collection of voice sets called the Isolate dataset into the frequency domain using the Mel-frequency Cepstral coefficients (MFCCs) mechanism. The proposed encoder applied for N frequency bins, which are all bundled to a single hyper-vector class representing the intended voice. To overcome the capacity issue in HDC, the design suggested retraining the AM memory by an incremental update to improve the classification accuracy. VoiceHD was five times faster in execution than the conventional DNN during training and testing.</p></div><div class=\"section_2\" id=\"sec5a3\"><h4>3) Biomedical (Body-Sensing) Applications</h4><p>The fast growth of efficient electronics enables a major enhancement in wearable and portable health care systems. Building on that, utilizing the HD framework in the biomedical domain represents an active research area. Works in <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_5a3\" data-range=\"ref18\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_5a3\" data-range=\"ref48\">[48]</a> developed a full set of HD templates that comprehensively encode different types of bio-signals like EMG, EEG, and ECoG for multi-class learning and classification. Authors in <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_5a3\" data-range=\"ref18\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_5a3\" data-range=\"ref48\">[48]</a> extended the encoder in <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_5a3\" data-range=\"ref19\">[19]</a> to process simultaneous analog bio-signal inputs using continuous mapping method used in <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_5a3\" data-range=\"ref49\">[49]</a>. The proposed approach used dense bipolar HD vectors representation for dense sensors <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_5a3\" data-range=\"ref48\">[48]</a>. The HD classifier learns 3 times faster than the SVM method, with accuracy higher than the SVM using the full training set.</p><p>In <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_5a3\" data-range=\"ref50\">[50]</a>, authors provided an efficient binarized algorithm for fast classification of human epileptic seizures using EEG signals <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_5a3\" data-range=\"ref51\">[51]</a> and the region of the brain that generates them using brain-inspired HD computing. Though the performance of the algorithms mentioned above surpasses the traditional ML methods, they are only limited to short-term signal recording. They require data processing, which would increase hardware complexity. Reference <a ref-type=\"bibr\" anchor=\"ref52\" id=\"context_ref_52_5a3\" data-range=\"ref52\">[52]</a> proposed a Laelaps algorithm to solve the issue for long-term recording signal and operates with end-to-end binary operations to avoid expensive fixed or floating-point arithmetic.</p><p>HD Computing-based Multimodality Emotion Recognition (HDC-MER) is explored in <a ref-type=\"bibr\" anchor=\"ref53\" id=\"context_ref_53_5a3\" data-range=\"ref53\">[53]</a> from physiological signals. The real-valued features of GSR, ECG, and EEG were extracted and mapped to dense HD binary vectors using a random non-linear mapping function. HDC-MER classifier\u2019s accuracy surpassed the extreme gradient boosting (XGB) method using only (1/4) training data.</p><p>Heart rate response during deep breathing (DB) was analyzed in <a ref-type=\"bibr\" anchor=\"ref54\" id=\"context_ref_54_5a3\" data-range=\"ref54\">[54]</a> using the principle of HDC. Both Heart rate and respiratory signals were synthesized and recorded from real health patients and modeled using Fourier series analysis to extract the desired features. These features are mapped to HD vectors using non-linear approximate mapping <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_5a3\" data-range=\"ref14\">[14]</a>. The proposed method using HDC was able to identify signals with low cardio-respiratory synchronization during DB due to arrhythmias or when the evaluation of autonomic function using DB test <a ref-type=\"bibr\" anchor=\"ref55\" id=\"context_ref_55_5a3\" data-range=\"ref55\">[55]</a> is considered as a problem.</p><p>HDC is relatively new in DNA profiling and has a great potential to play a significant role in taxonomic identification. In <a ref-type=\"bibr\" anchor=\"ref56\" id=\"context_ref_56_5a3\" data-range=\"ref56\">[56]</a>, the hyper-dimensional concept is applied to represent the DNA nucleotides: adenine (A), guanine (G), cytosine (C), and thymine (T), which are existed in the DNA molecules in a particular order, for sequences classification. The HD algorithm was tested on different data set and showed an outstanding performance compared with conventional ML algorithms such as KNN and SVM.</p></div></div><div class=\"section_2\" id=\"sec5b\"><h3>B. Two-Dimension (2D) HD Applications</h3><p>Processing visual data in HD space seems to be the newest application domain, and the least examined one. In <a ref-type=\"bibr\" anchor=\"ref57\" id=\"context_ref_57_5b\" data-range=\"ref57\">[57]</a>, HDC was used to represent the structured combination of features detected by a perceptual circuitry from a visual scene. The system structure is displayed through functional imitation of the learning concept occurring in honey bees. The encoding module A visual question answering (VQA) system was proposed in <a ref-type=\"bibr\" anchor=\"ref58\" id=\"context_ref_58_5b\" data-range=\"ref58\">[58]</a> in which the machine must infer an answer about the provided image.</p><p>The architecture consists of two parts; the first part maps each image in the data set into bipolar 1000 dimension HD vector using two layers of forward fully connected (FFW) neural network. The second part contains the seeds of HD vectors stored in IM and the related five questions described in HD format as well. The system was queried on unseen images for the five questions the accuracy was limited to (60%\u201372%) for new images that were not available during the training.</p><p>In <a ref-type=\"bibr\" anchor=\"ref59\" id=\"context_ref_59_5b\" data-range=\"ref59\">[59]</a>, a holographic reduced representations method was used to convert an input image into an HD vector. Interesting property in this model is its ability to perform a continuous mapping from positions to HD vectors to preserve the distance between the HD vectors. The examined experiments demonstrated good performance on a simple visuospatial inference task with an accuracy of 95% as well as on a 2D navigation task.</p><p>Cellular Automata (CA) based HDC was used for an analogy reasoning in <a ref-type=\"bibr\" anchor=\"ref60\" id=\"context_ref_60_5b\" data-range=\"ref60\">[60]</a>. The proposed method extracts features from an image using a neural network, which is then expanded into binary non-random HD vectors using CA 90 rule <a ref-type=\"bibr\" anchor=\"ref61\" id=\"context_ref_61_5b\" data-range=\"ref61\">[61]</a>. A similar approach was used in <a ref-type=\"bibr\" anchor=\"ref62\" id=\"context_ref_62_5b\" data-range=\"ref62\">[62]</a> for medical image classification. The proposed classifier was assessed using IM- AGE CLEF2012 data set and collections from the web. Even though the classifier\u2019s performance was competing with conventional CNN and BOVW classifiers, it had variations due to the random permutation of data fed into the CA grid in each iteration.</p><p class=\"has-inline-formula\">Recent work in <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_5b\" data-range=\"ref11\">[11]</a> bridge the gap between perception and action in robotic applications using HD binary vector (HBV) as a currency to produce the \u201cmemories\u201d concept of previous events. The work target was to find the associative velocity given time image, using only memories. The time-image is mapped to HBV by first constructing an intensity space containing values from [0\u2013255] encoded as a binary HD space with 8k vector length for each intensity. Both time image HBV and velocity HBV vector are bounded into a single HD space to create action-perception space. The proposed method was tested on ego-motion estimation in the autonomous vehicle using the MVSEC dataset. The results showed comparable performance with the traditional CNN approach by applying less training data. However, the data density to be represented using HBV could limit the HD performance. A concise summary of main works that exploit HDC for various applications, its encoding module, and main hardware implementations is highlighted in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">table 2</a>.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nSummary Table for the Main Works on HDC Including Main Encoding Algorithms, HD Vector Representation, Experimental Results and Hardware Implementations</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t2-3059762-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t2-3059762-small.gif\" alt=\"Table 2- &#10;Summary Table for the Main Works on HDC Including Main Encoding Algorithms, HD Vector Representation, Experimental Results and Hardware Implementations\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>The next section will compare HDC and CNN in terms of the number of operations and accuracy for 1D and 2D applications.</p></div></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION VI.</div><h2>Case Studies on HDC and CNN</h2></div><div class=\"section_2\" id=\"sec6a\"><h3>A. HDC Accuracy as a Function of Training Dataset Size</h3><p class=\"has-inline-formula\">This section studied both HD and CNN for popular data set and compared the 2-approaches in terms of accuracy, computing complexity, and overall performance. We implemented both HDC and CNN approaches independently and then analyzed the results to guide the selection based on target needs. MNIST dataset was used in this work for HDC-based classification. MNIST is a well-known dataset with wide varieties for digit representation. For the HDC model, each pixel position, in (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$28\\times 28$\n</tex-math></inline-formula>) image is assigned a unique HD vector. Therefore, 784 randomly generated vectors act as seeds stored in IM and fixed over the system\u2019s life. D is assumed to be equal to 10k. To preserve the pixel\u2019s location in the image, each pixel HD vector bounded with its intensity. The gray-scale images are converted to black and white (BW) for simple representation. This means that 0 denotes a black pixel intensity, and 1 denotes a white pixel intensity for a particular image. This procedure is called orthogonal mapping, which was inspired from <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_6a\" data-range=\"ref14\">[14]</a> and modified according to the targeted task. The mapping procedure for the MNIST dataset is performed using the following steps and highlighted in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>:\n<ul style=\"list-style-type:disc\"><li><p>Initialize the HD Mapper\n<ul style=\"list-style-type:disc\"><li><p>Set the dimensionality D of the HD vectors.</p></li><li><p>Flatten input images, convert them to binary, and set the features\u2019 number. The digit image consists of 784 pixels, where each pixel represents a feature.</p></li><li><p>Initialize the seeds randomly. For every feature i, a dense binary random HD vector will be generated; 748 dissimilar vectors are stored in the IM.</p></li></ul></p></li><li><p>Encoding mechanism: for each pixel position, its corresponding HD vector is bound by the value of that pixel (the random HD vector for the pixel position is multiplied by a special matrix, called permutation matrix, where it performs a 1-bit shift for black and 0-bit shift for white)</p></li><li><p>Generate the HD distributed representation of the digit by applying <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Eq.(1)</a>.</p></li><li><p class=\"has-inline-formula\">Training stages: combine HD vectors, for all similar samples, into a single pattern binary representation <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H_{b}$\n</tex-math></inline-formula> using the majority sum operation: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H_{b} =\\left[{\\sum _{i=1}^{n} ({I})}\\right] $\n</tex-math></inline-formula>, where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$[]$\n</tex-math></inline-formula> refers to the majority sum operation.</p></li><li><p>Store the binary representation for each digit (class prototype) in the AM.</p></li></ul> During the testing phase, the same mapping procedure is used to generate the query HD vector. Later, this encoded binary HD vector is compared to the stored HD representations (classes) in the AM through the Hamming distance similarity measurement.</p><p>Here, we study the effect of the training sets size for the MNIST dataset on classification accuracy. In the first experiment, we used series of randomly selected 50 images for each digit. All the HD vectors of the presented image for the particular digits were combined to form a single HD representation of that digit(class). Thus, by the end of the training phase, the associative memory contains 10 HD vectors, each mutually representing all digits\u2019 variations. We repeated the experiment for 100, 500, 1000, 2000, 3000, 4400, and 5200 samples for each digit. During the testing phase, 1000 new images for all digits were used as input. The overall accuracy was measured as the percentage of the correctly classified digits averaged over the test set size.</p><p><a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a> illustrates that the obtained accuracy for small sets starts small and then increases in an approximately linear fashion until it hits an upper limit slightly less than 90% accuracy. After that, the classification accuracy starts to decrease. That is due to the majority sum operation, which generates the class binary representations of the observed patterns. It imposes a limit on the number of HD vectors included in the sum, above which robust decoding of the individual operands becomes very difficult <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_6a\" data-range=\"ref14\">[14]</a>. This explains accuracy behavior when the number of patterns increased above a specific limit. Most HDC research focuses on overall accuracy to measure the system\u2019s performance. We added other metrics to measure performance, such as Precision and Recall for this work. Precision is the classifier\u2019s ability to identify only the relevant samples in the dataset. While Recall refers to the system\u2019s ability to identify all pertinent instances of the dataset <a ref-type=\"bibr\" anchor=\"ref79\" id=\"context_ref_79_6a\" data-range=\"ref79\">[79]</a>. The results show that both Recall (86.5%) and the Precision (85%) were achieved in retrieving the HDC distributed representations based method. We found that the accuracy, precision, and recall values are in the same range. Further analysis for other metrics such as F1-score (86%), MCC-Matthews correlation coefficient (0.85), and kappa-Cohen\u2019s kappa (0.81) reveals the robustness of the classifier and that the system utilizes a balanced dataset.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham3-3059762-large.gif\" data-fig-id=\"fig3\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham3-3059762-small.gif\" alt=\"FIGURE 3. - LeNet 5 architecture. It consisting of 7 layers: the input layer, two convolution layers followed by pooling ones. And two fully connected layers.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>LeNet 5 architecture. It consisting of 7 layers: the input layer, two convolution layers followed by pooling ones. And two fully connected layers.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham4-3059762-large.gif\" data-fig-id=\"fig4\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham4-3059762-small.gif\" alt=\"FIGURE 4. - The average accuracy (%) of HD classifier for MNIST dataset as a function of the training dataset size. The system was examined using 50, 100, 500,1000, 2200, 4400, and 5200 samples from each class. The accuracy saturate at (89%) between 1000\u20132200 samples.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>The average accuracy (%) of HD classifier for MNIST dataset as a function of the training dataset size. The system was examined using 50, 100, 500,1000, 2200, 4400, and 5200 samples from each class. The accuracy saturate at (89%) between 1000\u20132200 samples.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec6b\"><h3>B. HDC Accuracy as a Function of Dimension Size</h3><p>In our HDC simulations, we have assumed a 10k vector length. However, some studies show that using an HD vector with a dimension less than 1000 elements is adequate to represent the system <a ref-type=\"bibr\" anchor=\"ref80\" id=\"context_ref_80_6b\" data-range=\"ref80\">[80]</a>. This reduction in vector size would positively reflect on the execution time and the chip area implementation. Moreover, energy and search time are direct functions of the dimension of the HD vector and the number of classes considered in the search operation <a ref-type=\"bibr\" anchor=\"ref80\" id=\"context_ref_80_6b\" data-range=\"ref80\">[80]</a>. Results show that a 1k representation can achieve 90.4% compared to 97.8% classification accuracy when utilizing a 10k HD vector for language recognition dataset. For the MNIST dataset employed in this work, we examine the accuracy level for different HD vector sizes. It is clear from <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig.5</a> that the HD of 6k dimension is the adequate size where the accuracy hits above 89% level. Beyond that dimension, the accuracy improves slightly at the cost of time and area. Nevertheless, a cost reduction will emerge when using the 2k HD vector, where the accuracy drops only by 0.2%. Hence, trade-off accuracy for efficiency is application dependent and would greatly impact system performance, especially for IoT edge devices.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham5-3059762-large.gif\" data-fig-id=\"fig5\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham5-3059762-small.gif\" alt=\"FIGURE 5. - MNIST classification average accuracy (%) as a function of the HD vector dimension. 1k, 2k, 4k, 6k, 8k and 10k HD vector sizes were selected to examine the accuracy level. Result confirm that for (8%) of training size, the need for 8K vector size to reach the maximum accuracy.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>MNIST classification average accuracy (%) as a function of the HD vector dimension. 1k, 2k, 4k, 6k, 8k and 10k HD vector sizes were selected to examine the accuracy level. Result confirm that for (8%) of training size, the need for 8K vector size to reach the maximum accuracy.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec6c\"><h3>C. HDC VS CNN</h3><p class=\"has-inline-formula\">HDC is a promising model for edge devices as it does not include the computationally demanding training step found in the widely used CNN <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_6c\" data-range=\"ref17\">[17]</a>, <a ref-type=\"bibr\" anchor=\"ref80\" id=\"context_ref_80_6c\" data-range=\"ref80\">[80]</a>. A significant difference between the two computing paradigms is that HDC departs from the dimensionality reduction concept found in machine learning, such as neural networks. And focuses on dimensionality expansion by emulating the neuron\u2019s activity. Nonetheless, HDC comes with its challenges as encoding alone takes about 80% of the execution time of the training <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_6c\" data-range=\"ref25\">[25]</a> and some encoding algorithms might even increase the size of encoded data by <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$20\\times $\n</tex-math></inline-formula> <a ref-type=\"bibr\" anchor=\"ref76\" id=\"context_ref_76_6c\" data-range=\"ref76\">[76]</a>. This section compares the two computing paradigms for 1D and 2D applications in terms of accuracy, computational complexity, and number of utilized parameters.</p><p>It has been shown that HDC outperforms digital neural network (DNN) in 1D data set applications such as speech recognition <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_6c\" data-range=\"ref6\">[6]</a>. They proposed two HDC-based designs: a single-stage HDC architecture, VoiceHD, and one that is followed by a NN, VoiceHD+NN, to increase the accuracy. Their designs were compared to a 3-layer NN implemented using TinyDNN with around 3k neurons. <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> compares the three designs in terms of full and partial training accuracies, training and testing times, and testing energy consumption. As can be deduced, HDC-based designs can still maintain high classification accuracy even with 40% only of the training set. Although the number of parameters in this example for HDC is more than the DNN, the designs could perform a much faster runtime with lower energy consumption.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nAccuracy, Energy Consumption and Execution Time of VoiceHD, VoiceHD+NN and TinyDNN [6]</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t3-3059762-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t3-3059762-small.gif\" alt=\"Table 3- &#10;Accuracy, Energy Consumption and Execution Time of VoiceHD, VoiceHD+NN and TinyDNN [6]\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>HDC has been widely implemented for 1D signals, but the complexity increases once it is expanded to 2D. In this section, we want to quantify the number of operations and parameters required by each computing paradigm. MNIST classification was performed using LeNet5 on the Caffe framework. LeNet 5 is a simple CNN architecture that consists of 7 layers as shown in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> <a ref-type=\"bibr\" anchor=\"ref81\" id=\"context_ref_81_6c\" data-range=\"ref81\">[81]</a>. Caffe, a deep learning framework, has been used to simulate and quantify the LeNet for MNIST digit classification <a ref-type=\"bibr\" anchor=\"ref82\" id=\"context_ref_82_6c\" data-range=\"ref82\">[82]</a>. Moreover, we have used Netscape, a CNN analyzer tool, by importing the LeNet design into it <a ref-type=\"bibr\" anchor=\"ref83\" id=\"context_ref_83_6c\" data-range=\"ref83\">[83]</a>. MNIST digit recognition based on LeNet5 CNN achieves ~ 99% classification accuracy using a learning rate of 0.01 for 10,000 iterations. While the HDC classification accuracy for the same MNIST data set achieved 86% classification accuracy with the full training dataset.</p><p><a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> below presents the accuracy comparison between the recall results for the HDC model and the reference CNN approach for MNIST dataset. The analysis shows that the performance of the HDC is lower than the CNN approach when considering the overall accuracy level. However, in certain digits for the HDC model, the recall is inferior to other digits. For example, as illustrated in the confusion matrix attached in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig.6</a>, the accuracy of the digit \u201c4\u201d, \u201c5\u201d, and \u201c8\u201d recognition is persistently lower than other digits. Thas is due to its similarity to several other characters. In particular, for example, when recalling \u201c4\u201d, the recall scoring is \u201c9\u201d \u2013 19.1%, and scoring \u201c6\u201d \u2013 4.04%. For recalling \u201c5\u201d, the inference scoring \u201c3\u201d \u2013 30%, \u201c2\u201d \u2013 3.2%.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nComputing Complexity of 2D (MNIST) Using Both HDC and CNN Computing Paradigms</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t4-3059762-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham.t4-3059762-small.gif\" alt=\"Table 4- &#10;Computing Complexity of 2D (MNIST) Using Both HDC and CNN Computing Paradigms\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham6-3059762-large.gif\" data-fig-id=\"fig6\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9354795/moham6-3059762-small.gif\" alt=\"FIGURE 6. - Confusion matrix (CM) displays the total number of observations in each cell, the rows in the CM corresponds to the right class, and the column corresponds to the predicted class. The diagonal corresponds to the correctly classified classes. The row at the bottom of the plot shows the percentages of all samples belonging to each class correctly and incorrectly classified. These metrics are often called the recall. The column on the far right of the plot shows the percentages of all the samples predicted to belong to each class correctly and incorrectly classified. These metrics are often called the precision.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Confusion matrix (CM) displays the total number of observations in each cell, the rows in the CM corresponds to the right class, and the column corresponds to the predicted class. The diagonal corresponds to the correctly classified classes. The row at the bottom of the plot shows the percentages of all samples belonging to each class correctly and incorrectly classified. These metrics are often called the recall. The column on the far right of the plot shows the percentages of all the samples predicted to belong to each class correctly and incorrectly classified. These metrics are often called the precision.</p></fig></div><p class=\"links\"><a href=\"/document/9354795/figures\" class=\"all\">Show All</a></p></div></p><p>It is essential to highlight that for HD architecture, the Hamming distance acts as a quantitative metric of the similarity via direct comparison of distributed representations without decoding those representations. Results in <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_6c\" data-range=\"ref7\">[7]</a> show that the larger number of common elements lead to a more considerable similarity between resulting vectors. In MNIST HD representation, the high resemblance in some digits, for example, \u201c4\u201d and \u201c9\u201d makes the number of overlapped elements between their HD vectors relatively high, making the difference between analyzable patterns sometimes indistinguishable and prone to error. That imposes a limit on the number of overlapped elements between two patterns that can be robustly detected. To reduce the error, one needs to explore another method for mapping the HD space inputs, such as using linear mapping instead of orthogonal mapping <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_6c\" data-range=\"ref14\">[14]</a>. Another suggestion is to extract the image\u2019s main features using CNN techniques and then map those features to HD space. Also, utilizing some retraining techniques would enhance the system accuracy and reduce the error <a ref-type=\"bibr\" anchor=\"ref77\" id=\"context_ref_77_6c\" data-range=\"ref77\">[77]</a>.</p><p class=\"has-inline-formula\">To compute the No. of parameters required by the HDC-based design during the testing phase, we assume: (a) An array of size <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$784\\times 10\\text{k}$\n</tex-math></inline-formula> is generated for random seeds and stored in the IM. (b) Encoded classes stored in the associative memory have a size of 10k<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\times 10$\n</tex-math></inline-formula>. For computation operations, we assume:(a) The No. of shifting is (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$0.5\\times 784$\n</tex-math></inline-formula>) assuming that 50% of the seeds array needs to be shifted (HD vector level operation). (b) Remove the shifting effect by performing the shifting operation again (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$0.5\\times 784$\n</tex-math></inline-formula>). (c) Column-wise addition operation is (784 - 1)<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\times 10\\text{k}$\n</tex-math></inline-formula>. (d) XOR operation is (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$10\\times 10\\text{k}$\n</tex-math></inline-formula>) between the query and the encoded patterns in the AM. (e) Addition operation following the XOR is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$10\\times $\n</tex-math></inline-formula>(10k \u2013 1). (f) Comparison operations are 9.</p><p>As can be deduced from <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>, CNN is still superior in accuracy utilizing full training set and with D = 10k for the HDC architecture. To further study the impact of the size of the training set on the CNN accuracy, the LeNet trained with 10k and 5k samples. The reported accuracy was 98.29% (compared to 89.5% in HDC) and 97.70% (86%), respectively. It is apparent that CNN yet performs better for 2D applications as the accuracy drops slightly by 1.4%, which is consistent with the findings in <a ref-type=\"bibr\" anchor=\"ref81\" id=\"context_ref_81_6c\" data-range=\"ref81\">[81]</a>. Nonetheless, in the HDC design, no MAC operations are required, positively reflecting on area, energy, and execution time.</p></div></div>\n<div class=\"section\" id=\"sec7\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION VII.</div><h2>Conclusion and Recommendations for Future Work</h2></div><p>AI at the edge is important to enable intelligent machines. Efficient hardware architectures with low computing complexity and memory requirements to allow low power and small form factor is of paramount importance.</p><p>HDC is still considered a new paradigm and faces challenges requiring further analysis. One of these challenges that need to address is the sensitivity of the HDC model, which depends on several factors, such as the dimension of the vector used to represent the input data, the mapping form (Binary, Ternary, integer) to high dimension space, and the majority sum (bundling sensitivity). Moreover, once the model is implemented into hardware, there will be an unavoidable source of variations like devices and temperature, which would lead to imprecision.</p><p>Future works direction may include but not limited to: (1) exploit HDC intrinsic characteristics for more classification/cognitive tasks in different domains like security, image processing, and real-time applications. (2) focus on developing an efficient encoding algorithm that handles HDC capacity limitation and would improve data representation for 2D applications. (3) develop more hardware friendly metrics for similarity measurement that would enhance system accuracy. (4) design a unified HD processor that addresses diverse data types and can trade-offs accuracy to efficiency based on the application requirements. (5) investigate the dimension of HD vector that store and manipulate different data representations to reduce the memory footprint and enhance system efficiency. (6) study various methods for integrating the DL/ML techniques with HDC and analyzing system performance effects.</p><p>In addition, two computing paradigms are studied and compared: Firstly, CNN focuses on building decisions based on small features of the target input. The second one is the HDC, which uses the HD vector to encode all possible input views to match the holographic distributed representation. Selecting either one has its implication on accuracy, computing complexity, and power consumption.</p><p>This paper showed that for 1D data, HDC is more efficient and can provide superior overall performance. While in 2D applications, CNN still achieves higher classification accuracy at the expense of more computations. Thus, 2D-HDC reduced the number of required MAC operations by &gt;140 M, which has a direct impact on area and power with 10% accuracy loss. Besides, to architecture further optimization of the implementation, CNN and HDC can benefit from many new approaches such as in-memory computing, data reuse, efficient data flow,.. etc.</p></div>\n\n<div class=\"section\" id=\"app1\"><h2><div class=\"header article-hdr\"><h2>Appendix</h2></div></h2><p>The following Github link can be used for Matlab code: <a target=\"_blank\" href=\"https://github.com/emfhasan/HDCvsCNN_StudyCase\">https://github.com/emfhasan/HDCvsCNN_StudyCase</a>. Two computing approaches, HD and CNN for popular MNIST dataset, are studied and compared in terms of accuracy, computing complexity and overall performance.</p></div>\n</div></div></response>\n"
}