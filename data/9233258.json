{
    "abstract": "Learning based hashing approaches have achieved considerable success in large-scale image retrieval due to the query effectiveness and efficiency. However, most studies highly rely on supervised knowledge like data labels, thus might fail in unsupervised setting. To address this issue, we propose a self-collaborative unsupervised hashing method (SCUH), which jointly learns hashing function and vir...",
    "articleNumber": "9233258",
    "articleTitle": "Self-Collaborative Unsupervised Hashing for Large-Scale Image Retrieval",
    "authors": [
        {
            "preferredName": "Hongmin Zhao",
            "normalizedName": "H. Zhao",
            "firstName": "Hongmin",
            "lastName": "Zhao",
            "searchablePreferredName": "Hongmin Zhao",
            "id": 37089296506
        },
        {
            "preferredName": "Zhigang Luo",
            "normalizedName": "Z. Luo",
            "firstName": "Zhigang",
            "lastName": "Luo",
            "searchablePreferredName": "Zhigang Luo",
            "id": 37538021300
        }
    ],
    "doi": "10.1109/ACCESS.2020.3032628",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9233258/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION I.</div><h2>Introduction</h2></div><p>Massive concerns on hashing have emerged in computer vision, machine learning, information retrieval and related areas, due to amazing characters of low storage and fast retrieval. Currently, learning based hashing techniques mainly learn binary codes from data and meanwhile keep their neighborhood relations in original samples <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\" data-range=\"ref1\">[1]</a>. This kind of hashing methods are called data-dependent.</p><p>In contrast, data-independent hashing algorithms directly construct hashing functions with random projections or other strategies. Of them, LSH <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\" data-range=\"ref2\">[2]</a> as the most representative method efficiently generates hash functions by random projections. It is probabilistic far from deterministic. Notably, LSH can be used anywhere dimension reduction are accounted for. A disadvantage of the LSH family is that LSH usually needs long bit to achieve both high precision and recall. This leads to a huge storage overhead and thus limits the sale at which an LSH algorithm may be applied. Many LSH based variants <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\" data-range=\"ref3 ref4 ref5\">[3]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\" data-range=\"ref3 ref4 ref5\">[5]</a> have been investigated to advance the other shortcomings of LSH. Despite their simplicity and flexibility, they need longer binary codes with reasonable performance, as compared to data-dependent ones. Many methods <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6 ref7 ref8\">[6]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref6 ref7 ref8\">[8]</a> tend to keep the neighborhood relations among the original samples mapped in a low-dimensional Hamming space. Thus, it is not strange that data-dependent counterparts become the main candidate for large-scale image retrieval <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref8 ref9 ref10 ref11 ref12 ref13 ref14 ref15\">[8]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\" data-range=\"ref8 ref9 ref10 ref11 ref12 ref13 ref14 ref15\">[15]</a>.</p><p>With or without use of supervised data labels, data-dependent hashing methods can be separated into two groups: unsupervised <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\" data-range=\"ref16 ref17 ref18 ref19\">[16]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\" data-range=\"ref16 ref17 ref18 ref19\">[19]</a> and supervised <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\" data-range=\"ref20 ref21 ref22 ref23 ref24\">[20]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\" data-range=\"ref20 ref21 ref22 ref23 ref24\">[24]</a>. Compared with the latter, the unsupervised can work well in more broad cases by aid of some extra method, such as clustering <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\" data-range=\"ref25\">[25]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_1\" data-range=\"ref26\">[26]</a>. To name a few, spectral hashing <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\" data-range=\"ref16\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_1\" data-range=\"ref27\">[27]</a> seeks compact binary codes of data-points so that the Hamming distance between binary codes correlates with semantic similarity. The discrete constraints imposed on the binary codes that the target hash functions generate lead to mixed-integer optimization problems. To simplify the optimization involved in a binary code learning procedure, some methods discard the discrete constraints and then get an approximate solution which will increase the accumulated quantization error. ITQ <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\" data-range=\"ref17\">[17]</a> tries to minimize the binary loss by an iterative quantization without relaxations. One disadvantage of ITQ is that it learns orthogonal rotations over pretreatment operation (e.g., PCA or CCA). The two-step learning procedure usually makes ITQ suboptimal. For the sake of non-optimal discrete relaxation, discrete graph hashing (DGH <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1\" data-range=\"ref28\">[28]</a>) and large graph hashing with spectral rotation (LGHSR <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_1\" data-range=\"ref29\">[29]</a>) either directly solve discrete constraints or the rotation transformation to reduce quantitation errors. Principal component analysis hashing (PCAH <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\" data-range=\"ref18\">[18]</a>) also acquires compact binary codes and preserve the similarities among neighbors. Later on, feature clustering hashing (FCH <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_1\" data-range=\"ref30\">[30]</a>) further considers the unbalanced variance distribution and uncertain similarity relations, when learning the projection matrix as in PCAH and PCA-ITQ. To model manifold structure, scalable graph hashing (SGH <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\" data-range=\"ref19\">[19]</a>) and anchor graph hashing (AGH <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_1\" data-range=\"ref31\">[31]</a>) are devised to capture the geometrical structure within dataset in a Hamming space and show the advantages of unsupervised graph hashing. AGH leveraged anchor graphs for solving the eigenfunctions of the resulting graph Laplacians, making hash code training and out-of-sample extension to novel data both tractable and efficient for large-scale datasets. Shen <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_1\" data-range=\"ref32\">[32]</a>, <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_1\" data-range=\"ref33\">[33]</a> proposed a general Inductive Manifold Hashing (IMH) scheme that also generates nonlinear hash functions.</p><p>Orthogonal to such studies, this paper proposes a self-collaborative unsupervised hashing method (SCUH), which learns the hashing function with virtual labels in a self-collaborative manner. This manner has three distinct aspects: 1) SCUH offers itself with supervised virtual labels in a self-supervised fashion, 2) the learned virtual labels behave like modality features engage in hash function learning processing, thereby enabling the model stability, and 3) the specific and common latent semantics are modeled by three corresponding projection matrices in a collaborative framework <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_1\" data-range=\"ref34\">[34]</a>. In detail, owing to this joint self-collaborative manner, we learn two specific projection matrix for virtual labels and sample features, and then learn the common semantics between them for semantic consistency. This way can capture diverse specific information from different features. We optimize SCUH via an alternate optimization algorithm, in which several sub-problems are solved with the corresponding analytical solutions. However, when deriving virtual labels, since solving the closed-form solution is very expensive, we provide an efficient alternative to achieving the same goal through the fast iterative threshold shrinkage algorithm. Experiments of image retrieval on CIFAR-10 <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_1\" data-range=\"ref35\">[35]</a>, YouTube Faces <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_1\" data-range=\"ref36\">[36]</a>, and MNIST <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_1\" data-range=\"ref37\">[37]</a> datasets show that SCUH outstrips several representative counterparts in quantity.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION II.</div><h2>Preliminary Knowledge</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. Supervised Discrete Hashing</h3><p class=\"has-inline-formula\">In large-scale image retrieval, efficiency is a core issue for retrieval performance, even if the hash trick is adopted. Thus, it is non-trivial to efficiently learn hash function. As mentioned in Introduction, most effective hashing methods often belong to supervised ones. For instance, Shen <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2a\" data-range=\"ref38\">[38]</a> proposed supervised discrete hashing (SDH) for the purpose of fully leveraging supervised labels in a simple form. Suppose that there are <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> training samples <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X=\\{x_{i}\\}_{i=1}^{N}$\n</tex-math></inline-formula>. The corresponding objective function is:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\min \\limits _{B,P,W} ||Y-W^{T}B||^{2}+v||B-P^{T}K(X)+\\lambda ||W||^{2} \\\\&amp;s.t. B \\in \\left \\{{-1,1 }\\right \\}^{L \\times N}\\tag{1}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\min \\limits _{B,P,W} ||Y-W^{T}B||^{2}+v||B-P^{T}K(X)+\\lambda ||W||^{2} \\\\&amp;s.t. B \\in \\left \\{{-1,1 }\\right \\}^{L \\times N}\\tag{1}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K(X)$\n</tex-math></inline-formula> is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>-dimensional column vector obtained by the RBF kernel mapping: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K(X,a)=[exp(||x-a_{1}||^{2}/\\sigma),\\cdots,exp(||x-a_{m}||^{2}/\\sigma)]^{T} $\n</tex-math></inline-formula>, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\{a_{j}\\}_{j=1}^{m} $\n</tex-math></inline-formula> are by clustering on the training data and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> is the kernel width. The matrix <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P\\in R^{m\\times L}$\n</tex-math></inline-formula> projects the data <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K(X,a)$\n</tex-math></inline-formula> onto the low dimensional space. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> is a set of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L$\n</tex-math></inline-formula>-bits binary codes for <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> are projection matrices. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> is the label of the training sample <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula>.</p><p>According to <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Eq. (1)</a>, it seems simple and intuitive. In fact, its formulation gives out the collaborative learning way which treats labels and sample features in a fair manner. But it is still inefficiency to learn hash function. Moreover, SDH is not stable for learning binary code <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2a\" data-range=\"ref39\">[39]</a>. To address this issue, Gui <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2a\" data-range=\"ref23\">[23]</a> developed a fast supervised discrete hashing method (FSDH), which directly regresses the labels with the learned binary codes. For clarity, we list the objective as below:<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\min \\limits _{B,P,W} ||B-YW||^{2}+||B-K(X)P||^{2}+\\lambda ||W||^{2} \\\\&amp;s.t. B \\in \\left \\{{-1,1 }\\right \\}^{N \\times L}\\tag{2}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\min \\limits _{B,P,W} ||B-YW||^{2}+||B-K(X)P||^{2}+\\lambda ||W||^{2} \\\\&amp;s.t. B \\in \\left \\{{-1,1 }\\right \\}^{N \\times L}\\tag{2}\\end{align*}\n</span></span></disp-formula></p><p>Obviously, solving <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">Eq. (2)</a> equals to two least square problems with the closed-form solutions. At the meantime, they can be derived efficiently. More importantly, this has theory interpretation for its effectiveness. Inspired by this point, our proposed method can inherit the merits from FSDH in both efficiency and theory. Different from FSDH, our method can be applied to unsupervised settings.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Collaborative Subspace Graph Hashing</h3><p class=\"has-inline-formula\">Zhang <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\" data-range=\"ref34\">[34]</a> claimed that diverse multi-modal features was beneficial for retrieval performance. To this end, a learning framework is introduced by simultaneously learning specific modality transformation matrices and a common transformation matrices. This insight can be formulated as:<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\min \\limits _{Y,B,C} ||EY-X_{C}^{T}CB||^{2}+||I_{\\lambda} CB||^{2}+J_{1}\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\min \\limits _{Y,B,C} ||EY-X_{C}^{T}CB||^{2}+||I_{\\lambda} CB||^{2}+J_{1}\\tag{3}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\begin{aligned} X_{C}=\\left [{\\begin{matrix} X_{1}&amp; 0\\\\ 0&amp; X_{2}\\\\ \\end{matrix} }\\right] \\end{aligned}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\begin{aligned} C=\\left [{\\begin{matrix} \\hspace {0em}C_{1}\\\\ \\hspace {0em}C_{2}\\\\ \\end{matrix} }\\right]\\hspace {0em} \\end{aligned}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\begin{aligned} E=\\left [{\\begin{matrix} \\hspace {0em}I_{n}\\\\ \\hspace {0em}I_{n}\\\\ \\end{matrix} }\\right] \\end{aligned}$\n</tex-math></inline-formula>. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y\\in R^{N \\times L}$\n</tex-math></inline-formula> indicates the hashing codes shared by two modalities. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}\\in R^{d_{1} \\times s}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}\\in R^{d_{2} \\times s}$\n</tex-math></inline-formula> indicate the modality- specific transformation matrices of image and text data, respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B\\in R^{s \\times r}$\n</tex-math></inline-formula> is the shared transformation matrix and maps the latent features to a shared collaborative subspace whilst reducing the quantization loss for hashing codes. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$J_{1}$\n</tex-math></inline-formula> denotes that other function which is not related to our idea and more details are offered in <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\" data-range=\"ref34\">[34]</a>. Here we easily derive that image features and text features play a cooperative role in each other which provides the basis for our method. Besides, since sample features and their labels are heterogeneous and thus might contain diverse semantics, which can be used to improve retrieval performance. Inspired by this, we further refine this collaborative learning way in a self-supervised manner.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION III.</div><h2>Self-Collaborative Unsupervised Hashing</h2></div><p>This section introduces a self-collaborative unsupervised hashing method, then details an efficient alternate optimization algorithm, where solving soft labels is non-trivial.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Model</h3><p class=\"has-inline-formula\">Most studies investigate how to regress the labels with the learned binary codes so as to advance retrieval performance. Recently, a fast supervised discrete hashing method (FSDH) is explored to learn hashing function in a distinct way, which treats the labels as another features and then learns the corresponding projection matrix to leverage supervised knowledge. More impressively, the sound theory about this learning way is provided to prove its effectiveness. Considering this insight, we take a further step by introducing the interplay between labels and features in a collaborative manner. In detail, we introduce two individual projection matrices to keep specific information for labels and features, respectively; meanwhile, the shared projection matrix is learned to make the low-dimensional representation be into a common latent subspace. Among them, the shared projection matrix plays a role of associating the labels and features. This point is not mentioned in FSDH. Besides, such collaborative learning is introduced in cross-modal hashing method <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3a\" data-range=\"ref34\">[34]</a> and proven beneficial for modeling the diversity of cross-modal features. Thus, we formulate this idea into <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">Eq. (4)</a> as <disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-0.5pc}\\min \\limits _{B,C_{1},C_{2},W} ||B-YC_{1}W||^{2}+\\alpha ||B-K(X)C_{2}W||^{2} \\\\&amp;+\\,\\lambda _{1}||C_{1}W||^{2}+\\lambda _{2}||C_{2}W||^{2}\\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-0.5pc}\\min \\limits _{B,C_{1},C_{2},W} ||B-YC_{1}W||^{2}+\\alpha ||B-K(X)C_{2}W||^{2} \\\\&amp;+\\,\\lambda _{1}||C_{1}W||^{2}+\\lambda _{2}||C_{2}W||^{2}\\tag{4}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}$\n</tex-math></inline-formula> denote the projection matrices for the virtual labels and sample features. We can view <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">Eq. (4)</a> as a synthesis of two merits of <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">Eq. (2)</a> and the so-called collaborative learning way. The devised joint model can enjoy the sound theory and information fusion between labels and samples, simultaneously.</p><p class=\"has-inline-formula\">The <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">Eq.(4)</a> like other supervised methods rely on manual annotations. To this end, we consider to learn soft labels in a self-collaborative manner, which unites self-supervised learning and collaborative learning together into a unified framework. A candidate approach to arriving at this goal is the usage of unsupervised clustering methods. Of them, spectral clustering as a typically unsupervised method tries to smooth the soft labels on graphs. This scheme acts as a simple and intuitive yet effective methodology and has been broadly used into many other learning methods <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_3a\" data-range=\"ref31\">[31]</a>. We absorb the merit of this methodology and introduce it into our model, named self-collaborative unsupervised hashing (SCUH), then it can be formulated as:<disp-formula id=\"deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\min \\limits _{B,C_{1},C_{2},W,Y} ||B-YC_{1}W||^{2}+\\alpha ||B-K(X)C_{2}W||^{2} \\\\&amp;\\qquad \\qquad \\quad +\\,\\lambda _{1}||C_{1}W||^{2}+\\lambda _{2}||C_{2}W||^{2}+\\beta Tr(Y^{T}LY) \\\\&amp;\\quad s.t. Y^{T}Y=I,B \\in \\left \\{{-1,1 }\\right \\}^{N \\times L}\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\min \\limits _{B,C_{1},C_{2},W,Y} ||B-YC_{1}W||^{2}+\\alpha ||B-K(X)C_{2}W||^{2} \\\\&amp;\\qquad \\qquad \\quad +\\,\\lambda _{1}||C_{1}W||^{2}+\\lambda _{2}||C_{2}W||^{2}+\\beta Tr(Y^{T}LY) \\\\&amp;\\quad s.t. Y^{T}Y=I,B \\in \\left \\{{-1,1 }\\right \\}^{N \\times L}\\tag{5}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L=I-Z\\Lambda ^{-1}Z^{T}$\n</tex-math></inline-formula> denotes the Laplacian matrix, in where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Z$\n</tex-math></inline-formula> denotes the similarities between samples and specific anchor points, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\Lambda =diag(Z^{T}1)$\n</tex-math></inline-formula>, according to <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3a\" data-range=\"ref29\">[29]</a>.</p><p>In fact, most previously-mentioned methods consider to directly discard orthogonal constraint over virtual labels in order to efficiently solve the optimization problem. This could not be optimal. To address this issue, we design an efficient optimization algorithm to directly solve this problem in the following content.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Optimization</h3><p>It is clear that the <a ref-type=\"disp-formula\" anchor=\"deqn5\" href=\"#deqn5\" class=\"fulltext-link\">Eq.(5)</a> is non-convex yet sub-convex about each variable. Thus, we alternatively optimize multiple sub-problem to solve each variable with the other fixed. The procedure at each round iteration consists of four steps as follows:</p><p class=\"has-inline-formula\"><b>W-step.</b> By fixing the other variables, we can get the solution of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula>.<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} W=M^{-1}N\\tag{6}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} W=M^{-1}N\\tag{6}\\end{equation*}\n</span></span></disp-formula> where <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} M=&amp;C_{1}^{T}Y^{T}YC_{1}\\!+\\!\\alpha C_{2}^{T}K(X)^{T}K(X)C_{2}\\!+\\!\\lambda _{1}C_{1}^{T}C_{1} \\!+\\!\\lambda _{2}C_{2}^{T}C_{2}, \\\\ N=&amp;C_{1}^{T}Y^{T}B+\\alpha C_{2}^{T}K(X)^{T}B\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} M=&amp;C_{1}^{T}Y^{T}YC_{1}\\!+\\!\\alpha C_{2}^{T}K(X)^{T}K(X)C_{2}\\!+\\!\\lambda _{1}C_{1}^{T}C_{1} \\!+\\!\\lambda _{2}C_{2}^{T}C_{2}, \\\\ N=&amp;C_{1}^{T}Y^{T}B+\\alpha C_{2}^{T}K(X)^{T}B\\end{align*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\"><b>C-step.</b> With the other variables fixed, the objective function about <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> can be addressed:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\max \\limits _{C_{1},C_{2}}tr(N^{T}M^{-1}N)\\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\max \\limits _{C_{1},C_{2}}tr(N^{T}M^{-1}N)\\tag{7}\\end{equation*}\n</span></span></disp-formula></p><p>Suppose <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} C=&amp;\\left [{\\begin{matrix} C_{1}\\\\ C_{2}\\\\ \\end{matrix} }\\right],\\\\ S_{t}=&amp;\\left [{\\begin{matrix} Y^{T} &amp;\\quad 0\\\\ 0 &amp;\\quad \\sqrt {\\alpha }K(X)^{T} \\end{matrix} }\\right] \\left [{\\begin{matrix} Y &amp;\\quad 0\\\\ 0 &amp;\\quad \\sqrt {\\alpha }K(X)\\\\ \\end{matrix} }\\right] +\\left [{\\begin{matrix} \\lambda _{1}I &amp;\\quad 0\\\\ 0 &amp;\\quad \\lambda _{2}I\\\\ \\end{matrix} }\\right],\\\\ S_{b}=&amp;\\left [{\\begin{matrix} \\hspace {0em}Y^{T}\\\\ \\hspace {0em}\\alpha K(X)^{T}\\\\ \\end{matrix} }\\right] BB^{T}\\left [{\\begin{matrix} Y \\hspace {1em}\\alpha K(X)\\\\ \\end{matrix} }\\right]. \\\\{}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} C=&amp;\\left [{\\begin{matrix} C_{1}\\\\ C_{2}\\\\ \\end{matrix} }\\right],\\\\ S_{t}=&amp;\\left [{\\begin{matrix} Y^{T} &amp;\\quad 0\\\\ 0 &amp;\\quad \\sqrt {\\alpha }K(X)^{T} \\end{matrix} }\\right] \\left [{\\begin{matrix} Y &amp;\\quad 0\\\\ 0 &amp;\\quad \\sqrt {\\alpha }K(X)\\\\ \\end{matrix} }\\right] +\\left [{\\begin{matrix} \\lambda _{1}I &amp;\\quad 0\\\\ 0 &amp;\\quad \\lambda _{2}I\\\\ \\end{matrix} }\\right],\\\\ S_{b}=&amp;\\left [{\\begin{matrix} \\hspace {0em}Y^{T}\\\\ \\hspace {0em}\\alpha K(X)^{T}\\\\ \\end{matrix} }\\right] BB^{T}\\left [{\\begin{matrix} Y \\hspace {1em}\\alpha K(X)\\\\ \\end{matrix} }\\right]. \\\\{}\\end{align*}\n</span></span></disp-formula></p><p>The objective function <a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">(7)</a> can become:<disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\max \\limits _{C} Tr[(C^{T}S_{t}C)^{-1}C^{T}S_{b}C)]\\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\max \\limits _{C} Tr[(C^{T}S_{t}C)^{-1}C^{T}S_{b}C)]\\tag{8}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">There is a closed solution to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula>, which is the eigenvector of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{t}^{-1}S_{b}$\n</tex-math></inline-formula> responding to the s largest eigenvalues <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_3b\" data-range=\"ref40\">[40]</a>,and then <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}$\n</tex-math></inline-formula> can be obtained.</p><p><b>B-step.</b> By introducing <a ref-type=\"disp-formula\" anchor=\"deqn6\" href=\"#deqn6\" class=\"fulltext-link\">Eq.(6)</a> into <a ref-type=\"disp-formula\" anchor=\"deqn5\" href=\"#deqn5\" class=\"fulltext-link\">Eq.(5)</a>, we can rewrite the objective function as:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\min \\limits _{B,C_{1},C_{2}} Tr[(1+\\alpha)BB^{T}-N^{T}M^{-1}N)]\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\min \\limits _{B,C_{1},C_{2}} Tr[(1+\\alpha)BB^{T}-N^{T}M^{-1}N)]\\tag{9}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">In order to balance binary codes, we initialize <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>:<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {\\mathbf{B}}(i,j) = \\begin{cases} 1,&amp;i \\le \\dfrac {N}{2} \\\\ - 1,&amp;otherwise, \\end{cases}\\tag{10}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {\\mathbf{B}}(i,j) = \\begin{cases} 1,&amp;i \\le \\dfrac {N}{2} \\\\ - 1,&amp;otherwise, \\end{cases}\\tag{10}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> are the indices of the matrix <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>.</p><p class=\"has-inline-formula\">Obviously, the value of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$BB^{T}$\n</tex-math></inline-formula> is constant, so the objective function <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">Eq.(9)</a> can be rewritten as follows:<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\max \\limits _{B} Tr[B^{T}(C_{1}^{T}Y^{T}+\\alpha C_{2}^{T} K(X)^{T})W]\\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\max \\limits _{B} Tr[B^{T}(C_{1}^{T}Y^{T}+\\alpha C_{2}^{T} K(X)^{T})W]\\tag{11}\\end{equation*}\n</span></span></disp-formula></p><p>We can yield the solution:<disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} B=sign((C_{1}^{T}Y^{T}+\\alpha C_{2}^{T} K(X)^{T})W)\\tag{12}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} B=sign((C_{1}^{T}Y^{T}+\\alpha C_{2}^{T} K(X)^{T})W)\\tag{12}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\"><b>Y-step.</b> To solve <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>, we fix the other variables. Firstly, suppose <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{1}=C_{1}W$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{2}=C_{2}W$\n</tex-math></inline-formula>, we can get:<disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} P_{1}=&amp;\\dfrac {1}{1+\\lambda _{1}}Y^{T}B\\\\ P_{2}=&amp;\\alpha (\\alpha K^{T}(X)K(X)+\\lambda _{2} I)^{-1}K^{T}(X)B\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} P_{1}=&amp;\\dfrac {1}{1+\\lambda _{1}}Y^{T}B\\\\ P_{2}=&amp;\\alpha (\\alpha K^{T}(X)K(X)+\\lambda _{2} I)^{-1}K^{T}(X)B\\end{align*}\n</span></span></disp-formula></p><p>The problem <a ref-type=\"disp-formula\" anchor=\"deqn5\" href=\"#deqn5\" class=\"fulltext-link\">(5)</a> can be simplified as:<disp-formula id=\"deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\min \\limits _{Y} Tr\\left({\\dfrac {1}{1+\\lambda _{1}}}\\right)BB^{T}YY^{T}+\\beta Y^{T}LY\\big)\\tag{13}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\min \\limits _{Y} Tr\\left({\\dfrac {1}{1+\\lambda _{1}}}\\right)BB^{T}YY^{T}+\\beta Y^{T}LY\\big)\\tag{13}\\end{equation*}\n</span></span></disp-formula></p><p>The object function <a ref-type=\"disp-formula\" anchor=\"deqn13\" href=\"#deqn13\" class=\"fulltext-link\">(13)</a> can be recast as:<disp-formula id=\"deqn14\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\min \\limits _{Y} Tr\\left({Y^{T}\\left({\\beta -\\beta Z\\Lambda ^{-1}Z^{T}-\\dfrac {1}{1+\\lambda _{1}}BB^{T}}\\right)Y}\\right)\\tag{14}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\min \\limits _{Y} Tr\\left({Y^{T}\\left({\\beta -\\beta Z\\Lambda ^{-1}Z^{T}-\\dfrac {1}{1+\\lambda _{1}}BB^{T}}\\right)Y}\\right)\\tag{14}\\end{equation*}\n</span></span></disp-formula></p><p>Denoting <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Q=\\left [{\\begin{matrix} \\sqrt {\\beta }Z\\Lambda ^{-1/2},\\dfrac {B}{\\sqrt {1+\\lambda _{1}}}\\\\ \\end{matrix} }\\right],\\quad Q^{T}Q\\overset {SVD}{=}U\\Sigma U^{T},\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Q=\\left [{\\begin{matrix} \\sqrt {\\beta }Z\\Lambda ^{-1/2},\\dfrac {B}{\\sqrt {1+\\lambda _{1}}}\\\\ \\end{matrix} }\\right],\\quad Q^{T}Q\\overset {SVD}{=}U\\Sigma U^{T},\\end{align*}\n</span></span></disp-formula> we can obtain <disp-formula id=\"deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} Y=QU\\Sigma ^{-1/2}\\tag{15}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} Y=QU\\Sigma ^{-1/2}\\tag{15}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">In empirical studies, solving <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> is time consuming, thus it is forbidden in large scale retrieval. To efficiently calculate <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>, we apply a fast iterative shrinkage-thresholding algorithm (FISTA) <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_3b\" data-range=\"ref41\">[41]</a> to defeat this issue in <b><a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a></b>.<div class=\"algorithm rule-both\" id=\"alg1\"><h3>Algorithm 1 FISTA for SCUH</h3><div class=\"alg-item\"><p><b>Input:</b></p></div><div class=\"alg-item\"><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y_{1}=QU\\Sigma^{-1/2}$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$U_{0}=Y_{1},t_{1}=1,\\varepsilon&gt;0 $\n</tex-math></inline-formula>;</p></div><div class=\"alg-item\"><p><b>Output:</b></p></div><div class=\"alg-item\"><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y_{j+1}$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">1:</span><p><b>while</b> not convergence <b>do</b></p></div><div class=\"alg-item label\"><span class=\"label\">2:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\nabla g(Y_{j})=-2BP_{1}^{T}+2Y_{j}P_{1}^{T}P_{1}+2\\beta Y_{j}-2\\beta Z\\Lambda^{-1}ZY_{j}$\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">3:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$U_{j+1}=Y_{j}-\\mu \\nabla (Y_{j})$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">4:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t_{j+1}=0.5*(1+\\sqrt{1+4*t_{j}})$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">5:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y_{j+1}=U_{j+1}+\\dfrac{t_{j}}{t_{j+1}}(U_{j+1}-U_{j})$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">6:</span><p class=\"has-inline-formula\"><b>if</b> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$| ||Y_{j}-Y_{j+1}||^{2}-||U_{j+1}-Y_{j}||^{2} | &lt; \\varepsilon $\n</tex-math></inline-formula></p></div><div class=\"alg-item label\"><span class=\"label\">7:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$[U_{1},\\,\\,,V_{1}]=svd(U_{j+1})$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">8:</span><p class=\"has-inline-formula\"><inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y=U_{1}V_{1}^{T}$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">9:</span><p>break;</p></div><div class=\"alg-item label\"><span class=\"label\">10:</span><p><b>end while</b></p></div><div class=\"alg-item label\"><span class=\"label\">11:</span><p class=\"has-inline-formula\"><b>return</b> <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>.</p></div></div><div class=\"algorithm rule-both\" id=\"alg2\"><h3>Algorithm 2 Optimization for SCUH</h3><div class=\"alg-item\"><p class=\"has-inline-formula\"><b>Input:</b> training examples <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${x_{i}}_{i=1}^{n}$\n</tex-math></inline-formula>;code length <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula>; maximum iteration number <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>; parameters <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{1}$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{2}$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>.</p></div><div class=\"alg-item\"><p class=\"has-inline-formula\"><b>Output:</b> the hashing codes <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B \\in R^{n \\times l}$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">1:</span><p class=\"has-inline-formula\">Construct m anchor points of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula> and get the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K(X)$\n</tex-math></inline-formula> via the RBF kernel;</p></div><div class=\"alg-item label\"><span class=\"label\">2:</span><p class=\"has-inline-formula\">Initialize <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y=Z\\Lambda ^{-0.5}U_{2}\\Sigma ^{-0.5}$\n</tex-math></inline-formula>;</p></div><div class=\"alg-item label\"><span class=\"label\">3:</span><p class=\"has-inline-formula\">Initialize <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> by <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. (10)</a>;</p></div><div class=\"alg-item label\"><span class=\"label\">4:</span><p><b>while</b> not convergence <b>do</b></p></div><div class=\"alg-item label\"><span class=\"label\">5:</span><p class=\"has-inline-formula\">Update <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> by <a ref-type=\"disp-formula\" anchor=\"deqn6\" href=\"#deqn6\" class=\"fulltext-link\">Eq.(6)</a>;</p></div><div class=\"alg-item label\"><span class=\"label\">6:</span><p class=\"has-inline-formula\">Update <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}$\n</tex-math></inline-formula> by <a ref-type=\"disp-formula\" anchor=\"deqn8\" href=\"#deqn8\" class=\"fulltext-link\">Eq. (8)</a>;</p></div><div class=\"alg-item label\"><span class=\"label\">7:</span><p class=\"has-inline-formula\">Update <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> by <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">Eq.(12)</a>;</p></div><div class=\"alg-item label\"><span class=\"label\">8:</span><p class=\"has-inline-formula\">Update <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> by <b><a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">algorithm 1</a></b>.;</p></div><div class=\"alg-item label\"><span class=\"label\">9:</span><p><b>end while</b></p></div></div></p><p>The proposed SCUH can converge in finite iterations, as in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure. 1</a>. The corresponding objective function has the monotone non-increasing tendency and remains stable after around 100 iterations on MNIST, 200 iterations on CIFAR-10, and 400 iterations on YouTube Faces, respectively.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao1abc-3032628-large.gif\" data-fig-id=\"fig1\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao1abc-3032628-small.gif\" alt=\"FIGURE 1. - The objective value of SCUH versus the number of iterations for 64-bit image retrieval on (a) MNIST, (b) CIFAR-10, and (c) YouTube Faces datasets, respectively.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>The objective value of SCUH versus the number of iterations for 64-bit image retrieval on (a) MNIST, (b) CIFAR-10, and (c) YouTube Faces datasets, respectively.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Theoretical Analysis</h3><p class=\"has-inline-formula\">In this section, we account for the stability of learning hashing codes <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> to indicate the robustness of SCUH according to <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>. Different from <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>, which is a supervised hashing method, our method is unsupervised. According to <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>, the output hash codes of a stable algorithm can not change much if a training example is deleted or replaced with an independent and identically distributed (iid) one. Let <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O=\\{X,Y\\}=\\{x_{i},y_{i}\\}_{i=1}^{n}$\n</tex-math></inline-formula> be the training samples for SCUH and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\hat {o}$\n</tex-math></inline-formula> be the sample with the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>th example <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$o_{i}=\\{x_{i},y_{i}\\},i=1,\\cdots,n$\n</tex-math></inline-formula> in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula> replaced with an iid one <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${o_{i}^{'}}=\\{{x_{i}^{'}},{x_{j}^{'}}\\}$\n</tex-math></inline-formula>. Suppose that the sample size is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>, the learned class size is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula>, code length is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula> and anchor number is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>. Following <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>, we modify <a ref-type=\"disp-formula\" anchor=\"deqn5\" href=\"#deqn5\" class=\"fulltext-link\">Eq.(5)</a> as:<disp-formula id=\"deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-0.5pc}\\dfrac {1}{nb}||B-YC_{1}W||^{2}+\\dfrac {\\alpha ^{'}}{nb}||B-K(X)C_{2}W||^{2} \\\\&amp;+\\,\\dfrac {\\lambda _{1}^{'}}{cb}||C_{1}W||^{2}+\\dfrac {\\lambda _{2}^{'}}{mb}||C_{2}W||^{2}+\\dfrac {\\beta ^{'}}{c^{2}} Tr(Y^{T}LY)\\tag{16}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-0.5pc}\\dfrac {1}{nb}||B-YC_{1}W||^{2}+\\dfrac {\\alpha ^{'}}{nb}||B-K(X)C_{2}W||^{2} \\\\&amp;+\\,\\dfrac {\\lambda _{1}^{'}}{cb}||C_{1}W||^{2}+\\dfrac {\\lambda _{2}^{'}}{mb}||C_{2}W||^{2}+\\dfrac {\\beta ^{'}}{c^{2}} Tr(Y^{T}LY)\\tag{16}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{1}={n\\lambda _{1}^{'}}/c$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{2}={n\\lambda _{2}^{'}}/m$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha =\\alpha ^{'}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta ={nb\\beta ^{'}}/c^{2}$\n</tex-math></inline-formula>.</p><p class=\"has-inline-formula\">From that, the objective function ensure that the regularization parameters are invariant to the sample size <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>, class size <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula>, and code length <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula>. Suppose that <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> is the label, then <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y_{i} \\in [{0,1}],i=1,\\cdots,n$\n</tex-math></inline-formula>. From <a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">Eq.(7)</a>, the solution of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> composed of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}$\n</tex-math></inline-formula> is an eigenvector and then <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{2}$\n</tex-math></inline-formula> are bounded respectively which derives that <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$||C_{1}||^{2}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$||C_{2}||^{2}$\n</tex-math></inline-formula> are the rank. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K(X)$\n</tex-math></inline-formula> expresses the similarity between sample X and its anchors and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$||K(X)||_{F}^{2}$\n</tex-math></inline-formula> owns up limit.</p><p class=\"has-inline-formula\"><div class=\"section_2\" id=\"\"><h4 class=\"statement definition run-in\">Definition 1:</h4><p class=\"has-inline-formula\">A hashing coding algorithm is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta (n)$\n</tex-math></inline-formula>-stable if the following holds.<disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\forall O,\\hat {O},o_{i},{o_{i}^{'}},\\quad i=1,\\cdots,n,||B(O)-B(\\hat {O})||_{F}\\leq \\beta (n)\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\forall O,\\hat {O},o_{i},{o_{i}^{'}},\\quad i=1,\\cdots,n,||B(O)-B(\\hat {O})||_{F}\\leq \\beta (n)\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B(O)$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B(\\hat {O})$\n</tex-math></inline-formula> are the hash codes learned by adopting <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$O,\\hat {O}$\n</tex-math></inline-formula>, respectively, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta (n)$\n</tex-math></inline-formula> converges to zero with respect to the sample size <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>.</p></div></p><p class=\"has-inline-formula\">Owing to the discrete characteristic of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta (n)$\n</tex-math></inline-formula>, it is not easy to prove the convergence of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta (n)$\n</tex-math></inline-formula>, but the sensitivity and stability of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> is equal to the robustness and stability of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>. Next, we evaluate the stability of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula>.</p><p class=\"has-inline-formula\">In <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>, the stability of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> seems to be independent of other variables including <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>. Due to binary values of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$||B||^{2}=nb$\n</tex-math></inline-formula>, where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of samples and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula> is the code length. For <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{i}, i=1,2$\n</tex-math></inline-formula>, they are the eigenvectors, thus their bounds equal to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula> due to orthogonality. Obviously, it is non-trivial to seek the bound of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>, which is the soft label to-be-learned. For sake of relaxation, the entries in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> might be negative. Despite this, we still can yield its bound. The objective function w.r.t. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> is <disp-formula id=\"deqn17\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\dfrac {1}{nb}||B-YC_{1}W||_{F}^{2}+\\dfrac {\\beta ^{'}}{c^{2}}Tr(Y^{T}LY) s\\tag{17}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\dfrac {1}{nb}||B-YC_{1}W||_{F}^{2}+\\dfrac {\\beta ^{'}}{c^{2}}Tr(Y^{T}LY) s\\tag{17}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Since <a ref-type=\"disp-formula\" anchor=\"deqn17\" href=\"#deqn17\" class=\"fulltext-link\">Eq.(17)</a> is convex about <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula>, the local optimal can be achieved. Obviously, the value of the objective function at <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y=0$\n</tex-math></inline-formula> can be treated as its upper bound on the convergence situation. Thus, <disp-formula id=\"deqn18\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\dfrac {1}{nb}||B-YC_{1}W||_{F}^{2}+\\dfrac {\\beta ^{'}}{c^{2}}Tr(Y^{T}LY)\\leq 1\\tag{18}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\dfrac {1}{nb}||B-YC_{1}W||_{F}^{2}+\\dfrac {\\beta ^{'}}{c^{2}}Tr(Y^{T}LY)\\leq 1\\tag{18}\\end{equation*}\n</span></span></disp-formula></p><p>By simple algebra, the <a ref-type=\"disp-formula\" anchor=\"deqn18\" href=\"#deqn18\" class=\"fulltext-link\">Eq.(18)</a> can become:<disp-formula id=\"deqn19\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-0.5pc}-\\dfrac {2}{nb}Tr(B^{T}YC_{1}W)+\\dfrac {1}{nb}Tr(YC_{1}WW^{T}C_{1}^{T}Y^{T}) \\\\[-3pt]&amp;+\\,\\dfrac {\\beta ^{'}}{C^{2}}Tr(Y^{T}LY)\\leq 0\\tag{19}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-0.5pc}-\\dfrac {2}{nb}Tr(B^{T}YC_{1}W)+\\dfrac {1}{nb}Tr(YC_{1}WW^{T}C_{1}^{T}Y^{T}) \\\\[-3pt]&amp;+\\,\\dfrac {\\beta ^{'}}{C^{2}}Tr(Y^{T}LY)\\leq 0\\tag{19}\\end{align*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Due to the unknown lower bound of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}W$\n</tex-math></inline-formula>, we approximate the upper bound of the left of <a ref-type=\"disp-formula\" anchor=\"deqn19\" href=\"#deqn19\" class=\"fulltext-link\">Eq.(19)</a> as follows:<disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\dfrac {\\beta ^{'}}{C^{2}}Tr(Y^{T}LY)\\leq \\dfrac {4}{nb}Tr(B^{T}YC_{1}W)\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\dfrac {\\beta ^{'}}{C^{2}}Tr(Y^{T}LY)\\leq \\dfrac {4}{nb}Tr(B^{T}YC_{1}W)\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Let <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} B^{T}YC_{1}W=B^{T}YC_{1}W\\Phi ^{1/2}\\Phi ^{-1/2},\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} B^{T}YC_{1}W=B^{T}YC_{1}W\\Phi ^{1/2}\\Phi ^{-1/2},\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\Phi =\\dfrac {nb\\beta ^{'}}{4c^{2}}L$\n</tex-math></inline-formula>.</p><p>Then <disp-formula id=\"deqn20\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} ||Y\\Phi ^{1/2}\\!-\\!\\dfrac {1}{2}\\Phi ^{-1/2}C_{1}WB^{T}||_{F}^{2}\\!\\leq \\!||\\dfrac {1}{2}\\Phi ^{-1/2}C_{1}WB^{T}||_{F}^{2}\\tag{20}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} ||Y\\Phi ^{1/2}\\!-\\!\\dfrac {1}{2}\\Phi ^{-1/2}C_{1}WB^{T}||_{F}^{2}\\!\\leq \\!||\\dfrac {1}{2}\\Phi ^{-1/2}C_{1}WB^{T}||_{F}^{2}\\tag{20}\\end{equation*}\n</span></span></disp-formula></p><p>In terms of <a ref-type=\"disp-formula\" anchor=\"deqn20\" href=\"#deqn20\" class=\"fulltext-link\">Eq.(20)</a>, we have <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} ||Y\\Phi ^{1/2}||_{F}\\leq ||\\Phi ^{-1/2}C_{1}WB^{T}||_{F}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} ||Y\\Phi ^{1/2}||_{F}\\leq ||\\Phi ^{-1/2}C_{1}WB^{T}||_{F}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Let <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\delta _{min}$\n</tex-math></inline-formula> be the non-zero minimum eigenvalue of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L$\n</tex-math></inline-formula>. Then, <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\sqrt {\\delta _{min}}||Y||_{F}\\leq&amp;||Y\\Phi ^{1/2}||_{F}\\leq ||\\Phi ^{-1/2}C_{1}WB^{T}||_{F} \\\\\\leq&amp;\\dfrac {1}{\\sqrt {\\delta _{min}}}||C_{1}WB^{T}||_{F}\\leq \\dfrac {1}{\\sqrt {\\delta _{min}}}||W||_{F}||B||_{F}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\sqrt {\\delta _{min}}||Y||_{F}\\leq&amp;||Y\\Phi ^{1/2}||_{F}\\leq ||\\Phi ^{-1/2}C_{1}WB^{T}||_{F} \\\\\\leq&amp;\\dfrac {1}{\\sqrt {\\delta _{min}}}||C_{1}WB^{T}||_{F}\\leq \\dfrac {1}{\\sqrt {\\delta _{min}}}||W||_{F}||B||_{F}\\end{align*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Thus, <disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} ||Y||_{F}\\leq&amp;\\dfrac {4C^{2}}{nb{\\beta ^{'}}\\delta _{min}}||W||_{F}||B||_{F} \\\\=&amp;\\dfrac {4C^{2}}{\\sqrt {nb}{\\beta ^{'}}{\\delta _{min}}}||W||_{F}\\leq \\dfrac {4C^{2}v}{\\sqrt {nb}{\\beta ^{'}}\\delta _{min}}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} ||Y||_{F}\\leq&amp;\\dfrac {4C^{2}}{nb{\\beta ^{'}}\\delta _{min}}||W||_{F}||B||_{F} \\\\=&amp;\\dfrac {4C^{2}}{\\sqrt {nb}{\\beta ^{'}}{\\delta _{min}}}||W||_{F}\\leq \\dfrac {4C^{2}v}{\\sqrt {nb}{\\beta ^{'}}\\delta _{min}}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$||W||_{F}\\le v$\n</tex-math></inline-formula> is assumed.</p><p class=\"has-inline-formula\">Following the proof of <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3c\" data-range=\"ref23\">[23]</a>, we obtain <disp-formula id=\"deqn21\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\dfrac {\\lambda _{1}^{'}}{cb}||Q_{1}-P_{1}||_{F}^{2}\\leq&amp;\\dfrac {2M}{nb}||y_{i}(Q_{1}-P_{1})||_{F} \\\\\\leq&amp;\\dfrac {8Mc^{2}v}{(nb)^{3/2}{\\beta ^{'}} \\delta _{min}}||Q_{1}-P_{1}||_{F}\\tag{21}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\dfrac {\\lambda _{1}^{'}}{cb}||Q_{1}-P_{1}||_{F}^{2}\\leq&amp;\\dfrac {2M}{nb}||y_{i}(Q_{1}-P_{1})||_{F} \\\\\\leq&amp;\\dfrac {8Mc^{2}v}{(nb)^{3/2}{\\beta ^{'}} \\delta _{min}}||Q_{1}-P_{1}||_{F}\\tag{21}\\end{align*}\n</span></span></disp-formula> and <disp-formula id=\"deqn22\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\dfrac {\\lambda _{2}^{'}}{mb}||Q_{2}-P_{2}||_{F}^{2}\\leq&amp;\\dfrac {2M\\alpha ^{'}}{nb}||K_{x}(Q_{2}-P_{2})||_{F} \\\\\\leq&amp;\\dfrac {2M\\mu \\alpha ^{'}}{nb}||(Q_{2}-P_{2})||_{F}\\tag{22}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\dfrac {\\lambda _{2}^{'}}{mb}||Q_{2}-P_{2}||_{F}^{2}\\leq&amp;\\dfrac {2M\\alpha ^{'}}{nb}||K_{x}(Q_{2}-P_{2})||_{F} \\\\\\leq&amp;\\dfrac {2M\\mu \\alpha ^{'}}{nb}||(Q_{2}-P_{2})||_{F}\\tag{22}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\mu $\n</tex-math></inline-formula> is the upper bound of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K_{x}$\n</tex-math></inline-formula>.</p><p>Thus, <disp-formula id=\"deqn23\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\|C_{1}W-{C_{1}^{'}}{W^{'}}\\|_{F}\\leq \\dfrac {8Mc^{3}v}{(n)^{3/2}(b)^{1/2}{\\lambda _{1}^{'}}{\\beta ^{'}} \\delta _{min}}\\tag{23}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\|C_{1}W-{C_{1}^{'}}{W^{'}}\\|_{F}\\leq \\dfrac {8Mc^{3}v}{(n)^{3/2}(b)^{1/2}{\\lambda _{1}^{'}}{\\beta ^{'}} \\delta _{min}}\\tag{23}\\end{equation*}\n</span></span></disp-formula> and <disp-formula id=\"deqn24\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\|C_{2}W-{C_{2}^{'}}{W^{'}}\\|_{F}\\leq \\dfrac {2mM\\mu \\alpha ^{'}}{n{\\lambda _{2}^{'}}}\\tag{24}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\|C_{2}W-{C_{2}^{'}}{W^{'}}\\|_{F}\\leq \\dfrac {2mM\\mu \\alpha ^{'}}{n{\\lambda _{2}^{'}}}\\tag{24}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">According to <a ref-type=\"disp-formula\" anchor=\"deqn23\" href=\"#deqn23\" class=\"fulltext-link\">Eq.(23)</a> and <a ref-type=\"disp-formula\" anchor=\"deqn24\" href=\"#deqn24\" class=\"fulltext-link\">Eq.(24)</a>, we find that the learned <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Y$\n</tex-math></inline-formula> changes the stability of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C_{1}W$\n</tex-math></inline-formula>. This way differs from the generic way. More specifically, the convergence becomes more stable when the number of instances and the minimum eigenvalue of the laplacian graph <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$L$\n</tex-math></inline-formula> increases. This indicates the effect of the learned graph over the learned binary codes. In <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3c\" data-range=\"ref42\">[42]</a>, Bousquet and Elisseeff have proven that stable algorithm well generalize well on the future coming dataset. Moreover, experimental results provide another evidence to support this claim as well. This also implies the efficacy of the proposed method.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IV.</div><h2>Experiments</h2></div><p>In this section, we verify the effectiveness of our proposed method by carrying out experiments of image retrieval on three real-world image datasets including CIFAR-10 <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_4\" data-range=\"ref35\">[35]</a>, YouTube Faces <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_4\" data-range=\"ref36\">[36]</a>, and MNIST <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_4\" data-range=\"ref37\">[37]</a>. We compare SCUH with six well-behaved hashing algorithms, including ITQ <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_4\" data-range=\"ref17\">[17]</a>, LGHSR <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_4\" data-range=\"ref43\">[43]</a>, SH <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_4\" data-range=\"ref38\">[38]</a>, SGH <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_4\" data-range=\"ref34\">[34]</a>, AGH <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_4\" data-range=\"ref42\">[42]</a>, DGH <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_4\" data-range=\"ref41\">[41]</a>, PCAH <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_4\" data-range=\"ref18\">[18]</a>, and FCH <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_4\" data-range=\"ref30\">[30]</a>.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Datasets</h3><p class=\"has-inline-formula\">MNIST contains 70,000 784-dimensional handwritten digit images from 0 to 9. Each image is cropped and normalized to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$28\\times28$\n</tex-math></inline-formula>. The dataset is split into a training set with 69,000 examples and a test set with all remaining examples. As a subset of the well-known 80M tiny image collection, CIFAR-10 contains 60,000 images from 10 classes with 6,000 instances for each class. Each image is represented by a 512-dimensional GIST feature vector. The entire dataset is split into a test set with 1,000 examples and a training set with all remaining examples. YouTube Faces is a database of 3,425 face videos captured from 1,595 different people, a new subset is constructed by selecting the person who has at least 500 face images, which results in 370,319 samples. And 1,770D LBP vector is also pre-extracted to represent images. For this subset, the testing set consists of 3,800 images from 38 people who have more than 2,000 images, and we uniformly sample 100 images from each people. The remaining samples constitute the training set.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Experimental Details</h3><p class=\"has-inline-formula\">The evaluation metrics in common have the mean of average precision (MAP), the precision-recall (P-R) and F-measure, respectively. Among them, MAP is based on the top <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$R$\n</tex-math></inline-formula> retrieved samples, which is the average of the rank-aware precision of all the samples relevant to the query. The precision indicates the ratio of the positive instances predicted to be positive over all the instances, i.e. the true positive and negative instances. The recall is the proportion of the positive instances predicted to be positive over all the true positive instances. The big area under the P-R curve means the good retrieval performance. According to the definition of both the prediction and the recall, we give out the definition of F-measure as below:<disp-formula id=\"deqn25\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} F-measure=\\dfrac {2\\times precision\\times recall}{precision + recall}\\tag{25}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} F-measure=\\dfrac {2\\times precision\\times recall}{precision + recall}\\tag{25}\\end{equation*}\n</span></span></disp-formula> According to <a ref-type=\"disp-formula\" anchor=\"deqn25\" href=\"#deqn25\" class=\"fulltext-link\">Eq.(25)</a>, the higher the F-measure, the higher either the precision or recall.</p><p class=\"has-inline-formula\">We set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$R=1000$\n</tex-math></inline-formula> in all the experiments. During constructing <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> anchors, which is used to efficiently construct the graph matrix, we set <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m=2$\n</tex-math></inline-formula>,000 and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$s=3$\n</tex-math></inline-formula> for CIFAR-10, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m=2$\n</tex-math></inline-formula>,000 and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$s=2$\n</tex-math></inline-formula> for MNIST, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$m=200$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$s=2$\n</tex-math></inline-formula> for YouTube Faces. All of the source code of the compared methods are test in the same running environment. We perform each individual experiment for five times and report their average values as the resultant evaluation results. The parameters <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta $\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{1}$\n</tex-math></inline-formula>,<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda _{2}$\n</tex-math></inline-formula> in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> can be obtained by multiple experiments.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nParameter Settings of SCUH on Three Datasets</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t1-3032628-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t1-3032628-small.gif\" alt=\"Table 1- &#10;Parameter Settings of SCUH on Three Datasets\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Map Results</h3><p>To improve the reliability of the results, the replicated experimental for five times is afforded. Obviously, the bigger the value of MAP, the more precise the results. <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> reports the different methods based on different bits on three datasets. SCUH outperforms the other compared methods in all the cases. As shown in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>, the MAP of SCUH is stable with increasing the length of binary codes, as compared to the other eight methods. This indicates our proposed method is effective and provides an alternative to the other ones for image retrieval.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nThe MAP Results on MNIST, YouTube Faces, and CIFAR-10 Datasets</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t2-3032628-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t2-3032628-small.gif\" alt=\"Table 2- &#10;The MAP Results on MNIST, YouTube Faces, and CIFAR-10 Datasets\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4d\"><h3>D. Precision-Recall Results</h3><p>As the above definition of P-R, if the P-R curve of one method completely covers another method, this means that the former is better than the latter. Besides MAP, we further show the experimental results of all the compared methods on three datasets, in terms of the precision-recall curves. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a> shows that SCUH dominates a larger area under the precision-recall curve than the other methods on MNIST. Similarly, SCUH behaves well in most cases on CIFAR-10, as in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig 3</a> and on YouTube Faces in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>, respectively.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao2abcd-3032628-large.gif\" data-fig-id=\"fig2\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao2abcd-3032628-small.gif\" alt=\"FIGURE 2. - Precision-recall curves of the compared methods versus different numbers of bits on MNIST dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Precision-recall curves of the compared methods versus different numbers of bits on MNIST dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao3abcd-3032628-large.gif\" data-fig-id=\"fig3\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao3abcd-3032628-small.gif\" alt=\"FIGURE 3. - Precision-recall curves of the compared methods versus different numbers of bits on CIFAR-10 dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Precision-recall curves of the compared methods versus different numbers of bits on CIFAR-10 dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao4abcd-3032628-large.gif\" data-fig-id=\"fig4\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao4abcd-3032628-small.gif\" alt=\"FIGURE 4. - Precision-recall curves of the compared methods versus different numbers of bits on YouTube Faces dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Precision-recall curves of the compared methods versus different numbers of bits on YouTube Faces dataset: (a) 24 bits, (b) 32 bits and (c) 48 bits (d) 64bits.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4e\"><h3>E. F-Measure Results</h3><p>In the retrieval, the F-measure value comes from the results of precision and recall. we want to have high precision and recall rate at the same time. As in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a><a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\"/>\u2013\u200b<a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">7</a>, the F-measure curves provide ample evidence to exposit the effectiveness of SCUH. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a> shows that SCUH outperforms the baseline methods in all cases and achieves higher F-measure than the compared counterparts, with the rise of the number of retrieved images. <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a> also indicates that F-measure of SCUH is higher than the other methods. This also implies that the proposed method is sound for image retrieval. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig. 7</a> shows that SCUH is excellent.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao5abc-3032628-large.gif\" data-fig-id=\"fig5\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao5abc-3032628-small.gif\" alt=\"FIGURE 5. - The F-measure of image retrieval of the compared hashing methods versus number of retrieved images on MNIST datasets with different bits (a) 24bits, (b) 32bits and (c) 48 bits (d)64bits., respectively.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>The F-measure of image retrieval of the compared hashing methods versus number of retrieved images on MNIST datasets with different bits (a) 24bits, (b) 32bits and (c) 48 bits (d)64bits., respectively.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao6abcd-3032628-large.gif\" data-fig-id=\"fig6\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao6abcd-3032628-small.gif\" alt=\"FIGURE 6. - The F-measure of image retrieval of the compared methods versus number of retrieved images on CIFAR-10 datasets with different bits (a) 24 bits, (b) 32 bits, (c) 48 bits, and (d) 64 bits, respectively.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>The F-measure of image retrieval of the compared methods versus number of retrieved images on CIFAR-10 datasets with different bits (a) 24 bits, (b) 32 bits, (c) 48 bits, and (d) 64 bits, respectively.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao7abcd-3032628-large.gif\" data-fig-id=\"fig7\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao7abcd-3032628-small.gif\" alt=\"FIGURE 7. - The F-measure of image retrieval of the compared methods versus number of retrieved images on YouTube Faces datasets with different bits (a) 24 bits, (b) 32 bits, (c) 48 bits, and (d) 64 bits, respectively.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>The F-measure of image retrieval of the compared methods versus number of retrieved images on YouTube Faces datasets with different bits (a) 24 bits, (b) 32 bits, (c) 48 bits, and (d) 64 bits, respectively.</p></fig></div><p class=\"links\"><a href=\"/document/9233258/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4f\"><h3>F. Statistic Analysis</h3><p class=\"has-inline-formula\">MAP essentially refers to the enclosed area under PR curve and is adopted by many algorithms. In this section, we will use it to construct the average error rates, which is equals to 1-MAP, as the input of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>-test. We perform the paired sample <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>-test <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_4f\" data-range=\"ref44\">[44]</a> experiments on three previously-used datasets.</p><p class=\"has-inline-formula\">Suppose that the difference between the error rates of our method and the compared methods is <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$d$\n</tex-math></inline-formula>. The null hypothesis is that the total mean is equal to a value <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\mu _{0}$\n</tex-math></inline-formula>. The paired sample <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>-test is defined as follows:<disp-formula id=\"deqn26\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} t =\\dfrac {\\bar {d}-\\mu _{0}}{S_{d}/\\sqrt {n}}\\tag{26}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} t =\\dfrac {\\bar {d}-\\mu _{0}}{S_{d}/\\sqrt {n}}\\tag{26}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\bar {d}$\n</tex-math></inline-formula> is the mean value of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$d$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{d}$\n</tex-math></inline-formula> denotes the standard deviation of the difference between paired samples, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of the paired samples. Besides, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p$\n</tex-math></inline-formula> is obtained after <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> is calculated by looking up the table of values of Student\u2019s <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>-distribution <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_4f\" data-range=\"ref45\">[45]</a>. If the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p$\n</tex-math></inline-formula>-value is below the statistical threshold (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\alpha =0.05$\n</tex-math></inline-formula> usually adopted), the null hypothesis will be rejected, which means that the compared methods are considered to be statistically identical error rate with 95% confidence.</p><p class=\"has-inline-formula\"><a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> shows the statistic differences between the compared methods used in experiments on average error rates. On CIFAR-10 dataset, the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p$\n</tex-math></inline-formula> of between SCUH and LGHSR exceeds 0.05. This is because the images in CIFAR-10 are often blur and easily derive the noisy embedded graph, which affects the retrieval performance. Nevertheless, overly, SCUH is statistically better than the other methods such as PCAH, ITQ, SH, SGH, FCH, AGH and DGH.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\n\n$p$\n-Values Between SCUH and the Other Compared Methods by the Average Error Rates on Three Datasets</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t3-3032628-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9233258/zhao.t3-3032628-small.gif\" alt=\"Table 3- &#10;&#10;$p$&#10;-Values Between SCUH and the Other Compared Methods by the Average Error Rates on Three Datasets\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION V.</div><h2>Conclusion</h2></div><p>This paper proposes an unsupervised hashing method in a self-collaborative manner, called self-collaborative unsupervised hashing (SCUH). In this way, SCUH learns virtual labels in local manifolds, and simultaneously enjoys specific subspaces for both samples and virtual labels and a common subspace to associate with each other to better induce hashing function. The involved collaborative and self-supervised learning manners are seamlessly coupled with each other in a unified formulation for retrieval task. Experiments of image retrieval on three benchmarks indicate the efficacy of SCUH as compared to several well-established counterparts.</p></div>\n</div></div></response>\n"
}