{
    "abstract": "This paper proposes a novel method for active collision avoidance to protect the human who enters a robot\u2019s workspace in a human-robot collaborative environment. The proposed method uses a somatosensory sensor to monitor the robot\u2019s workspace and detect anyone attempting to enter it. When someone enters the workspace, a Kinect detects and calculates the position of his or her skeleton points in re...",
    "articleNumber": "9031397",
    "articleTitle": "Active Collision Avoidance for Human-Manipulator Safety",
    "authors": [
        {
            "preferredName": "Guanglong Du",
            "normalizedName": "G. Du",
            "firstName": "Guanglong",
            "lastName": "Du",
            "searchablePreferredName": "Guanglong Du",
            "id": 37999836800
        },
        {
            "preferredName": "Yinhao Liang",
            "normalizedName": "Y. Liang",
            "firstName": "Yinhao",
            "lastName": "Liang",
            "searchablePreferredName": "Yinhao Liang",
            "id": 37088347707
        },
        {
            "preferredName": "Gengcheng Yao",
            "normalizedName": "G. Yao",
            "firstName": "Gengcheng",
            "lastName": "Yao",
            "searchablePreferredName": "Gengcheng Yao",
            "id": 37087239330
        },
        {
            "preferredName": "Chunquan Li",
            "normalizedName": "C. Li",
            "firstName": "Chunquan",
            "lastName": "Li",
            "searchablePreferredName": "Chunquan Li",
            "id": 37086131934
        },
        {
            "preferredName": "Ronigues J. Murat",
            "normalizedName": "R. J. Murat",
            "firstName": "Ronigues J.",
            "lastName": "Murat",
            "searchablePreferredName": "Ronigues J. Murat",
            "id": 37089303498
        },
        {
            "preferredName": "Hua Yuan",
            "normalizedName": "H. Yuan",
            "firstName": "Hua",
            "lastName": "Yuan",
            "searchablePreferredName": "Hua Yuan",
            "id": 37894318500
        }
    ],
    "doi": "10.1109/ACCESS.2020.2979878",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9031397/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION I.</div><h2>Introduction</h2></div><p>With the development of Industry 4.0, robots tend to be intelligent and collaborative in the future <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\" data-range=\"ref1 ref2 ref3\">[1]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\" data-range=\"ref1 ref2 ref3\">[3]</a>. The human-robot collaboration will bring wider use of robots and promote robots to become indispensable partners in human life <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\" data-range=\"ref4\">[4]</a>. However, the traditional methods of industrial production could not meet the needs of the enterprise\u2019s production safety. Safety has become one of the most important factors in the future development of human-robot interaction. Although human-robot collaboration allows robots to do broader and more complex tasks, it raises the safety concerns of the robots. However, increasing robot safety often means a compromise in their performance, which requires designers to find a balance between safety and performance. At present, collaborative robots can perceive the environment and change their behavior accordingly. For example, when a robotic arm hits the human arm, the robot can perceive the existence of the human arm through the force sensor. Then it stops or goes away to protect human safety. This feature limits the performance of collaborative robots, such as speed, load, and so on. Therefore, an effective method is proposed to predict the possible collision between robots and humans, and timely feedback is provided to ensure human safety.</p><p>When a robot performs work in an unstructured, dynamic, and populated environment, finding a way to work safely and efficiently becomes more challenging <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\" data-range=\"ref5\">[5]</a>. Safe and efficient human-robot interaction is indispensable to future robotics applications, where humans and robots must collaborate to perform tasks jointly <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6\">[6]</a>. To meet the strictest safety requirements, robots must have an excellent ability to change motion paths in complex environments. In any human-robot collaborative system or any environment where humans and robots coexist, ensuring human safety at all times is a considerable challenge. In order to help humans in daily life, several studies have been performed to develop robot systems, which include interaction and cooperation with humans <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\" data-range=\"ref7\">[7]</a>. These studies indicated that robots are very useful in a variety of tasks performed in collaboration with humans. For this reason, finding a way to protect human operators in human-robot collaborative system is highly relevant. This not only includes detecting possible collisions in real-time, but also avoiding collision with humans during runtime.</p><p>Active collision avoidance in real time consists of three parts <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref8\">[8]</a>: 1) Environment perception; 2) collision avoidance; 3) robot control. Collision avoidance is one of the most important fields in robotics. Many real-time capable planning concepts are based on the potential field methods that were introduced in two previous works. In these methods, virtual attractive and repulsive fields are used to represent targets and obstacles. In this case, the robot is attracted to targets and moves away from obstacles. Real-time planning algorithms are significant to robotic systems <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\" data-range=\"ref9\">[9]</a>.</p><p>Despite many achievements in robotic safety, most real-time collision avoidance methods are either too expensive or have very limited security in practical applications.\n<ol><li value=\"1\" data-list-item-label=\"1)\"><p>Some methods, for example <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\" data-range=\"ref17\">[17]</a>, are not practical in the real world because these methods require a dedicated set with built-in sensors, which can hinder the user\u2019s motion and increase cost in practical applications.</p></li><li value=\"2\" data-list-item-label=\"2)\"><p>References <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\" data-range=\"ref21 ref22 ref23 ref24\">[21]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\" data-range=\"ref21 ref22 ref23 ref24\">[24]</a> treat the human body as a moving obstacle, so the robot may mistakenly believe that the human body is the operation object and put people at risk. In addition, due to the lack of independent intelligence, the efficiency of collision avoidance is relatively low.</p></li><li value=\"3\" data-list-item-label=\"3)\"><p>Current collaborative robots are relatively expensive, so replacing a large number of traditional robots with cooperative robots is not a realistic task. Moreover, cooperative robots tend to adopt post-collision detection, which inevitably puts people at risk.</p></li></ol></p><p>To address these issues, this paper proposeds an active collision avoidance method for human-robot incorporation. In this proposed method, Kinect is used to detect humans in real-time; instead of particle filter (PF), the improved particle filter (IPF) algorithm for Kinect is applied to estimate the position of the skeleton joint accurately, thus improving the accuracy of human tracking. Moreover, robot velocity control is implemented to ensure the safety of humans while improving production efficiency. A rule-based logic system is developed to enable the collision avoidance system to obtain the most appropriate response strategy according to different behaviors of the human. When humans are immobile, dynamic roadmap (DRM) is used to plan new paths for the robot directly. When humans move slowly, the boundary box is calculated, and the robot bypass this box by using DRM to plan a new path. When humans move too fast, the robot immediately stops. Therfore, the proposed collision avoidance method can take different measures according to different human behaviors for more effectively and safely protecting humans. The main contributions are listed as follow:\n<ol><li value=\"1\" data-list-item-label=\"1)\"><p>Cylinders were adopted to build a bounding box model for both the human bones and the robots. Hence, collision detection can be achieved by detecting the relative position between the cylinders instead of the point cloud interaction in the depth image, thereby improving the timeliness and effectiveness of the collision avoidance method.</p></li><li value=\"2\" data-list-item-label=\"2)\"><p>The proposed active collision avoidance method can analyze and predict human behavior and choose the best path for the robot so that the flexibility and intelligence of the robot movement can be improved.</p></li><li value=\"3\" data-list-item-label=\"3)\"><p>In the proposed active collision avoidance method, an intelligent system with rule-based logic is proposed for analyzing human behavior so that the system can correctly respond to different human movement speeds, and the DRM was adopted to plan a new path, thereby improving the effectiveness and the independent intelligence of the proposed collision avoidance method.</p></li></ol></p><p>The remainder of the paper is organized in sections. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a> provides an overview of the paper. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> presents details about the modeling of humans and robots, which is the basis of the paper. <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a> describes the process of estimating skeleton position using improved PF. <a ref-type=\"sec\" anchor=\"sec6\" class=\"fulltext-link\">Section VI</a> presents fast path planning using DRM. <a ref-type=\"sec\" anchor=\"sec7\" class=\"fulltext-link\">Section VII</a> details robot velocity control and <a ref-type=\"sec\" anchor=\"sec8\" class=\"fulltext-link\">Section VIII</a> describes active collision avoidance using a set of rules. The experiments and results are presented in <a ref-type=\"sec\" anchor=\"sec9\" class=\"fulltext-link\">Section IX</a>. Discussion is given in <a ref-type=\"sec\" anchor=\"sec10\" class=\"fulltext-link\">Section X</a> and the conclusion in <a ref-type=\"sec\" anchor=\"sec11\" class=\"fulltext-link\">Section XI</a>.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION II.</div><h2>Related Work</h2></div><p>Planning algorithms allow robots to avoid obstacles in real environments. Jiang <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2\" data-range=\"ref10\">[10]</a> described a planning algorithm that can adaptively coordinate humans and robots in the hybrid assembly system. Zhang <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2\" data-range=\"ref11\">[11]</a> proposed an autonomous path planning method based on improved rapidly-exploring random tree algorithm. Han <i>et al.</i> detected obstacles in real-time and re-planed the path based on distance calculation and discrete detection <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2\" data-range=\"ref12\">[12]</a>.</p><p>Some studies have assessed obstacle avoidance in dynamic environments <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\" data-range=\"ref13\">[13]</a>. Most of them have treated operators as moving obstacles. This meant that they only need to consider collision avoidance of robots. References <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\" data-range=\"ref14\">[14]</a>, <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2\" data-range=\"ref15\">[15]</a> computed the distance between the human and the robot by analyzing images of the environment. Most of the collision detection methods cited above require that information about the environment must be available. Such collision avoidance algorithms are primarily based on the distance between the industrial robot and the detected objects in the real environment.</p><p>In human-robot collaboration systems, two approaches of avoiding collision have been used for safety: vision-based methods, which analyze motion, color and texture <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2\" data-range=\"ref16\">[16]</a>, and inertial sensor-based methods, which uses special suits for motion capture <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2\" data-range=\"ref17\">[17]</a>. Currently, vision-based methods are a better choice for avoiding collisions in robotic control systems because in this way the operator does not have to wear additional equipment. Meanwhile, the surrounding environmental information, not just the motions of the operator, can be obtained. In addition, with the development of various 3D vision sensing system, some new depth cameras, like the Microsoft Kinect sensor <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2\" data-range=\"ref18\">[18]</a>, have facilitated the development of a powerful and easy to use sensor system.</p><p>In recent years, studies of the vision-based method have focused on increasing the efficiency of collision detection. One method involved a multi-camera system to detect obstacles <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2\" data-range=\"ref19\">[19]</a>. Tan and Arai <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2\" data-range=\"ref20\">[20]</a> used a triple stereo-vision system to track the movement of a sitting operator who was wearing a color marker. References <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2\" data-range=\"ref21 ref22 ref23\">[21]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2\" data-range=\"ref21 ref22 ref23\">[23]</a> implemented collision detection and avoidance based on the depth images. Ahmad and Plappera <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2\" data-range=\"ref24\">[24]</a> proposed an automatic path planning algorithm for robot manipulator, which located the unknown obstacles based on the time-of-flight (TOF) sensor.</p></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION III.</div><h2>Overview</h2></div><p>Human safety can be questionable in environments containing working robots. In this paper, an active collision-avoidance method is proposed to protect the humans who enter robot\u2019s workspace. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1</a>. Shows the flowchart of the proposed method. First, a Kinect, which has two infrared cameras for depth detection and one visual-spectrum camera for visual recognition, is used to detect the skeletons inside humans who enter the workspace. According to the position data of the skeleton joints of the human body from the Kinect, the human breaking into the robot\u2019s workspace can be detected even if the human is static. In this way, the risk that the robot mistakes humans for the object of operation can be reduced, thereby ensuring the safety of the human. After building bounding cylinders for humans and robots, collision detection is achieved by detecting the relative position between the cylinders instead of the point cloud intersections of the depth image, which improves the accuracy and effectiveness of the proposed method.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq1-2979878-large.gif\" data-fig-id=\"fig1\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq1-2979878-small.gif\" alt=\"FIGURE 1. - Process chart of the proposed method.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Process chart of the proposed method.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p>To improve the precision of the skeleton points detection, an IPF was used instead of the traditional PF for estimation. For the sake of the safety of humans while improving efficiency, the robot velocity control is implemented based on the distance between the robots and humans. Once the robot-to-human distance is less than the safe distance, the speed of the robot begins to linearly reduce relative to the distance. Furthermore, in order to achieve real-time obstacle avoidance, the most commonly used strategy is to repeatedly plan the path, but this will greatly increase the avoidance time and the avoidance number, reducing the intelligence and flexibility of the system. Therefore, a rule-based logic system was developed to analyze the behaviors of the human so that the system could take different measures based on the current motion states of the human. When the human is static, the system plans a new path using DRM. If the human approaches slowly, the system builds a bounding sphere for the human and then plans a new path using DRM. When the human approaches quickly, the robot immediately stops. According to the current motion state of the human, the robot adopts different reaction strategies, thus improving the overall intelligence of the system. In this way, not only can the avoidance time of the robot caused by the repetitive planning path be reduced, but also the required cost can be reduced.</p></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IV.</div><h2>Modeling of Humans and Robots</h2></div><div class=\"section_2\" id=\"sec4a\"><h3>A. Building Bounding Cylinders for Humans Using Kinect</h3><p>The human movement tracking approach is driven by two key design goals: computational efficiency and robustness. Nowadays, various depth cameras are widely used in human motion recognition as the position of the skeletal joints can be estimated by the depth image caught by these cameras. As illustrated in <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_4a\" data-range=\"ref25\">[25]</a>, a single input depth image is segmented into dense body parts with different probabilities and labels, which are used to estimate the 3D skeletal joints. In the proposed method, Kinect is used as a depth camera for extracting human motion data considering that the Kinect has a larger infrared sensor size and a wider field of view and produces a depth image with a higher quality relative to other depth cameras <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_4a\" data-range=\"ref26\">[26]</a>. The Kinect also provides an application program interface (API) for human tracking and positioning. When someone enters the robot\u2019s workspace, the API can detect the human according to the characteristics of the three-dimensional data and calculate the location of the human\u2019s skeleton.</p><p>As mentioned above, the joints of the skeleton can be located using Kinect. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a> shows the 15 skeleton joints in the RGB image. The 15 skeleton joint points are numbered from top to bottom and left to right. Cylinders are used to construct the human body based on the volume of the different parts of the body (<a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>). Pairs of adjacent points are used to build each cylinder. The distance between the two adjacent points was used to determine the radius and length of the cylinders, so accounting for the sizes of different persons. The cylinders are different for every part of the body. For example, the cylinder for the torso is bigger than the cylinder for the arm. The scaling relationship between the size of a cylinder and the length of the corresponding two adjacent points can be calculated using statistical data.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq2ab-2979878-large.gif\" data-fig-id=\"fig2\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq2ab-2979878-small.gif\" alt=\"FIGURE 2. - Joint points in the 3D space.1: HEAD; 2: SHOULDER CENTER; 3: RIGHT SHOULDER; 4: RIGHT ELBOW; 5: RIGHT WRIST; 6: LEFT SHOULDER; 7: LEFT ELBOW; 8: LEFT WRIST; 9: HIP CENTER; 10: RIGHT HIP; 11: RIGHT KNEE; 12: RIGHT FOOT; 13: LEFT HIP; 14: LEFT KNEE; 15: LEFT FOOT.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Joint points in the 3D space.1: HEAD; 2: SHOULDER CENTER; 3: RIGHT SHOULDER; 4: RIGHT ELBOW; 5: RIGHT WRIST; 6: LEFT SHOULDER; 7: LEFT ELBOW; 8: LEFT WRIST; 9: HIP CENTER; 10: RIGHT HIP; 11: RIGHT KNEE; 12: RIGHT FOOT; 13: LEFT HIP; 14: LEFT KNEE; 15: LEFT FOOT.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Building Bounding Cylinders for Robots with DH Parameters</h3><p class=\"has-inline-formula\">The standard Denavit-Hartenberg (D-H) <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_4b\" data-range=\"ref27\">[27]</a> convention is the most widely used means of describing robot kinematics <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_4b\" data-range=\"ref27\">[27]</a>. The position of the robot and its tools and their orientation are defined according to the controller conventions. Through consecutive homogeneous transformations from the base coordinate to the robot tool coordinate, the kinematic equation can be defined as follows:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} T_{N}^{0} =T_{N}^{0} (v)=T_{1}^{0} T_{2}^{1} \\cdots T_{N}^{N-1} =\\prod \\limits _{i=1}^{N} {T_{i}^{i-1}}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} T_{N}^{0} =T_{N}^{0} (v)=T_{1}^{0} T_{2}^{1} \\cdots T_{N}^{N-1} =\\prod \\limits _{i=1}^{N} {T_{i}^{i-1}}\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$T_{i}^{i-1}$\n</tex-math></inline-formula> is the translation matrix from the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i-1$\n</tex-math></inline-formula> coordinate to the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i $\n</tex-math></inline-formula> coordinate, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> is the number of joints (or joints coordinates). The position of each joint <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> can be calculated using <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Eq. (1)</a>.<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} T_{N}^{j} =\\prod \\limits _{i=1}^{j} {T_{i}^{i-1}}\\tag{2}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} T_{N}^{j} =\\prod \\limits _{i=1}^{j} {T_{i}^{i-1}}\\tag{2}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">Because the size of each link can be obtained from factory parameters, the radius of the cylinder of each link can be determined. The length of the bounding cylinder of link <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is the distance between the position of joint <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j-1$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. Then, the cylinder of each link can be built after the radius and the length is determined (<a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a>).\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq3-2979878-large.gif\" data-fig-id=\"fig3\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq3-2979878-small.gif\" alt=\"FIGURE 3. - GOOGOL GRB3016 robot and the Skeleton.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>GOOGOL GRB3016 robot and the Skeleton.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Collision Detection for Cylinders</h3><p class=\"has-inline-formula\">Because both the robot and the human are bounded by cylinders, using cylinder-cylinder collision detection can efficiently approximate the collision case between a human and a robot. Given two colliding cylinders <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>, the relative positions between them could be classified into three categories according to whether the cylinders intersect or not at their bases, as shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>. The case in which one base of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> interact with one base of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> is called base-intersection (<a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4(a)</a>). The case in which the two cylinders intersect only at the sides is called lateral-quadrature (<a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4(b)</a>). And the case in which one base of one cylinder intersects with the side of another cylinder is called lateral-heterotrophic (<a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4(c)</a>).\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq4-2979878-large.gif\" data-fig-id=\"fig4\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq4-2979878-small.gif\" alt=\"FIGURE 4. - Possible relative positions of colliding cylinders.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Possible relative positions of colliding cylinders.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p>For details about collision detection for cylinder-shaped bodies, please refer to <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_4c\" data-range=\"ref28\">[28]</a>.</p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION V.</div><h2>Human Tracking Using Improved Particle Filter</h2></div><div class=\"section_2\" id=\"sec5a\"><h3>A. Improved PF</h3><p class=\"has-inline-formula\">Although the human skeleton position can be measured by the Kinect, the inherent noises of the sensor can increase over time. To reduce the measurement errors and noise, an improved PF was used to improve the reliability of the proposed methodology. The PF algorithm, which is a state estimator <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_5a\" data-range=\"ref29\">[29]</a>, can be used to approximate the posterior with a finite number of state samples that have corresponding normalized weights. At time <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t_{k}$\n</tex-math></inline-formula>, the approximation of posterior density can be defined as follows <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_5a\" data-range=\"ref29\">[29]</a>:<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} p\\left ({{x_{k} \\vert z_{1:k}} }\\right)\\approx \\sum \\limits _{i=1}^{N} {w_{i,k} \\delta \\left ({{x_{k} -x_{i,k}} }\\right)}\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} p\\left ({{x_{k} \\vert z_{1:k}} }\\right)\\approx \\sum \\limits _{i=1}^{N} {w_{i,k} \\delta \\left ({{x_{k} -x_{i,k}} }\\right)}\\tag{3}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{i, k}$\n</tex-math></inline-formula> is the position state of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> particle at time <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t_{k}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$z_{i:k}$\n</tex-math></inline-formula> represents the observations, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> is the number of samples, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$w_{i, k}$\n</tex-math></inline-formula> is the normalized weight of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> particle at time <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$t_{k}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\delta (\\cdot)$\n</tex-math></inline-formula> is the Dirac delta function.</p><p class=\"has-inline-formula\">As in one previous study <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_5a\" data-range=\"ref30\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_5a\" data-range=\"ref31\">[31]</a>, an ensemble Kalman filter (EnKF) is used to approximate<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vphantom {\\sum _{R}}$\n</tex-math></inline-formula> the posterior probability function (PDF) of state variables <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\left \\{{x_{i,k} }\\right \\}_{i=1}^{N}$\n</tex-math></inline-formula>. Given the initial ensembles <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\left \\{{x_{i,0} }\\right \\}_{i=1}^{N}$\n</tex-math></inline-formula>, the forecast ensembles <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\left \\{{ x_{i,k}^{f} }\\right \\}_{i=1}^{N}$\n</tex-math></inline-formula> can be calculated as follows:<disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} x_{i,k}^{f} =f(x_{i,k-1}^{a})+w_{i,k-1},w_{i,k-1} \\sim N(0,Q_{k-1})\\tag{4}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} x_{i,k}^{f} =f(x_{i,k-1}^{a})+w_{i,k-1},w_{i,k-1} \\sim N(0,Q_{k-1})\\tag{4}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$w_{k}$\n</tex-math></inline-formula> is the model error and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Q_{k-1} $\n</tex-math></inline-formula> represents the covariance of the model error. The analyzed particles can be calculated as follows:<disp-formula id=\"deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} x_{i,k}^{a} =x_{i,k}^{f} +K_{k} [z_{k} -h(x_{i,k}^{f})+v_{i,k}],v_{i,k} \\sim N(0,R_{k})\\tag{5}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} x_{i,k}^{a} =x_{i,k}^{f} +K_{k} [z_{k} -h(x_{i,k}^{f})+v_{i,k}],v_{i,k} \\sim N(0,R_{k})\\tag{5}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{k}$\n</tex-math></inline-formula> is the observation error, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$R_{k} $\n</tex-math></inline-formula> represents the covariance of the observed error and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$h(\\cdot)$\n</tex-math></inline-formula> is the measurement matrix. The Kalman gain <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$K_{k}$\n</tex-math></inline-formula> can be obtained as follows:<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} {K_{k} =P_{k}^{f} H^{T}(HP_{k}^{f} H^{T}+R_{k})^{-1}} \\\\ {P_{k}^{f} H^{T}=1/(N-1)\\displaystyle \\sum \\limits _{i=1}^{N} {(x_{i,k}^{f} -\\bar {x}_{k}^{f})[h(x_{i,k}^{f})-h(\\bar {x}_{k}^{f})]^{T}}} \\\\ {HP_{k}^{f} H^{T}\\!=\\!1/(N\\!-\\!1)\\displaystyle \\sum \\limits _{i=1}^{N} {[h(x_{i,k}^{f})-h(\\bar {x}_{k}^{f})][h(x_{i,k}^{f})\\!-\\!h(\\bar {x}_{k}^{f})]^{T}}} \\\\ {\\bar {x}_{k}^{f} =1/N\\displaystyle \\sum \\limits _{i=1}^{N} {x_{i,k}^{f}}} \\\\ \\end{cases}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\\\\\tag{6}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} {K_{k} =P_{k}^{f} H^{T}(HP_{k}^{f} H^{T}+R_{k})^{-1}} \\\\ {P_{k}^{f} H^{T}=1/(N-1)\\displaystyle \\sum \\limits _{i=1}^{N} {(x_{i,k}^{f} -\\bar {x}_{k}^{f})[h(x_{i,k}^{f})-h(\\bar {x}_{k}^{f})]^{T}}} \\\\ {HP_{k}^{f} H^{T}\\!=\\!1/(N\\!-\\!1)\\displaystyle \\sum \\limits _{i=1}^{N} {[h(x_{i,k}^{f})-h(\\bar {x}_{k}^{f})][h(x_{i,k}^{f})\\!-\\!h(\\bar {x}_{k}^{f})]^{T}}} \\\\ {\\bar {x}_{k}^{f} =1/N\\displaystyle \\sum \\limits _{i=1}^{N} {x_{i,k}^{f}}} \\\\ \\end{cases}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\\\\\tag{6}\\end{align*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">According to the literature <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_5a\" data-range=\"ref32\">[32]</a>, the weight updating formula is calculated as follows:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} w_{i,k} =w_{i,k-1} \\frac {p\\left ({{z_{k} \\vert x_{i,k}} }\\right)\\cdot p\\left ({{x_{i,k} \\vert x_{i,k-1}} }\\right)}{r\\left ({{x_{i,k} \\vert x_{i,k-1},z_{k} } }\\right)}\\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} w_{i,k} =w_{i,k-1} \\frac {p\\left ({{z_{k} \\vert x_{i,k}} }\\right)\\cdot p\\left ({{x_{i,k} \\vert x_{i,k-1}} }\\right)}{r\\left ({{x_{i,k} \\vert x_{i,k-1},z_{k} } }\\right)}\\tag{7}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$r\\left ({x_{i,k}\\vert x_{i,k-1},z_{k} }\\right)$\n</tex-math></inline-formula> is the importance density. When <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$r\\left ({x_{i,k}\\vert x_{i,k-1},z_{k} }\\right)$\n</tex-math></inline-formula> is selected as <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p\\left ({x_{i,k}\\vert x_{i,k-1} }\\right)$\n</tex-math></inline-formula>, <a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">Eq. (7)</a> can be expressed as:<disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} w_{i,k} =w_{i,k-1} p\\left ({{z_{k} \\vert x_{i,k}} }\\right)\\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} w_{i,k} =w_{i,k-1} p\\left ({{z_{k} \\vert x_{i,k}} }\\right)\\tag{8}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">To avoid particle degradation, particles must be resampled and then the same weight (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$w_{i,k}=1/N$\n</tex-math></inline-formula>) is assigned to all the particles. A Markov Chain Monte Carlo (MCMC) method <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_5a\" data-range=\"ref32\">[32]</a> is used to improve the diversity of particles after resampling. The transition process is implemented as follows:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\int {p(x_{k} \\vert z_{1:k})k(x_{k}^{\\ast } \\vert x_{k})dx_{k} }= p(x_{k}^{\\ast } \\vert z_{1:k})\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\int {p(x_{k} \\vert z_{1:k})k(x_{k}^{\\ast } \\vert x_{k})dx_{k} }= p(x_{k}^{\\ast } \\vert z_{1:k})\\tag{9}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k(x_{k}^{\\ast } \\vert x_{k})$\n</tex-math></inline-formula> is the Markov transition kernel.</p><p class=\"has-inline-formula\">In the Metropolis Hasting algorithm, the resampled particles move to the proposed particle only if <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$u \\le a$\n</tex-math></inline-formula>, which are defined as <disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} {a=\\min \\left({1,\\dfrac {p(z_{k} \\vert x_{k}^{\\ast })}{p(z_{k} \\vert x_{k})}}\\right)} \\\\ {u\\sim U[{0,1}]} \\\\ \\end{cases}\\tag{10}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} {a=\\min \\left({1,\\dfrac {p(z_{k} \\vert x_{k}^{\\ast })}{p(z_{k} \\vert x_{k})}}\\right)} \\\\ {u\\sim U[{0,1}]} \\\\ \\end{cases}\\tag{10}\\end{align*}\n</span></span></disp-formula></p><p>After the MCMC step, the new particles are more diverse because they have a distribution closer to the posterior PDF.</p></div><div class=\"section_2\" id=\"sec5b\"><h3>B. Estimation of Skeleton Position Using Improved PF</h3><p class=\"has-inline-formula\">In the proposed method, the human skeleton position is measured by the Kinect and an improved PF is used to estimate the position. Each particle position has three <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{IPF}^{i}$\n</tex-math></inline-formula> states (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\begin{aligned} x_{IPF,k}^{i}=\\left [{ {\\begin{array}{cccccccccccccccccccc} p_{x} &amp; p_{y} &amp; p_{z}\\\\ \\end{array}} }\\right] \\end{aligned}$\n</tex-math></inline-formula>). The deterministic input is set to zero since there is no deterministic input to the system. The accumulated position difference of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> particle at the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$s^{\\mathrm {th}}$\n</tex-math></inline-formula> position iteration as follows:<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-.5pc} JD_{s}^{i} =\\sum \\limits _{k=(s-1)M_{s} +1}^{M_{s} \\cdot s} \\left \\{{(p_{px,k}^{i} -p_{kx,k}^{i})^{2}+(p_{py,k}^{i} -p_{ky,k}^{i})^{2}}\\right. \\\\&amp;\\qquad\\qquad\\qquad\\qquad\\left.{ +\\,(p_{pz,k}^{i} -p_{kz,k}^{i})^{2} }\\right \\}\\tag{11}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-.5pc} JD_{s}^{i} =\\sum \\limits _{k=(s-1)M_{s} +1}^{M_{s} \\cdot s} \\left \\{{(p_{px,k}^{i} -p_{kx,k}^{i})^{2}+(p_{py,k}^{i} -p_{ky,k}^{i})^{2}}\\right. \\\\&amp;\\qquad\\qquad\\qquad\\qquad\\left.{ +\\,(p_{pz,k}^{i} -p_{kz,k}^{i})^{2} }\\right \\}\\tag{11}\\end{align*}\n</span></span></disp-formula> Here, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$M_{s} = \\Delta T_{s}/t$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p_{p-axis,k}^{i}$\n</tex-math></inline-formula> is the position state of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> particle and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p_{k-axis,k}^{i} $\n</tex-math></inline-formula> is the position state of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> particle estimated by EnKF at time <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>. Through the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$JD_{s}^{i}$\n</tex-math></inline-formula> values, the weight of each particle would be recalculated in every <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\Delta T_{s}$\n</tex-math></inline-formula> period. Then, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$JD_{s}^{i} $\n</tex-math></inline-formula> should be adopted in the IPF instead of the direct measurements <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_5b\" data-range=\"ref33\">[33]</a>. Therefore, the approximate posterior is represented by:<disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} p\\left ({x_{s}\\vert JD_{1:s}^{i} }\\right)\\approx \\sum \\nolimits _{i=1}^{N} {\\omega _{i,k}\\delta \\left ({x_{s}-x_{0:s}^{i} }\\right)}\\tag{12}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} p\\left ({x_{s}\\vert JD_{1:s}^{i} }\\right)\\approx \\sum \\nolimits _{i=1}^{N} {\\omega _{i,k}\\delta \\left ({x_{s}-x_{0:s}^{i} }\\right)}\\tag{12}\\end{equation*}\n</span></span></disp-formula> Then, the following equation is satisfied:<disp-formula id=\"deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} p\\left ({x_{s}\\vert JD_{1:s}^{i} }\\right)\\propto p\\left ({JD_{s}^{i}\\vert x_{s} }\\right)\\!\\cdot \\! p\\left ({x_{s}\\vert x_{s-1} }\\right)\\!\\cdot \\!p\\left ({x_{s-1}\\vert JD_{1:s-1}^{i} }\\right) \\\\\\tag{13}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} p\\left ({x_{s}\\vert JD_{1:s}^{i} }\\right)\\propto p\\left ({JD_{s}^{i}\\vert x_{s} }\\right)\\!\\cdot \\! p\\left ({x_{s}\\vert x_{s-1} }\\right)\\!\\cdot \\!p\\left ({x_{s-1}\\vert JD_{1:s-1}^{i} }\\right) \\\\\\tag{13}\\end{align*}\n</span></span></disp-formula> and the weight of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> particle <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_5b\" data-range=\"ref33\">[33]</a> can be expressed as:<disp-formula id=\"deqn14\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\omega _{i,s} \\propto \\frac {p\\left ({{JD_{s}^{i} \\vert x_{i,s}} }\\right)\\cdot p\\left ({{x_{i,s} \\vert x_{i,s-1}} }\\right)}{r\\left ({{x_{i,s} \\vert x_{i,s-1},JD_{s}^{i}} }\\right)}\\cdot \\omega _{i,s-1}\\tag{14}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\omega _{i,s} \\propto \\frac {p\\left ({{JD_{s}^{i} \\vert x_{i,s}} }\\right)\\cdot p\\left ({{x_{i,s} \\vert x_{i,s-1}} }\\right)}{r\\left ({{x_{i,s} \\vert x_{i,s-1},JD_{s}^{i}} }\\right)}\\cdot \\omega _{i,s-1}\\tag{14}\\end{equation*}\n</span></span></disp-formula> By picking the important density and resampling, the weight of the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> particle can be obtained as follows:<disp-formula id=\"deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\omega _{i,s}\\propto p\\left ({JD_{s}^{i}\\vert x_{IPF,s}^{i} }\\right)\\tag{15}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\omega _{i,s}\\propto p\\left ({JD_{s}^{i}\\vert x_{IPF,s}^{i} }\\right)\\tag{15}\\end{equation*}\n</span></span></disp-formula></p></div></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION VI.</div><h2>Fast Path Planning Using DRM</h2></div><p>In dynamic environments, the angles between joints and the degree of rotation will change a lot. The motions of the robot in three-dimensional space are very complex and may be occluded at any time considering that obstacles are mostly dynamic. In this way, obstacle avoidance is necessary for the robot to work in changing environments. In this paper, dynamic roadmap (DRM <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_6\" data-range=\"ref34\">[34]</a>), an adaptive real-time path planning method that incorporates a pre-processing stage and a planning stage, was adopted for obstacle avoidance. DRM uses pre-computation to increase online efficiency by generating a pre-processed roadmap. For a given time, DRM can quickly determine which parts of the roadmap are blocked before each planning step. An enhanced DRM can be produced through a pre-processing stage and a planning stage <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_6\" data-range=\"ref35\">[35]</a>.</p><p class=\"has-inline-formula\">In the preprocessing stage, the first step is to build undirected roadmaps in the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$q$\n</tex-math></inline-formula>-dimensional configuration space of the robot, in which <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$q$\n</tex-math></inline-formula> is the number of robot joints. The second step is to look for conflict points and edges in the roadmaps. First, the nodes of the roadmaps were created using a uniform sampling method. Then, for each node, according to pseudo-norm space, its <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> nearest neighbors were identified provided that the segment between the two nodes was collision-free and connected as part of the roadmap. The discrete configurations at roadmap nodes and along edges for self-collision were checked by using a related algorithm. The direct workspace metric was defined as follows:<disp-formula id=\"deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} d_{\\infty }^{w} (p,q)=\\max \\limits _{a\\in A} \\left \\|{ {a(p)-a(q)} }\\right \\|\\tag{16}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} d_{\\infty }^{w} (p,q)=\\max \\limits _{a\\in A} \\left \\|{ {a(p)-a(q)} }\\right \\|\\tag{16}\\end{equation*}\n</span></span></disp-formula> Here, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> is the set of all reference points on the surface of the robot and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$a(p)$\n</tex-math></inline-formula> is the location of reference point <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> in the workspace if the robot is in configuration <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$p$\n</tex-math></inline-formula>. <a ref-type=\"disp-formula\" anchor=\"deqn16\" href=\"#deqn16\" class=\"fulltext-link\">Eq. (16)</a> provides the maximum degree to which any reference point may be displaced between two checked configurations. If the robot is bounded by a convex polyhedron and all of its vertices are selected as reference points, the metric gives the maximum displacement of any point on the robot.</p><p>It is to be noted that self-collision points and edges must be removed when creating the roadmap. For this reason, after creating the roadmaps, in order to look for conflict points and edges, the DRM method was used to create a map of the area between the workspace grid and the configuration space roadmap. The planning stage involves the following steps: invalidating blocked parts of the roadmap, connecting start, goal configurations to the roadmap and searching the graph.</p></div>\n<div class=\"section\" id=\"sec7\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION VII.</div><h2>Robot Velocity Control</h2></div><p>In a human-robot collaborative environment, human safety can be difficult to guarantee.</p><p class=\"has-inline-formula\">In order to ensure the safety of humans while improving the efficiency of the robot in the process of human-robot cooperation, the movement speed of the robot is determined based on the distance between the robot and the human. When the present distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{present}$\n</tex-math></inline-formula> is less than the safety distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}$\n</tex-math></inline-formula>, the speed of the robot decreases linearly with <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{present}$\n</tex-math></inline-formula>. When the present distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{present}$\n</tex-math></inline-formula> is less than the minimum distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula>, the robot stops working. As shown in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a>, according the distance between the robot and the human, the zone is divided into three regions including the safe zone, the acc/dec zone and the stopping zone.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq5-2979878-large.gif\" data-fig-id=\"fig5\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq5-2979878-small.gif\" alt=\"FIGURE 5. - Three regions for the robot manipulator.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Three regions for the robot manipulator.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\">In the proposed method, the robot takes different speeds based on the distance between the robot and the human. The allowable speed of the robot is defined in the three areas (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a>) as follows <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_7\" data-range=\"ref36\">[36]</a>:<disp-formula id=\"deqn17\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} V_{r}=\\begin{cases} V_{R} &amp; S_{safe} &lt; S_{present}\\\\ f(S_{present},V_{h},T_{r},\\\\ T_{b},S_{b},C,Z) &amp; S_{min} &lt; S_{present} &lt; S_{safe}\\\\ 0 &amp; S_{present} &lt; S_{min}\\\\ \\end{cases}\\tag{17}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} V_{r}=\\begin{cases} V_{R} &amp; S_{safe} &lt; S_{present}\\\\ f(S_{present},V_{h},T_{r},\\\\ T_{b},S_{b},C,Z) &amp; S_{min} &lt; S_{present} &lt; S_{safe}\\\\ 0 &amp; S_{present} &lt; S_{min}\\\\ \\end{cases}\\tag{17}\\end{align*}\n</span></span></disp-formula> The safe distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}$\n</tex-math></inline-formula> is given by:<disp-formula id=\"deqn18\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} S_{safe}=V_{h}(T_{r}+T_{b})+V_{r} T_{r}+S_{b}+C+Z\\\\ T_{b}=\\dfrac {T_{b}\\vert _{V_{max}}}{V_{max} V_{r}}\\\\ \\end{cases}\\tag{18}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} S_{safe}=V_{h}(T_{r}+T_{b})+V_{r} T_{r}+S_{b}+C+Z\\\\ T_{b}=\\dfrac {T_{b}\\vert _{V_{max}}}{V_{max} V_{r}}\\\\ \\end{cases}\\tag{18}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{h}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{r}$\n</tex-math></inline-formula> represent the speed of the human and the robot, respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$T_{r}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$T_{b,}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{b}$\n</tex-math></inline-formula> denote the collision avoidance system reaction time, the braking time with respect to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{r}$\n</tex-math></inline-formula> and the braking distance of the robot, respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Z$\n</tex-math></inline-formula> represent the resolution distance of the sensor and the uncertainty distance based on the human and the sensor. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{max}$\n</tex-math></inline-formula> is the maximum speed of the robot under the maximum load. The minimum distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula> is defined as:<disp-formula id=\"deqn19\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} S_{\\min } =V_{h} T_{r} +C+Z \\quad when V_{r} \\approx 0\\tag{19}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} S_{\\min } =V_{h} T_{r} +C+Z \\quad when V_{r} \\approx 0\\tag{19}\\end{equation*}\n</span></span></disp-formula></p><p class=\"has-inline-formula\">From <a ref-type=\"disp-formula\" anchor=\"deqn17\" href=\"#deqn17\" class=\"fulltext-link\">Eq. (17)</a>, When the distance between the robot and the person is greater than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}$\n</tex-math></inline-formula>, the speed of the robot <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{R}$\n</tex-math></inline-formula> remains static. When the distance is greater than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula> and less than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}$\n</tex-math></inline-formula>, the speed of the robot mainly depends on the speed of the robot and the person and the braking distance. When the distance is less than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula>, the robot stops moving to ensure safety.</p></div>\n<div class=\"section\" id=\"sec8\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION VIII.</div><h2>Active Collision Avoidance Using Rule-Based Logic System</h2></div><p>Since human behavior is unpredictable, real-time path planning alone cannot achieve intelligent obstacle avoidance. Our collision avoidance system uses a knowledge-based set of rules for decision making <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_8\" data-range=\"ref36\">[36]</a>. The knowledge-based contains rules that are usually expressed in the form of \u201cIF A THEN B,\u201d where A is the antecedent conditions and B is the consequences.</p><p class=\"has-inline-formula\">In the proposed approach, when operators enter the robot\u2019s workspace, that is, when the distance of the operator from the robot <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR}$\n</tex-math></inline-formula> is lower than the dangerous distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR\\_{}min}$\n</tex-math></inline-formula> (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR\\_{}min}$\n</tex-math></inline-formula> refers to the maximum working range of the robot and its tool), the collision avoidance system is triggered. According to the distance of the Kinect from the industrial robot, the maximum detection radius <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$R_{max}$\n</tex-math></inline-formula> of the sensor can be determined. From the above Section of the robot velocity control, the safety distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}$\n</tex-math></inline-formula> represents the distance for which the robot can avoid people in time. Let <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{safe}=R_{max}$\n</tex-math></inline-formula>, and formula <a ref-type=\"disp-formula\" anchor=\"deqn18\" href=\"#deqn18\" class=\"fulltext-link\">(18)</a> can be rewritten as follow:<disp-formula id=\"deqn20\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} R_{\\max } \\!=\\!V_{H\\_{}danger} (T_{r} \\!+\\!T_{b} \\vert _{V_{\\max }})\\!+\\!V_{\\max } T_{r} \\!+\\!S_{b} \\!+\\!C+Z\\tag{20}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} R_{\\max } \\!=\\!V_{H\\_{}danger} (T_{r} \\!+\\!T_{b} \\vert _{V_{\\max }})\\!+\\!V_{\\max } T_{r} \\!+\\!S_{b} \\!+\\!C+Z\\tag{20}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{max}$\n</tex-math></inline-formula> represents the maximum speed of the robot under the maximum load. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$T_{r}$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$T_{b}\\vert _{Vmax,}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{b}$\n</tex-math></inline-formula> denote the collision avoidance system reaction time, the braking time with respect to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{max}$\n</tex-math></inline-formula> and the braking distance of the robot, respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$Z$\n</tex-math></inline-formula> represent the resolution distance of the sensor and the uncertainty distance based on the human and the sensor. Once the allowable maximum speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$V_{max}$\n</tex-math></inline-formula> of the robot is determined, the allowable maximum speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula> of the people can be obtained. Furthermore, the final speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula> is determined by simulating the robot obstacle avoidance in the simulation environment. To achieve active collision avoidance, a set of rules is formulated in three cases where a human may enter the robot\u2019s workspace:</p><p class=\"has-inline-formula\">Case 1: The human is approaching at a fast pace. When the human gets close to the manipulator at the speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H}&gt;v_{H\\_{}danger}$\n</tex-math></inline-formula> m/s (where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula> is a dangerous speed), the system cannot ensure the new planned path is safe during the next iteration. The wisest choice is to avoid the human rather than plan a new path right away. When the human gets close to the manipulator in the direction of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vec {D}_{H}$\n</tex-math></inline-formula>, the manipulator should avoid the human in the direction of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vec {D}_{H}$\n</tex-math></inline-formula> so that they do not collide.</p><p class=\"has-inline-formula\">Case 2: The human is slowly approaching the robot. When the human gets close to the manipulator at the speed (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$0 &lt; v_{H} &lt; v_{H\\_{}danger }$\n</tex-math></inline-formula> m/s), the system predicts the motion trail of the human and plan a new path to avoid the human using DRM. Because of the randomness of human movement, the system calculates a bounding sphere that should contain all possible motion trails in a period (including cases in which the human suddenly stops). Then the robot should plan a new path to avoid the calculated bounding sphere rather than the human during that period. When the human suddenly accelerated (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}}&gt; v_{H\\_{}danger}$\n</tex-math></inline-formula> m/s), the system should respond as in case 1.</p><p class=\"has-inline-formula\">Case 3: The human is immobile. When the human stands near the robot, the system judges whether the human impedes the robot\u2019s movements. If so, the system plans a new path using DRM to avoid him or her. In this case, the robot does not need to avoid a bounding sphere. Instead, the system can plan a shorter, more efficient path. When the human suddenly moves at the speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}}&gt; v_{H\\_{}danger}$\n</tex-math></inline-formula> m/s, the system should respond as in case 1. Otherwise, the system should respond as in case 2 when the speed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$0 &lt; v_{H} &lt; v_{H\\_{}danger}$\n</tex-math></inline-formula> m/s.</p><p class=\"has-inline-formula\">The rules can be expressed as follows:\n<ul style=\"list-style-type: none;\" data-outdent=\"yes\"><li value=\"1\" data-list-item-label=\"Rule 1:\"><h4 class=\"inline run-in\">Rule 1:</h4><p class=\"has-inline-formula\">IF (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}}&gt; v_{H\\_{}danger}$\n</tex-math></inline-formula>) &amp; (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR} &lt; D_{HR\\_{}min}$\n</tex-math></inline-formula>),</p><p class=\"has-inline-formula\">THEN <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">${P_{R}=P_{R}+\\left ({V_{H-max} }\\right)\\cdot \\vec {D}}_{H}$\n</tex-math></inline-formula></p></li><li value=\"2\" data-list-item-label=\"Rule 2:\"><h4 class=\"inline run-in\">Rule 2:</h4><p class=\"has-inline-formula\">IF (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$0 &lt; v_{H} &lt; v_{H\\_{}danger}$\n</tex-math></inline-formula>) &amp; (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR} &lt; D_{HR\\_{}min}$\n</tex-math></inline-formula>),</p><p class=\"has-inline-formula\">THEN <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{R}=P_{New,Bounding}$\n</tex-math></inline-formula></p></li><li value=\"3\" data-list-item-label=\"Rule 3:\"><h4 class=\"inline run-in\">Rule 3:</h4><p class=\"has-inline-formula\">IF (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H}=0$\n</tex-math></inline-formula>) &amp; (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR} &lt; D_{HR\\_{}min}$\n</tex-math></inline-formula>),</p><p class=\"has-inline-formula\">THEN <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{R}=P_{New,Static}$\n</tex-math></inline-formula></p></li></ul>where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{R}$\n</tex-math></inline-formula> is the next position of the robot. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{New,Bounding}$\n</tex-math></inline-formula> is the new plan calculated by DRM. With this plan, the robot can avoid the bounding sphere. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{New,Static}$\n</tex-math></inline-formula> is the new plan calculated by DRM, with which the robot can avoid the human.</p></div>\n<div class=\"section\" id=\"sec9\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IX.</div><h2>Experiment</h2></div><div class=\"section_2\" id=\"sec9a\"><h3>A. Experimental Environment</h3><p>A series of experiments were performed to assess the effectiveness of the proposed method. The time requirement and efficiency of the proposed method were compared to those of previous methods <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9a\" data-range=\"ref19\">[19]</a> and <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9a\" data-range=\"ref21\">[21]</a>. During the experiments, the proposed active collision avoidance system was executed on an eight-core CPU, of which four were for the visualization and the robot movement control and the other four were for the complex position calculation and planning path.</p><p class=\"has-inline-formula\">In the proposed method, a GOOGOL GRB3016 robot was used to finish the experiments. The link parameters in D-H model of the robot are listed in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. In the robot control system, a working path was designed, and the robot was made to move along this path repetitively by using the reverse kinematics algorithm <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_9a\" data-range=\"ref37\">[37]</a>. An emulation scene including the robot manipulator model and the human model was built into the robot control system. In order to locate the 3D positions of the human and the robot, a position measurement system with a Kinect and a calibration board was designed. The calibration board was tightly attached to the robot near its base and the Kinect was firmly fixed to a tripod, which was placed 2.1 meters away horizontally, 1.6 meters away vertically and 1.6 meters height relative to the robot base (see <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a>). The resolution of the Kinect depth images was <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$640\\times480$\n</tex-math></inline-formula> and the capture frequency was 30 Hz. In order to determine the position of the robot and the human, the angle of the Kinect was adjusted before the experiments, so that the human skeleton and the calibration board can be captured in real-time. Suppose that <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$(x_{k},y_{k},z_{k})$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$(x_{b},y_{b},z_{b})$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$(x_{c},y_{c},z_{c})$\n</tex-math></inline-formula> are the frames of the Kinect, robot base and the calibration board respectively. The relative location between the robot base frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{B} Y_{B} Z_{B}$\n</tex-math></inline-formula> and the calibration target frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{C} Y_{C} Z_{C}$\n</tex-math></inline-formula> could be measured offline using a ruler. A method described in previous work <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_9a\" data-range=\"ref38\">[38]</a> is used to calculate the relative location between the calibration target frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{C} Y_{C} Z_{C}$\n</tex-math></inline-formula> and the Kinect frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{K} Y_{K} Z_{K}$\n</tex-math></inline-formula>. Then, the Kinect frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{K} Y_{K} Z_{K}$\n</tex-math></inline-formula>, with respect to the robot base frame <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$X_{B} Y_{B} Z_{B}$\n</tex-math></inline-formula>, could be determined using a calibration target (<a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig. 7</a>). The transformation from the robot base to the calibration board should be measured and determined before any experiments begin. During the experiments, the positions of the human were obtained by the depth information from the Kinect. By capturing the images of the calibration board with the RGB camera in the Kinect, the position of the robot could be determined by detecting the corners of the calibration board.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nLink Parameters in D-H Model for the GOOGOL GRB3016 Robot</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t1-2979878-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t1-2979878-small.gif\" alt=\"Table 1- &#10;Link Parameters in D-H Model for the GOOGOL GRB3016 Robot\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq6-2979878-large.gif\" data-fig-id=\"fig6\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq6-2979878-small.gif\" alt=\"FIGURE 6. - Experimental environment.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Experimental environment.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq7-2979878-large.gif\" data-fig-id=\"fig7\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq7-2979878-small.gif\" alt=\"FIGURE 7. - Structure of the system.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Structure of the system.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\">The goal is to allow the robot to avoid the human who has entered its workspace. In the experiments, to ensure the safety of the human, the robot velocity control is introduced, and the movement speed of the robot is determined based on the distance between the robot and the human. When the distance is less than the minimum distance <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$S_{min}$\n</tex-math></inline-formula>, the robot would stop working. Moreover, in order to further protect the human from the accidental injuries in the event of an experimental failure in the experiments, we set the force threshold to 50 Newton which is less than the maximum permissible forces of various part of the body as illustrated in the Collaborative Robot Collision Standard ISO/TS 15066. When the detected force is greater than the set value, the robot stops working. In this way, when incidental contact between robots and humans occurs, contact shall not result in pain or injury. The parameters used for the experiment were as follows: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}=0.2$\n</tex-math></inline-formula> m/s, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$D_{HR\\_{}min}=0.2$\n</tex-math></inline-formula> m.</p><p>During the experiments, the robot moved all the way along the designed path and the position of the robot EE with respect to the Kinect frame was calculated by the robot control system in real-time. When a human enters its workspace, the skeleton of the person was detected immediately by the Kinect. With the captured human skeleton, the position of the human related to the Kinect frame and the speed at which the human moves could be determined by the robot control system. Once the system determined that the human was approaching the robot, the active collision-avoidance system performed collision avoidance according to the speed of the human\u2019s movements. Five testers were invited to carry out the experiments to verify the efficiency of the proposed method. The testers are aged from 20 to 26, four males and one female. Note that the testers were asked to approach the robot at different speeds. Firstly, the testers moved toward the robot at a relatively high speed which was more than 0.2 m/s. Then, the testers approached the robot at a speed between 0 and 0.2 m/s. Finally, the tester moves away from the robot. Each experiment was repeated three times, for a total of 15 times.</p></div><div class=\"section_2\" id=\"sec9b\"><h3>B. Experimental Results</h3><p>In the experiment, the testers attempted to interrupt the robot\u2019s work. <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig. 8 (a)</a> shows the experimental scene. In order to monitor the robot\u2019s environment, a three-dimensional virtual scene was constructed as shown in <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig. 8(b)</a>. During the initialization of the virtual scene, there was only one virtual robot. When the Kinect sensor detected a human, the sensor calculated the position of his or her skeleton in real environment and then a 3D skeleton and a 2D skeleton were drawn in the virtual scene (<a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig. 8(b)</a>) and on the 2D image (<a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig. 8(c)</a>), respectively.\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq8abc-2979878-large.gif\" data-fig-id=\"fig8\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq8abc-2979878-small.gif\" alt=\"FIGURE 8. - Experimental scene: (a) RGB image. (b) Virtual scene. (c) Three-dimensional virtual model projected onto the two-dimensional image.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>Experimental scene: (a) RGB image. (b) Virtual scene. (c) Three-dimensional virtual model projected onto the two-dimensional image.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\"><a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig. 9</a> shows the experimental procedure. The human tried to approach the robot, then broke into its planned path, and finally left it. The avoidance component of the robot was equal to the approach component of the human, as shown in <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig. 10</a>. The pink dotted line is the movement trajectory of the human, the blue solid line is the new path of the robot, and the red line is the old path of the robot. In the beginning, as the human was approaching the robot quickly, the system found that his or her speed exceed <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula>. Then the robot tried to avoid the human directly. When the human slowed down, the system calculated a bounding sphere to plan a new path for the robot.\n<div class=\"figure figure-full\" id=\"fig9\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq9abcd-2979878-large.gif\" data-fig-id=\"fig9\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq9abcd-2979878-small.gif\" alt=\"FIGURE 9. - Experimental procedure: (a) The human approached the robot. (b) The human broke into the planned path of the robot. (c) The robot avoided human. (d) The human left.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 9. </b><fig><p>Experimental procedure: (a) The human approached the robot. (b) The human broke into the planned path of the robot. (c) The robot avoided human. (d) The human left.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig10\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq10-2979878-large.gif\" data-fig-id=\"fig10\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq10-2979878-small.gif\" alt=\"FIGURE 10. - Three-dimensional trajectory.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 10. </b><fig><p>Three-dimensional trajectory.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\"><a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig. 11</a> shows the velocity curve of both the human and the robot in the direction of the connections between them. From 0 s to 1.3 s, the human accelerated as he or she approached the robot. Because the approach of the component was less than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula>, the avoidance component of the robot was 0. In the period <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{\\mathrm {H1}}$\n</tex-math></inline-formula> between 1.3 s to 2.4 s, the approach component was greater than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$v_{H\\_{}danger}$\n</tex-math></inline-formula>, the robot accelerated to avoid the human. The avoidance component should be equal to the approach component. After 2.4<sup>th</sup> s (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{\\mathrm {H2}}$\n</tex-math></inline-formula>), the human slowed down and then left the robot after 4.2 s (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{\\mathrm {H3}}$\n</tex-math></inline-formula>). Because the robot needed to avoid the human by bypassing the bounding sphere, the avoidance component remained positive until 3.95 s (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{\\mathrm {R2}}$\n</tex-math></inline-formula>). Then, when the human began to move away from the robot, the robot returned to its previous path (<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$P_{\\mathrm {R3}}$\n</tex-math></inline-formula>).\n<div class=\"figure figure-full\" id=\"fig11\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq11-2979878-large.gif\" data-fig-id=\"fig11\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq11-2979878-small.gif\" alt=\"FIGURE 11. - Velocity curve of the human and robot in the direction of the connections between the human and the robot.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 11. </b><fig><p>Velocity curve of the human and robot in the direction of the connections between the human and the robot.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p>In the experiment, the proposed method was compared with method <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and method <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a>. When a human is approaching the robot at a faster speed, the robot moved to avoid him directly, while in these previous methods, the robot realizes obstacle avoidance by repeatedly planning a new path. When the speed of the human slows down, the robot avoids him or her by avoiding the calculated boundary sphere, while in methods <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a>, the robot still avoids humans by repeatedly planning new paths. <a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig. 12</a> shows the comparison of the proposed method and other methods in the robot moving trajectory.\n<div class=\"figure figure-full\" id=\"fig12\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq12-2979878-large.gif\" data-fig-id=\"fig12\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq12-2979878-small.gif\" alt=\"FIGURE 12. - Robot moving trajectory.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 12. </b><fig><p>Robot moving trajectory.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13</a> shows the comparative results in X, Y, Z. Compared with methods <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a>, in the X direction, it is obvious that the path of the proposed method is most close to the default trajectory, which is nearly the same as the default trajectory. And in the Y and Z direction, though three methods\u2019 paths all have clear differences with the default trajectory from 7<sup>th</sup> sec to the 10<sup>th</sup> sec, we can find that the difference between the proposed method and default path is minimum. In general, the path of the proposed method is most close to the default trajectory in a three-dimension space. It is because that the proposed method can correctly respond to different human movement speeds by analyzing the human\u2019s behaviors and choose the best path for the robot so that the flexibility and intelligence of the robot movement can be improved.\n<div class=\"figure figure-full\" id=\"fig13\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq13abc-2979878-large.gif\" data-fig-id=\"fig13\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq13abc-2979878-small.gif\" alt=\"FIGURE 13. - The comparison in robot moving trajectory: (a) In X direction (b) In Y direction (c) In Z direction.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 13. </b><fig><p>The comparison in robot moving trajectory: (a) In X direction (b) In Y direction (c) In Z direction.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig. 14</a> shows the comparisons of the proposed method, method <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and method <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a> in terms of the avoidance time, the number of avoidance events and the increased route. <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> lists the statistics for the data in <a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig. 14</a>, including the average, minimum and maximum values of the five testers using the three methods. In the proposed method, the system had to move to avoid the human 3.4 times on average, which collectively took 4.76s on average. In the method <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a>, because the system planned a new path for the robot repeatedly, the robot had to move 8.4 times on average to avoid the human and spent more time doing it, at an average of 11.52s. In the method <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a>, the average number of avoidance events and average avoidance time are 5.2 times and 6.72s, respectively. Because the robot moved in a new path to avoid the human, the route was longer than the initial one. In the current method, the path is increased by an average of 783.2 mm, and the paths that were built by method <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and method <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a> are 1173.2 mm and 937.4 mm, respectively.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nComparisons of the Proposed Method, Method [19] and Method [21]</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t2-2979878-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t2-2979878-small.gif\" alt=\"Table 2- &#10;Comparisons of the Proposed Method, Method [19] and Method [21]\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig14\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq14abc-2979878-large.gif\" data-fig-id=\"fig14\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq14abc-2979878-small.gif\" alt=\"FIGURE 14. - Comparisons of the three methods in terms of: (a) Avoidance Time (b) Number of Avoidance Events (c) Increased route.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 14. </b><fig><p>Comparisons of the three methods in terms of: (a) Avoidance Time (b) Number of Avoidance Events (c) Increased route.</p></fig></div><p class=\"links\"><a href=\"/document/9031397/figures\" class=\"all\">Show All</a></p></div></p><p>Moreover, a serial of statistical tests is conducted. The statistical test data of the paired t-test are shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. All the two-tail p-values are less than alpha level (0.05), which indicated that the proposed method has better performance than method <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_9b\" data-range=\"ref19\">[19]</a> and method <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_9b\" data-range=\"ref21\">[21]</a> significantly in our experiments.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nStatistical Test of the Proposed Method, Method [19] and Method [21]</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t3-2979878-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9031397/chunq.t3-2979878-small.gif\" alt=\"Table 3- &#10;Statistical Test of the Proposed Method, Method [19] and Method [21]\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>When the human slowed down, the proposed method calculated a bounding sphere for the robot so that it could avoid the human quickly, which saved a considerable amount of time. However, the previous methods constantly calculated a new path for the robot so that the robot could move around the human. Although the increase in the length of the route was less pronounced using the previous methods, avoidance time and avoidance events increased significantly. It can be concluded that the proposed rule-based logic system can formulate reasonable response strategies according to different behaviors of people. In addition, the establishment of a cylindrical bounding box model for human bones and robots helps to improve the timeliness and effectiveness of the collision avoidance system.</p></div></div>\n<div class=\"section\" id=\"sec10\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION X.</div><h2>Discussions</h2></div><p>For the sake of safe interaction, human beings and robots are usually required to work in separate spaces. In current industrial applications that lack sensor surveillance, robot work cells must be static. If a human enters the robot\u2019s work cell, some additional strategies must be used to ensure the safety of both the human and the robot. Existing applications for collaboration between robots and humans are relatively common. These applications allow robots to work in less space in situations in which there are no physical barriers or in which no humans are involved. In complex environments, humans may be mistaken for operation objects, like workpieces. Robots\u2019 lack of independent intelligence can place humans in danger. This paper uses Kinect to detect humans who entered the robot\u2019s workspace and the IPF is applied to improve the accuracy of estimating the skeleton position. By building bounding cylinders for the human and the robot, the collision detection between the human and the robot can be achieved by detecting the relative position of the cylinders, which can improve the effectiveness and accuracy of the system. In order to make a reasonable reaction strategy for the obstacle avoidance system based on human behaviors, a rule-based logic system is proposed, and DRM is applied to plan a new path to avoid humans. When humans are immobile, DRM is used to plan a new path for the robot directly. When the humans are moving slowly, a bounding sphere is calculated, and the robot bypasses it by planning a new path using DRM. When the human is moving at a fast pace, the robot stops working immediately. By taking different measures, the system was found to actively protect humans.</p><p>In future work, voice control will be added to the system so that the robot can talk to any human who enters its workspace. The robot can be made more clearly aware of humans\u2019 intentions. This may make avoidance more efficient.</p></div>\n<div class=\"section\" id=\"sec11\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION XI.</div><h2>Conclusion</h2></div><p>This paper proposes an active collision avoidance method for robots using the Kinect sensor and the IPF. This proposed method can effectively detect human breaking into the workspace of the robots. In the proposed method, the collision detection is achieved by detecting the relative position of the bounding cylinders of the human and the robot. To take different reaction strategies according to human movement, a rule-based logic system was proposed to analyze the behavior of the human and DRM was used to plan a new path for the robot to avoid the human. Finally, experiments were performed to show the effectiveness of the proposed algorithm. Experimental results demonstrated that this proposed method could be applied to practical applications such as protecting the behaviors of humans who enter a robot\u2019s workspace in a human-robot collaborative environment.</p></div>\n</div></div></response>\n"
}