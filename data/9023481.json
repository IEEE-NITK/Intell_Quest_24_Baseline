{
    "abstract": "Breast cancer is the most common cancer with highest mortality risk among the female worldwide and breast mass is the most effective sign for cancer identification. Thus, accurate segmentation of breast mass is regarded as a key step to reduce the death rate. Traditional segmentation methods require prior knowledge and manually set parameters, while recent studies prefer to construct neural networ...",
    "articleNumber": "9023481",
    "articleTitle": "Spatial Enhanced Rotation Aware Network for Breast Mass Segmentation in Digital Mammogram",
    "authors": [
        {
            "preferredName": "Yulin Cheng",
            "normalizedName": "Y. Cheng",
            "firstName": "Yulin",
            "lastName": "Cheng",
            "searchablePreferredName": "Yulin Cheng",
            "id": 37089524834
        },
        {
            "preferredName": "Ying Gao",
            "normalizedName": "Y. Gao",
            "firstName": "Ying",
            "lastName": "Gao",
            "searchablePreferredName": "Ying Gao",
            "id": 37086476091
        },
        {
            "preferredName": "Linsen Xie",
            "normalizedName": "L. Xie",
            "firstName": "Linsen",
            "lastName": "Xie",
            "searchablePreferredName": "Linsen Xie",
            "id": 37088649114
        },
        {
            "preferredName": "Xinyan Xie",
            "normalizedName": "X. Xie",
            "firstName": "Xinyan",
            "lastName": "Xie",
            "searchablePreferredName": "Xinyan Xie",
            "id": 37089524474
        },
        {
            "preferredName": "Wengen Lin",
            "normalizedName": "W. Lin",
            "firstName": "Wengen",
            "lastName": "Lin",
            "searchablePreferredName": "Wengen Lin",
            "id": 37089524446
        }
    ],
    "doi": "10.1109/ACCESS.2020.2978009",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9023481/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION I.</div><h2>Introduction</h2></div><p>Breast cancer is one of the most harmful diseases among the female worldwide. According to 2018 Global Cancer Statistics <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\" data-range=\"ref1\">[1]</a>, breast cancer sufferers account for a quarter of all female cancer patients. Even more shocking, about 3% of early-stage patients die from it <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\" data-range=\"ref2\">[2]</a>. Therefore, early diagnosis is highly suggested for reducing death rate of breast cancer. As is known to all, breast mass is one of the most effective signs for cancer identification <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\" data-range=\"ref3\">[3]</a>. Thus, breast mass segmentation on medical image is regarded as the first step of early diagnosis and the key step prior to classification of benign and malignant. Traditional approaches for breast mass segmentation are manually, time-consuming and heavily dependent on radiologist\u2019s experience. To reduce processing time and improve the accuracy of segmentation result, computer-aided detection (CADe) technology has been rapidly developed since the late 1980s <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\" data-range=\"ref4\">[4]</a> and digital mammogram is the most reliable technique which widely used in breast mass segmentation <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\" data-range=\"ref5\">[5]</a>. However, breast masses are varied in a wide range in shape, size and texture, which make the segmentation remain a challenging task <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6\">[6]</a>.</p><p>Various machine learning algorithms were utilized to establish traditional CADe systems for disease detection and segmentation <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\" data-range=\"ref7\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\" data-range=\"ref8\">[8]</a>, especially for breast mass segmentation <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\" data-range=\"ref9\">[9]</a>. Region growing and thresholding were the two most widely used methods. With respect to region growing, Mencattini <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\" data-range=\"ref10\">[10]</a> introduced an effective region growing algorithm for breast mass segmentation on Digital Database for Screening Mammography (DDSM) <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\" data-range=\"ref11\">[11]</a> dataset. They followed the typical processing flow which consists of artifacts removal, contrast enhancement and segmentation. Moreover, an iterative post processing step was designed to remove peninsulas on the boundary of mass. A precision of 93.38% and a recall of 88.34% were achieved on a subset of DDSM with 200 images. Regarding to thresholding technique, Kom <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\" data-range=\"ref12\">[12]</a> reported promising results for detecting mass using a local adaptive thresholding method combined with linear transformation enhancement. The adaptive threshold value was calculated based on two context windows of distinct sizes. Their method achieved great performance on a dataset of 61 mammograms images and got a sensitive of 95.91%. Besides, energy function-based methods <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\" data-range=\"ref13\">[13]</a> and clustering based methods <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\" data-range=\"ref14\">[14]</a> were also adapted as the detectors. To integrate the advantages of different techniques, some studies combined several techniques in a processing flow and achieved great improvement in breast mass segmentation. Min <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\" data-range=\"ref6\">[6]</a> used multi-scale morphological filtering and simple linear iterative clustering segmentation to detect suspicious areas of different sizes. Multiple cascaded random forests were employed to make the final decision based on the features extracted from suspicious areas. Two sensitives of 94% and 77% were achieved by their approach for INbreast and DDSM BCRP respectively. Chakraborty <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\" data-range=\"ref15\">[15]</a> presented a multilevel method controlled by thresholding and region growing in a coarse-to-fine manner. At each step, a thresholding method was used to find the focal region of a mass, and then, a region growing algorithm was employed to detect the accurate mass using gradient and intensity information. With a mixed dataset of 107 images from the mini-MIAS database and 158 digital radiography images from a local database, two sensitives of 95% and 98.8% were achieved by their approach respectively. Sharma <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\" data-range=\"ref16\">[16]</a> combined watershed segmentation and k-means clustering to detect breast mass. The texture features extracted from suspicious areas were used to identify the true breast mass.</p><p>Although these methods achieved good performances, they were relatively simple and sensitive to some initialization parameters, such as seed, threshold value and number of cluster centroids. In most cases, the parameters or the initialization strategies were decided by experience. Otherwise, these methods highly rely on image enhancement and lack the abilities to extract required pattern from raw image data directly. This made them may ineffective in complex scenes and large-scale dataset.</p><p>In recent years, convolutional neural network (CNN) has developed rapidly and has become the most commonly selected deep learning structure in image processing domain. CNN can extract features by abundant kernels automatically and fuse them in a non-linear manner. All the parameters of CNN can be initialized randomly, adjusted by back propagation and trained in an end-to-end manner. Based on these characteristics, CNN has achieved great improvement in most computer vision tasks, such as image classification <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\" data-range=\"ref17\">[17]</a>, object detection <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\" data-range=\"ref18\">[18]</a> and semantic segmentation <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\" data-range=\"ref19\">[19]</a>.</p><p>Regarding to semantic segmentation using traditional CNN, the input image is divided into small patches using sliding window technique and the label of the central pixel of each patch is predicted by CNN <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\" data-range=\"ref20\">[20]</a>. However, small image patch only contains local context information about the patch itself and losses global context information about the whole image. Because of these, the performance of traditional CNN is limited. To address these problems, Long <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\" data-range=\"ref21\">[21]</a> proposed fully convolutional networks (FCN) for segmentation. FCN replaces all the fully connected layers by convolution layers. For this reason, the input of FCN is allowed in arbitrarily size. As the size of segmentation result should be the same as the input, FCN upsamples the feature maps from high level layers and introduces skip connection to fuse the outputs from different layers. The experimental results showed that the spatial information supplied by skip connection and gradually upsampling can improve the performance of FCN. However, the segmentation details are still far from expectation. Inspired by FCN, many successful neural networks were proposed for image segmentation task, such as U-Net <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\" data-range=\"ref19\">[19]</a>, DeepLab <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\" data-range=\"ref22\">[22]</a> and PSPNet <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\" data-range=\"ref23\">[23]</a>. Among these works, the encoder-decoder structure of U-Net is the easiest to implement and extend. The encoder part of U-Net employs multi-stage convolutions connected by maxpooling layers to extract features and expand receptive fields, while the decoder part contains multi-stage convolutions connected by upsampling layers to fuse the features and expand the size. Skip connection is applied in every decoder stage to concatenate the feature map from upsampling layers with the feature maps from the corresponding stage in encoder. Benefit from small stride upsampling to expand the size gradually and skip connection to make up for spatial information, U-Net achieves great improvement in image segmentation.</p><p>Afterwards, various U-Net based architectures have been proposed to handle different application scenes <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\" data-range=\"ref24 ref25 ref26 ref27\">[24]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_1\" data-range=\"ref24 ref25 ref26 ref27\">[27]</a>, especially in breast mass segmentation <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1\" data-range=\"ref28 ref29 ref30\">[28]</a>\u2013\u200b<a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_1\" data-range=\"ref28 ref29 ref30\">[30]</a>. Li <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1\" data-range=\"ref28\">[28]</a> proposed an Attention Dense-U-net for breast mass segmentation on a subset of DDSM. They used dense connected block to remould the encoder and applied Attention Gates <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_1\" data-range=\"ref31\">[31]</a> at skip connection step for spatial information enhancement. An Area under the Receiver Operating Characteristic Curve (AUC) of 0.8605 was reported. Hai <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_1\" data-range=\"ref29\">[29]</a> remoulded both the encoder and decoder by dense connected block. Moreover, they held the opinion that the multi-scale context was useful in segmentation task. Thus, they used atrous spatial pyramid pooling (ASPP) mechanism for multi-scale context captation. Their method achieved the best performance on a self-collected dataset with 380 mammograms in total. Different from the approaches mentioned above, Sun <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_1\" data-range=\"ref30\">[30]</a> proposed an attention-guided dense-upsampling block to decrease the information loss in upsampling operation. The original upsampling operation was replaced by a dense-upsampling block and a channel-wise attention was equipped for information enhancement. An average Dice coefficient of 81.8% was achieved on CBIS-DDSM dataset.</p><p>To improve the encoder part of U-Net, most of the studies mentioned above held the opinion that feature reuse, which can be modeled by residual connection <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\" data-range=\"ref17\">[17]</a>, is the most important factor. Unlike them, we think that the feature extraction can be further improved by spatial attention. Spatial attention, which is mentioned in <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_1\" data-range=\"ref32\">[32]</a>, can adjust the focus to the region of interest and filter out interference information. Thus, it is possible to improve the feature extraction by explicitly modeling the spatial attention. For this concern, spatial attention block (SA-Block) with a mask attention branch is designed in this paper to provide soft spatial attention about suspicious regions. Moreover, breast masses can display in every orientations in complex scenes, which can be summarized as rotation. Most studies employed data augment method to improve the perception to mass rotation. Different from them, we design multi-stream rotation aware block (MSRA-Block), which contains two extra asymmetric convolutions in parallel, to enhance the rotation invariance and refine the prediction in decoder part.</p><p>As a summary of the foregoing, a U-type Spatial Enhanced Rotation Aware Network (SERAN) is designed for breast mass segmentation in digital mammogram in this paper. The proposed SERAN consists of a residual spatial attention encoder and a multi-stream rotation aware decoder. The encoder contains five stages and each stage is made up of a residual convolution block and a SA-Block. Maxpooling layer is applied at the end of each encoder stage to expand the receptive field. The decoder is made up of four stages and each stage consists of two MSRA-Blocks. To expand the size, deconvolution layer is used after each decoder stage. Moreover, skip connection is employed in each decoder stage for spatial information supplement.</p><p>To train a segmentation neural network, binary cross entropy loss or Dice loss is the most common choice. However, both of them are not efficient for model optimization due to data imbalance and the implicitly constraint of background area respectively. To address this problem, a novel regulation item named Inside-outside Loss (IOL) is used to maximize the probabilities inside the mass and minimize the probabilities outside the mass.</p><p>The rest of this paper is organized as follows. In <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>, we describe the details about dataset and pre-processing as well as the architecture of proposed SERAN. To verify the ability of proposed method, extensive experiments are summarized in <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>. Finally, a conclusion is drawn in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a>.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION II.</div><h2>Materials and Methodology</h2></div><p>This paper aims to build a Spatial Enhanced Rotation Aware Network (SERAN) for breast mass segmentation in digital mammogram. The overview of SERAN is illustrated as <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1</a>. The materials used in this study and the details of SERAN are described in following sections.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao1-2978009-large.gif\" data-fig-id=\"fig1\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao1-2978009-small.gif\" alt=\"FIGURE 1. - The architecture of Spatial Enhanced Rotation Aware Network (SERAN).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>The architecture of Spatial Enhanced Rotation Aware Network (SERAN).</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><div class=\"section_2\" id=\"sec2a\"><h3>A. Data Preparation</h3><p class=\"has-inline-formula\">DDSM database from the University of Florida is used in this study. Approximately 2,500 cases are collected in DDSM and each of them contains two X-ray mammograms about each breast. The database can be downloaded online and all the image sizes are larger than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$2000\\times 3000$\n</tex-math></inline-formula> pixels. The location and type of suspicious area in each image has been annotated by experienced radiologists as ground truth. The images are encoded in \u201c.LJPEG\u201d format and the corresponding ground truths are saved in \u201c.OVERLAY\u201d format. To transform the images and the ground truths to \u201c.PNG\u201d format, which can be easily used during programing, a public tool named DDSM-LJPEG-Converter is used.</p><p>In this study, 400 representative images are selected from DDSM by an experience radiologist. The standard of the selection can be described as following: 1) the mass is relative clear in the image, and 2) the annotated area can cover the mass but not much bigger than the mass. The images are randomly divided into three nonoverlapping parts in a ratio of 4:1:1, which are used as training set, validation set and test set.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Pre-Processing and Data Augmentation</h3><p>Each image in DDSM is composed of breast region and background. However, some artifacts exist in the background. These artifacts can be regarded as noise and affect the ability of neural network badly. Thus, we design several steps to remove the artifacts in the background for denoising. Firstly, we employ gamma transformation to correct image and enhance the details about the breast region. Secondly, the image is binarized by OTSU <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2b\" data-range=\"ref33\">[33]</a>. Then, the maximum connected area in the binary image is obtained as the breast region. With these, the pixel value of the breast region is recorded as 1 and the background is recorded as 0. Finally, the original image is multiplied by the breast region binary image to get the unlabeled image. The results are shown as <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao2ab-2978009-large.gif\" data-fig-id=\"fig2\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao2ab-2978009-small.gif\" alt=\"FIGURE 2. - (a) is the original image with artifacts and noises in the background, (b) is the result image after artifact removal.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>(a) is the original image with artifacts and noises in the background, (b) is the result image after artifact removal.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\">After that, all the images are resized to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$224\\times 224$\n</tex-math></inline-formula> based on bilinear interpolation for reducing the computation cost and all the pixel values in an image are scaled into a range from 0 to 255 by Min-Max normalization <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\" data-range=\"ref34\">[34]</a>. The sizes of masses are varied in a wide range and the distribution in each data subset is similar to the others (as shown in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig.3</a>).\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao3-2978009-large.gif\" data-fig-id=\"fig3\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao3-2978009-small.gif\" alt=\"FIGURE 3. - The distributions of mass size in each data subset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>The distributions of mass size in each data subset.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\">For training a deep neural network for segmentation, a large number of annotated images are required to avoid overfitting. To enlarge the training set, several data augmentation methods are applied. Firstly, each image is mirrored in horizontal direction. Then, all the images are rotated according to following principles: 1) take the central point of the image as the origin of coordinate system, 2) generate an angle in a range from -10 to 10 degree randomly, 3) rotate the image according to the origin point and the degree, 4) fill the extra area by 0 to form a rectangle image, and 5) crop the image to <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$224\\times 224$\n</tex-math></inline-formula> according to origin point. With these, the size of training set is quadruple and the data diversity is increased.</p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. Spatial Enhanced Rotation Aware Network</h3><p>The proposed SERAN consists of four parts: a residual spatial attention encoder, a multi-stream rotation aware decoder, skip connections and a final prediction layer. The descriptions of all the modules are summarized in following subsections.</p><div class=\"section_2\" id=\"sec2c1\"><h4>1) Residual Spatial Attention Encoder</h4><p>As shown in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig.3</a>, the shapes and sizes of different breast masses are varied in a wide range. Thus, it is required to build a powerful encoder for effective feature extraction.</p><p class=\"has-inline-formula\">Typical encoder is made up of convolution layers for feature extraction and pooling layers for receptive field expanding. All the layers are formed in a series connection style and the input of layer <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> is only the output of layer <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula>-1. However, the ability of encoder may degrade with gradient vanishing. To address this problem, He <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2c1\" data-range=\"ref17\">[17]</a> proposed ResNet with residual connection. Using residual connection, the input of convolution layer <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> is summed with the convolution result to form the output, which can be expressed as:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} x_{l+1} =\\sigma \\left ({{F\\left ({{x_{l}} }\\right)+H\\left ({{x_{l}} }\\right)} }\\right)\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} x_{l+1} =\\sigma \\left ({{F\\left ({{x_{l}} }\\right)+H\\left ({{x_{l}} }\\right)} }\\right)\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{l+1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{l}$\n</tex-math></inline-formula> are inputs of layer <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l+1$\n</tex-math></inline-formula> respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$F(\\cdot,\\omega)$\n</tex-math></inline-formula> is a learning function and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H(\\cdot)$\n</tex-math></inline-formula> is a mapping function. When the number of channels of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{l}$\n</tex-math></inline-formula> is equal to the channel number of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$F(\\cdot,\\omega)$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H(\\cdot)$\n</tex-math></inline-formula> is an identity mapping function (shown as <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig.4(a)</a>). Otherwise, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 1$\n</tex-math></inline-formula> convolution is used in <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H(\\cdot)$\n</tex-math></inline-formula> to adjust the channel number (shown as <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig.4(b)</a>). <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> refers to the ReLu <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2c1\" data-range=\"ref35\">[35]</a> function for activation.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao4ab-2978009-large.gif\" data-fig-id=\"fig4\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao4ab-2978009-small.gif\" alt=\"FIGURE 4. - Two variants of residual connected block: (a) basic residual block, (b) residual block with channel number adjustment. The numbers in the boxes are kernel size, filter number, stride, and padding respectively.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Two variants of residual connected block: (a) basic residual block, (b) residual block with channel number adjustment. The numbers in the boxes are kernel size, filter number, stride, and padding respectively.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>Residual connection can avoid gradient vanishing and provide feature reuse for better convergence. For these reasons, we employ the residual learning structure as a basic module in encoder part.</p><p>In addition, in our view, spatial information is the most important factor to boost the performance in segmentation task. Thus, it will be helpful to model the attention on spatial information explicitly.</p><p class=\"has-inline-formula\">Inspired by <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2c1\" data-range=\"ref32\">[32]</a>, a spatial attention block (SA-Block) is designed to enhance the spatial context information at each pixel position. As shown in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig.5</a>, the SA-Block consists of a feature extraction branch and a mask attention branch. The feature extraction branch utilizes two <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> convolution layers to capture features from complex image context. The mask attention branch is implemented as a U-Net liked structure. A single-channel attention map is generated by the mask attention branch. Each value in the attention map represents the weight of the corresponding pixel position in spatial dimension. Because the values in the attention map range from 0 to 1, the output of the feature extraction branch is used twice to avoid the degradation of feature values in the deep layers. The output of the feature extraction branch is weighted by the attention map firstly and then summed with the weighted result. The summed result is then activated by ReLu. The formulation of SA-Block can be summarized as:<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} H_{i,j,c} \\left ({x }\\right)=\\sigma \\left ({{\\left ({{1+M_{i,j} \\left ({x }\\right)} }\\right)\\ast F_{i,j,c} \\left ({x }\\right)} }\\right)\\tag{2}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} H_{i,j,c} \\left ({x }\\right)=\\sigma \\left ({{\\left ({{1+M_{i,j} \\left ({x }\\right)} }\\right)\\ast F_{i,j,c} \\left ({x }\\right)} }\\right)\\tag{2}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> are pixel locations in width, height and channel dimension respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$M(\\cdot)$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$F(\\cdot)$\n</tex-math></inline-formula> are the learning functions of mask attention branch and feature extraction branch respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> is the ReLu activation function and Batch Normalization (BN) <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2c1\" data-range=\"ref36\">[36]</a> is equipped after each convolution operation.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao5-2978009-large.gif\" data-fig-id=\"fig5\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao5-2978009-small.gif\" alt=\"FIGURE 5. - The structure of SA-Block.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>The structure of SA-Block.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>As well known, the low level layers in a deep CNN model capture shallow features, such as shape, size and texture, while the high level layers contain more abstract semantic information. From this point of view, the low level layers need stronger spatial attention. Thus, two similar mask attention branches with different depths are proposed, which are shown in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig.6</a>. The deeper mask attention branch is equipped in the shallow layers of the encoder and the shallower one is embedded into the deeper layers.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao6-2978009-large.gif\" data-fig-id=\"fig6\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao6-2978009-small.gif\" alt=\"FIGURE 6. - Structures of Mask Attention Branches in SA-Block of different depths.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Structures of Mask Attention Branches in SA-Block of different depths.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>Composed of residual convolution blocks and SA-Blocks, the encoder can focus on extracting features from suspicious places (as shown in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig.7</a>). Moreover, the responses of regions with similar features to the target in one image can be restrained gradually. The architecture of encoder is shown as the left part of <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1</a>.\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao7abc-2978009-large.gif\" data-fig-id=\"fig7\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao7abc-2978009-small.gif\" alt=\"FIGURE 7. - The top row shows the comparative prediction result (c) of original image (a) compared to ground truth (b). The bottom row shows the attention maps from each encoder stages.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>The top row shows the comparative prediction result (c) of original image (a) compared to ground truth (b). The bottom row shows the attention maps from each encoder stages.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec2c2\"><h4>2) Multi-Stream Rotation Aware Decoder</h4><p>In complex scenes, the masses can display in every orientations, which can be summarized as rotation. In order to assist doctors for precise diagnosis, the neural network must be aware to mass rotation and predict robust results in different rotations. The goal of each decoder stage is to fuse the features and refine the prediction of mass location and contour details. Thus, it is possible to boost the rotation robustness of the neural network by building a rotation aware decoder.</p><p class=\"has-inline-formula\">According to <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2c2\" data-range=\"ref37\">[37]</a>, different asymmetric convolutions are robust to different rotation scenes. As shown in <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig.8</a>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> kernel is much more robust to horizontal flipping than <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> kernel.\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao8-2978009-large.gif\" data-fig-id=\"fig8\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao8-2978009-small.gif\" alt=\"FIGURE 8. - Comparison between &#10;$3\\times 1$&#10; kernel (red) and &#10;$3\\times 3$&#10; kernel (yellow). The &#10;$3\\times 1$&#10; kernel obtains the same results on horizontal flipped input, but &#10;$3\\times 3$&#10; kernel gets different result.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p class=\"has-inline-formula\">Comparison between <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> kernel (red) and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> kernel (yellow). The <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> kernel obtains the same results on horizontal flipped input, but <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> kernel gets different result.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p class=\"has-inline-formula\">In order to extract more effective features and boost the rotation robustness, we design a multi-stream rotation aware block (MSRA-Block) to fuse features with different receptive fields (as shown in <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig.9</a>). The MSRA-Block contains a <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> convolution and two asymmetric convolutions whose kernels are <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 3$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> respectively. The three convolutions are used to get different purposes. The <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> convolution extracts features in a relatively larger receptive field. The <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 3$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> convolutions are used to boost the rotation robustness to vertical and horizontal flipping respectively. Moreover, the width of the decoder is expanded resulting in multi-scale feature fusion. The feature maps from the three streams are added up as a fusion result. BN and ReLu are used to activate the result in a nonlinear manner. The formulation of MSRA-Block can be described as:<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} x_{l+1} =\\sigma \\left ({{F_{3\\times 3} \\left ({{x_{l}} }\\right)+F_{1\\times 3} \\left ({{x_{l}} }\\right)+F_{3\\times 1} \\left ({{x_{l}} }\\right)} }\\right)\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} x_{l+1} =\\sigma \\left ({{F_{3\\times 3} \\left ({{x_{l}} }\\right)+F_{1\\times 3} \\left ({{x_{l}} }\\right)+F_{3\\times 1} \\left ({{x_{l}} }\\right)} }\\right)\\tag{3}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{l+1}$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$x_{l}$\n</tex-math></inline-formula> are inputs of layer <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l+1$\n</tex-math></inline-formula> respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$F_{\\mathrm {3\\times 3}}(\\cdot), F_{\\mathrm {1\\times 3}}(\\cdot)$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$F_{\\mathrm {3\\times 1}}(\\cdot)$\n</tex-math></inline-formula> are the learning functions of the three feature extraction streams with <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 3$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> kernels respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> is the ReLu function used for activation.\n<div class=\"figure figure-full\" id=\"fig9\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao9-2978009-large.gif\" data-fig-id=\"fig9\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao9-2978009-small.gif\" alt=\"FIGURE 9. - The structure of MSRA-Block.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 9. </b><fig><p>The structure of MSRA-Block.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>The decoder consists of four stages and each stage is composed of two MSRA-Blocks (shown as the right part of <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1</a>). The input of each stage is the concatenate feature maps which is formed by the corresponding skip connection and the deconvolution result of previous stage.</p></div><div class=\"section_2\" id=\"sec2c3\"><h4>3) Skip Connection and Final Prediction</h4><p>As a common view, the detail information lost in pooling-upsampling structure can be supplied through skip connection. Thus, skip connection is used to connect each encoder stage with corresponding decoder stage. The output of each SA-Block is treated as a part of the input of corresponding MSRA-Block through skip connection.</p><p class=\"has-inline-formula\">The output of the last MSRA-Block is used for final prediction. A <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 1$\n</tex-math></inline-formula> convolution is employed to fuse the features, reduce the number of channels and keep the spatial size. Two feature maps are generated and a channel-wise softmax operation is used to transform the pixel values to probabilities. One of the feature maps is regarded as background and another one predicts the mass.</p></div></div><div class=\"section_2\" id=\"sec2d\"><h3>D. Inside-Outside Loss</h3><p class=\"has-inline-formula\">To train a segmentation network, binary cross-entropy (BCE) loss was the most common choice due to its smooth derivative and the stable optimization procedure. The formulation of BCE can be denoted as:<disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} l^{bce}=&amp;-\\frac {1}{N}\\sum \\limits _{i} {\\sum \\limits _{j} {l_{i,j}^{ce}}} \\\\ l^{ce}=&amp;-\\left ({{y^{true}\\log y^{predict}+\\left ({{1-y^{true}} }\\right)\\log \\left ({{1-y^{predict}} }\\right)} }\\right) \\\\\\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} l^{bce}=&amp;-\\frac {1}{N}\\sum \\limits _{i} {\\sum \\limits _{j} {l_{i,j}^{ce}}} \\\\ l^{ce}=&amp;-\\left ({{y^{true}\\log y^{predict}+\\left ({{1-y^{true}} }\\right)\\log \\left ({{1-y^{predict}} }\\right)} }\\right) \\\\\\tag{4}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$l^{ce}$\n</tex-math></inline-formula> is the cross entropy loss of one pixel. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$y^{predict}$\n</tex-math></inline-formula> is the predicted label, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$y^{true}$\n</tex-math></inline-formula> denotes the actual label. BCE is the average cross entropy loss of all the pixels.</p><p class=\"has-inline-formula\">It is obvious that BCE regards segmentation task as pixel-wise classification task and treats each pixel equally. However, the number of pixels representing breast mass is only a small proportion of the entire image. In other words, there exists heavy data imbalance when applying BCE as the objective function. To address the problem, Dice loss was presented to measure the similarity between the predicted area and the ground truth, which can be denoted as:<disp-formula id=\"deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} l^{dice}=1-\\frac {2\\times \\left |{ {A\\cap B} }\\right |}{\\left |{ A }\\right |+\\left |{ B }\\right |}\\tag{5}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} l^{dice}=1-\\frac {2\\times \\left |{ {A\\cap B} }\\right |}{\\left |{ A }\\right |+\\left |{ B }\\right |}\\tag{5}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> is the predicted probability map and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> is the ground truth. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert \\cdot \\vert $\n</tex-math></inline-formula> is used to calculate the summation of pixel values. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert A\\cap B\\vert $\n</tex-math></inline-formula> represents the intersection area of prediction and ground truth, which is named True Positive (<i>TP</i>).</p><p class=\"has-inline-formula\">Dice loss offers a hidden linked constraint about the intersection area within mass region and the area outside mass region, which is indicated by <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert A\\vert $\n</tex-math></inline-formula>. The change of <i>TP</i> is the main factor affecting Dice loss. In our point of view, explicitly constrain the two parts can be more efficient for model optimization. Thus, a novel regulation item, named Inside-outside Loss (IOL), is designed to explicitly constrain both the intersection area and the area outside the mass for better optimization. The goal of IOL is to maximize the probabilities inside the ground truth area and minimize the outside probabilities. The formulation of IOL is shown as following:<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} InsideLoss=&amp;1-\\frac {\\left |{ {A\\cap B} }\\right |}{\\left |{ B }\\right |} \\\\ OutsideLoss=&amp;\\frac {\\left |{ A }\\right |-\\left |{ {A\\cap B} }\\right |}{H\\times W-\\left |{ B }\\right |} \\\\ IOL=&amp;InsideLoss+OutsideLoss\\tag{6}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} InsideLoss=&amp;1-\\frac {\\left |{ {A\\cap B} }\\right |}{\\left |{ B }\\right |} \\\\ OutsideLoss=&amp;\\frac {\\left |{ A }\\right |-\\left |{ {A\\cap B} }\\right |}{H\\times W-\\left |{ B }\\right |} \\\\ IOL=&amp;InsideLoss+OutsideLoss\\tag{6}\\end{align*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> is the predicted probability map and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula> is the ground truth. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$H$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> are the height and width of image respectively. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert \\cdot \\vert $\n</tex-math></inline-formula> is used to calculate the summation of pixel values. <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert A\\cap B\\vert $\n</tex-math></inline-formula> represents the intersection area of prediction and ground truth, which is named True Positive (<i>TP</i>). And <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert A\\vert $\n</tex-math></inline-formula>-<inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\vert A\\cap B\\vert $\n</tex-math></inline-formula> represents the misclassification in background area, which is named False Positive (<i>FP</i>). The InsideLoss indicates the probability loss inside the ground truth mass while the OutsideLoss indicates the loss outside the mass. In other words, the Insideloss focus on increasing <i>TP</i> indictor and the Outsideloss aims to reduce the <i>FP</i> indictor.</p><p class=\"has-inline-formula\">To avoid overfitting, we treat IOL as a regulation item and set a small weight for IOL. The final objective function is composed by Dice loss and IOL, which is defined as following:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} l^{total}=l^{dice}+\\lambda \\times IOL\\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} l^{total}=l^{dice}+\\lambda \\times IOL\\tag{7}\\end{equation*}\n</span></span></disp-formula> where <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula> is the weight of IOL. Using IOL, the network can avoid misclassification in background area and make a better prediction (as shown in <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig.10</a>).\n<div class=\"figure figure-full\" id=\"fig10\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao10-2978009-large.gif\" data-fig-id=\"fig10\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao10-2978009-small.gif\" alt=\"FIGURE 10. - Segmentation results produced by SERAN trained by different loss function.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 10. </b><fig><p>Segmentation results produced by SERAN trained by different loss function.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION III.</div><h2>Experiments and Analysis</h2></div><div class=\"section_2\" id=\"sec3a\"><h3>A. Evaluation Metrics</h3><p>To estimate the performance of proposed approach, five commonly used evaluation metrics in image segmentation task are adopted, namely Sensitive, Specificity, Accuracy, Intersection Over Union (IOU) and Dice coefficient. All of the metrics are denoted as following:<disp-formula id=\"deqn8-deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Sensitive=&amp;\\frac {TP}{TP+FN} \\tag{8}\\\\ Specificity=&amp;\\frac {TN}{TN+FP} \\tag{9}\\\\ Accuracy=&amp;\\frac {TP+TN}{TP+TN+FP+FN} \\tag{10}\\\\ IOU=&amp;\\frac {TP}{TP+FN+FP} \\tag{11}\\\\ Dice=&amp;\\frac {2\\times TP}{2\\times TP+FP+FN}\\tag{12}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Sensitive=&amp;\\frac {TP}{TP+FN} \\tag{8}\\\\ Specificity=&amp;\\frac {TN}{TN+FP} \\tag{9}\\\\ Accuracy=&amp;\\frac {TP+TN}{TP+TN+FP+FN} \\tag{10}\\\\ IOU=&amp;\\frac {TP}{TP+FN+FP} \\tag{11}\\\\ Dice=&amp;\\frac {2\\times TP}{2\\times TP+FP+FN}\\tag{12}\\end{align*}\n</span></span></disp-formula> where <i>TP</i> is the number of pixels correctly predicted in mass region and <i>TN</i> is the number of correct predicted pixels in background. <i>FP</i> are the pixels predicted wrong as mass and <i>FN</i> are the wrong pixels as background. The confusion matrix is showed in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> to make a clear explanation about <i>TP</i>, <i>TN</i>, <i>FP</i> and <i>FN</i>.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nConfusion Matrix</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t1-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t1-2978009-small.gif\" alt=\"Table 1- &#10;Confusion Matrix\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Moreover, an average value (MEAN) of the results of 5 independent experiments is recorded as the final result to preclude statistical error and result bias. Furthermore, standard deviation (STD) is computed to reflect performance stability.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Implementation Details</h3><p class=\"has-inline-formula\">Adam <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3b\" data-range=\"ref38\">[38]</a> method is employed as an optimizer to optimize the model. The parameters of Adam are set as: <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta _{1}=0.9$\n</tex-math></inline-formula>, <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\beta _{2}=0.999$\n</tex-math></inline-formula>, and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\varepsilon =1e-8$\n</tex-math></inline-formula>. The learning rate is initialized to 0.001 and halved every 20 epochs after the 70<sup>th</sup> epoch. The batch size is set to 16 and the weight of IOL is set to 0.01 in training phase.</p><p>Several early stopping strategies are used to avoid overfitting. If the loss of validation set keeps increasing in 5 consecutive epochs or changes within a small range in 10 consecutive epochs, the training procedure will stop and the model with the lowest validation loss will be saved.</p><p>All the preprocessing steps and data augment methods are implemented on Python interface of OpenCV library. The proposed neural network is implemented on Python interface of Pytorch platform, while related experimentations are tested using Intel(R) Core(TM) i7-6850K @ 3.60GHz CPU and two GeForce GTX 1080p GPUs on Ubuntu16.04.1. The versions of Python, OpenCV and Pytorch are 3.6.6, 3.4.3 and 1.0.1 respectively.</p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Experiment Results</h3><p>In this section, the investigation of each proposed module and strategy is performed to analysis the contributions. And the comprehensive comparison with 5 state-of-the-art segmentation methods is presented at the end of this Section to show the advancement of proposed SERAN. U-Net is implemented as a baseline model. All the results are achieved in test set to show the generalization ability of each methods. Since the values of Specificity and Accuracy are close, we record them in more detail.</p><div class=\"section_2\" id=\"sec3c1\"><h4>1) Contribution Analysis of Residual Spatial Attention Encoder</h4><p>To analyze the contribution of residual spatial attention encoder, a spatial enhanced network (SEN) is constructed after replacing the encoder of U-Net by proposed encoder. The performances of SEN and U-Net are showed in the top 2 rows in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> for evaluation. In addition, both SEN and U-Net are trained by Dice loss.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nContribution Analysis (MEAN\u00b1STD)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t2-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t2-2978009-small.gif\" alt=\"Table 2- &#10;Contribution Analysis (MEAN\u00b1STD)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>In summary, SEN wins 9 out of 10 comparisons and most of the metrics are improved significantly. Due to the explicitly modeling of spatial attention, SEN can focus more on the region of interest and make the feature extraction more effective. As shown in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig.7</a>, the number of focuses is obviously decreased and the response values in mass region are relatively bigger in the attention map of deeper layer. That means the network owns the ability to identify the mass from multiple regions with high response values. Moreover, the performance of SEN is more stable than U-Net, which is indicated by STDs.</p></div><div class=\"section_2\" id=\"sec3c2\"><h4>2) Contribution Analysis of Multi-Stream Rotation Aware Decoder</h4><p>As shown in the line 2 and line 3 of <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>, the performances of SERAN and SEN are showed to investigate the effect of multi-stream rotation aware decoder mentioned in <a ref-type=\"sec\" anchor=\"sec2c2\" class=\"fulltext-link\">Section II.C.2</a>. SERAN transforms the decoder part of SEN by the proposed one. For the sake of fairness, Dice loss is used to train SERAN.</p><p>SERAN wins all the competitions about average performance. Due to the multi-stream structure, the width of the decoder is expanded and features from different aspects are fused. Thus, the prediction result can be refined to a certain extent. The Accuracy and Specificity of SERAN is better and more stable than U-Net and SEN. That means the location of suspicious area is easier to detect by using SERAN. Although SERAN seemed more unstable than U-Net and SEN in Sensitive, IOU and Dice coefficient, the overall performance of SERAN is relatively more outstanding.</p><p>In order to analyze the effectiveness of multi-stream rotation aware mechanism, we expand the test set using the data augment method mentioned in <a ref-type=\"sec\" anchor=\"sec2b\" class=\"fulltext-link\">Section II.B</a>. The masses in the expanded test set may display in different orientations due to the mirror operation and rotation operation. The experimental results are summarized in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. It is obvious that both SEN and SERAN achieve much better performances than U-Net due to the effective encoder we designed. Benefit from the multi-stream rotation aware mechanism, SERAN wins 8 out of 10 comparisons facing mass rotation. Although the performance of SERAN has declined to some extent compared to the performance on original test set, it is still at a relatively high level.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nPerformance on Expanded Test Set (MEAN\u00b1STD)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t3-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t3-2978009-small.gif\" alt=\"Table 3- &#10;Performance on Expanded Test Set (MEAN\u00b1STD)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3c3\"><h4>3) Contribution Analysis of Inside-Outsider Loss</h4><p>In this part, effectiveness of IOL is investigated. The line 4 of <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> shows the results achieved by SERAN, which is trained by Dice loss with IOL.</p><p>Compared to the performances of SERAN trained without IOL (as shown in the line 3 of <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>.), IOL can obviously improve the performances of SERAN. It achieves 8 comparable results among 10 comparisons. Accuracy, IOU and Dice coefficient of SERAN are significantly improved. According to the visual results shown in <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig.10</a>, IOL can avoid misclassification in background area and optimize the network for better prediction, which are the leading causes for the improvement.</p><p class=\"has-inline-formula\">For the purpose to determine the weight of IOL, we test several values of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula>, which are 1, 0.1, 0.01 and 0.001. The experimental results are showed in <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> and the visual results are showed in <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig.11</a>. The loss function used in this work is a combination of Dice loss and IOL, which is described in <a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">(7)</a>. According to <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">(4)</a>\u2013\u200b<a ref-type=\"disp-formula\" anchor=\"deqn5\" href=\"#deqn5\" class=\"fulltext-link\"/><a ref-type=\"disp-formula\" anchor=\"deqn6\" href=\"#deqn6\" class=\"fulltext-link\">(6)</a>, the change of <i>TP</i> is the main factor affecting the loss function. The Outsideloss in IOL aims to reduce the <i>FP</i> indictor. Thus, it can avoid the prediction of multiple regions. The Insideloss in IOL focus on the perceptron about the region of mass. When the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula> is small, such as 0.001, IOL has little effect on the loss function. Thus, the performance of SERAN trained by <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda =0.001$\n</tex-math></inline-formula> is similar to the one trained by Dice loss and multiple region prediction occurs (as shown in <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig.11(d)</a>). When the <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula> gets larger, IOL affects the loss function a lot. Thus, SERAN trends to predict the mass with larger size than ground truth to get the highest <i>TP</i> when trained by <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda =1$\n</tex-math></inline-formula> (as shown in <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig.11(a)</a>). That is the reason to explain why it gets the highest Sensitive but falls in other metrics. To make a balance about constraints of the mass and the background, we determined the value of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula> to 0.01 in this work based on the experimental results.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nPerformance of Different Weight of IOL (MEAN\u00b1STD)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t4-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t4-2978009-small.gif\" alt=\"Table 4- &#10;Performance of Different Weight of IOL (MEAN\u00b1STD)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig11\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao11-2978009-large.gif\" data-fig-id=\"fig11\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao11-2978009-small.gif\" alt=\"FIGURE 11. - Performances achieved by SERAN trained by different values of &#10;$\\lambda $&#10;.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 11. </b><fig><p class=\"has-inline-formula\">Performances achieved by SERAN trained by different values of <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula>.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3c4\"><h4>4) Comprehensive Comparison with State-Of-The-Arts</h4><p>For the purpose to show the advancement of proposed SERAN, 5 state-of-the-art methods are implemented for comprehensive comparison. Three of them are neural networks and the other two are traditional artificial intelligence methods. We implement the three neural networks on Pytorch platform. The two traditional artificial intelligence methods are implemented mainly based on OpenCV library and Sklearn library.</p><p><a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a> shows the comparison between the 5 methods and SERAN. And <a ref-type=\"table\" anchor=\"table6\" class=\"fulltext-link\">Table 6</a> displays the computation cost and model complexity of each method. In this section, SERAN is trained by Dice loss with IOL. The method proposed in <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_3c4\" data-range=\"ref16\">[16]</a> has achieved the same results every time. Thus, the STDs of the method are 0. The visual results of these methods are showed in <a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig.12</a>. It can be concluded that SERAN outperform all the state-of-the-arts in a certain extent. The performance of SERAN is more stable than other neural networks and competitive to traditional artificial intelligence methods. The comparison result of sensitive shows that all the methods can predict masses with accurate locations. Indicated by IOU and Dice coefficient, the mass predicted by SERAN owns the biggest overlapping area with ground truth. The Accuracy indicator shows that SERAN can make better prediction about both mass and background than other methods.<div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nComparison With State-of-the-Arts (MEAN\u00b1STD)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t5-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t5-2978009-small.gif\" alt=\"Table 5- &#10;Comparison With State-of-the-Arts (MEAN\u00b1STD)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table6\"><div class=\"figcaption\"><b class=\"title\">TABLE 6 </b>\nComputation Cost</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t6-2978009-large.gif\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao.t6-2978009-small.gif\" alt=\"Table 6- &#10;Computation Cost\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig12\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao12-2978009-large.gif\" data-fig-id=\"fig12\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao12-2978009-small.gif\" alt=\"FIGURE 12. - Segmentation results produced by state-of-the-arts and SERAN.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 12. </b><fig><p>Segmentation results produced by state-of-the-arts and SERAN.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>The most competitive performance is achieved by the method proposed by Sun <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_3c4\" data-range=\"ref30\">[30]</a>. It can be attributed to the dense-upsampling and channel attention. The two mechanisms are effective to maintain most useful features. However, the channel attention is in a fully connected style, which is the leading cause for the high time cost. The method proposed by Li <i>et al.</i> <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3c4\" data-range=\"ref28\">[28]</a> employs spatial attention to guide the prediction made by decoder and achieves competitive results in all the metrics. The method proposed in <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3c4\" data-range=\"ref29\">[29]</a> owns the minimal complexity among all the neural networks and gets a good performance as well. The multi-scale context is effective to extract features under different receptive fields.</p><p>All the neural networks run faster than the traditional artificial intelligence methods. This may be caused by the great ability of Pytorch framework and the end-to-end style of neural network. The method proposed in <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_3c4\" data-range=\"ref6\">[6]</a> has used a complex processing flow to detect and identify the masses in each scale, which consists of morphological filtering, simple linear iterative clustering segmentation, feature extraction and classification. The method proposed in <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_3c4\" data-range=\"ref16\">[16]</a> has used watershed segmentation twice to find out all the regions of interest. A k-means clustering algorithm is used to reduce the number of regions found out by the first watershed segmentation algorithm and provide the maker to the second watershed segmentation method. Finally, the features extracted from each region are used to identify the true mass. Most of the components are time-consuming which result in the high time costs. However, the performances of these methods are poorer than neural networks mentioned above.</p><p>In the real world, it is easy for specialists to discover the masses with large size while the small size masses are difficult to recognize. To evaluate the performance on small size masses, we select all the masses smaller than 200 pixels and recorded the Dice coefficient achieved by state-of-the-arts and proposed SERAN. As shown in <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig.13</a>, SERAN has achieved much better performance than other methods in most cases. It can be concluded that SERAN owns the ability to work in complex conditions.\n<div class=\"figure figure-full\" id=\"fig13\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao13-2978009-large.gif\" data-fig-id=\"fig13\"><img src=\"\" data-src-img=\"/mediastore_new/IEEE/content/media/6287639/9668973/9023481/gao13-2978009-small.gif\" alt=\"FIGURE 13. - Performances achieved by all the state-of-the-arts and proposed SERAN on masses smaller than 200 pixels.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 13. </b><fig><p>Performances achieved by all the state-of-the-arts and proposed SERAN on masses smaller than 200 pixels.</p></fig></div><p class=\"links\"><a href=\"/document/9023481/figures\" class=\"all\">Show All</a></p></div></p><p>Moreover, the visual results of all the cases in test set have been reviewed by an experienced radiologist. In summary, our work in this paper has achieved great improvement in breast mass segmentation and can assist the radiologists to a certain extent.</p></div></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n                                SECTION IV.</div><h2>Conclusion</h2></div><p class=\"has-inline-formula\">In this paper, a Spatial Enhanced Rotation Aware Network (SERAN) is developed for breast mass segmentation using digital mammogram. Two main critical components are proposed for effective feature extraction and prediction refinement. An encoder with spatial attention enhancement under residual learning paradigm is designed for effective feature extraction. Spatial attention maps are explicitly modeled to adjust the focus in every encoder stage. Moreover, residual connection is utilized to avoid gradient vanishing and achieve better convergence. To boost the robustness of SERAN to masses displayed in different orientations, a decoder using multi-stream rotation aware mechanism for feature fusion and prediction refinement is designed. A <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> convolution and two asymmetric convolutions whose kernels are <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$1\\times 3$\n</tex-math></inline-formula> and <inline-formula class=\"inline-formula\"><tex-math notation=\"LaTeX\">$3\\times 1$\n</tex-math></inline-formula> respectively are combined in parallel to boost the rotation robustness. To avoid misclassification in background area and optimize SERAN for better prediction, a novel regulation item named Inside-outside Loss is applied in training procedure. Comparing with state-of-the-arts, SERAN has achieved significant performance improvement for breast mass segmentation. A sensitive of 87.7%, an IOU of 73.95% and a Dice coefficient of 84.3% are achieved by SERAN on a representative subset of DDSM dataset. In future work, we will focus on developing SERAN to fit different types of medical image, such as ultrasound, CT and MRI. Moreover, we will try to transform 2D convolution by 3D convolution to adapt 3D scenes, such as 3D CT and 3D MRI. Besides, we will update the current system to fit different application scenes, such as brain cancer segmentation and prostate cancer segmentation.</p></div>\n</div></div></response>\n"
}